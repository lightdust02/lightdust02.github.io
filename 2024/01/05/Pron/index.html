<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>常见概念-科研向 |  LightDust</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Pron"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  常见概念-科研向
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/01/05/Pron/" class="article-date">
  <time datetime="2024-01-05T08:35:00.000Z" itemprop="datePublished">2024-01-05</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">9.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">33 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>这部分内容记录了一系列科研过程中常到的概念，包括深度学习、图像处理、计算机视觉、多模态等方面的内容。</p>
<p>该博文会一直持续更新。</p>
<span id="more"></span>





<h2 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h2><h3 id="卷积操作："><a href="#卷积操作：" class="headerlink" title="卷积操作："></a>卷积操作：</h3><p>说白了，卷积操作是利用卷积核，通过卷积操作来实现对图像特征的提取。</p>
<p><img src="/images/Pron/%E5%8D%B7%E7%A7%AF.png" alt="卷积"></p>
<h4 id="一些名词："><a href="#一些名词：" class="headerlink" title="一些名词："></a>一些名词：</h4><p><img src="/images/Pron/%E4%B8%80%E4%BA%9B%E5%90%8D%E8%AF%8D.png" alt="一些名词"></p>
<h3 id="空洞卷积："><a href="#空洞卷积：" class="headerlink" title="空洞卷积："></a>空洞卷积：</h3><p><img src="/images/Pron/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.png" alt="空洞卷积"></p>
<h3 id="channels："><a href="#channels：" class="headerlink" title="channels："></a>channels：</h3><p>也被称为通道。</p>
<p>灰度图都是<code>1 channel</code>，彩色图都是<code>3 channels(R、G、B)</code>。</p>
<p><strong>卷积核中</strong>的 <code>in_channels</code> 与需要进行卷积操作的数据的 <code>channels</code>一致。</p>
<p>如下图所示，从左到右大小分别为6×6×3，3×3×3，4×4×1.</p>
<p><img src="/images/Pron/channel.png" alt="channel"></p>
<h3 id="梯度发散："><a href="#梯度发散：" class="headerlink" title="梯度发散："></a>梯度发散：</h3><p>采用一些激活函数（如ReLu）时，会出现随着卷积层层数的增加而导致导数指数级增长，即出现梯度发散现象。可以通过引入shortcut来解决：</p>
<h3 id="shortcut："><a href="#shortcut：" class="headerlink" title="shortcut："></a>shortcut：</h3><p>翻译为“捷径”，为了解决梯度发散问题，在两层之间增加了（带权的）shortcut 。结构如图所示：</p>
<p><img src="/images/Pron/shortcut.png" alt="shortcut"></p>
<p>通过 shortcut 可以直接将浅层的信息传递到深层，可以解决退化问题。</p>
<h3 id="numpy操作"><a href="#numpy操作" class="headerlink" title="numpy操作"></a>numpy操作</h3><p>numpy中有很多很有趣的操作：</p>
<p>array.flatten()：展平array</p>
<p>np.transpose(a)：转置 a ，等效于 a.T</p>
<p>多维矩阵问题：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337829793">【全面理解多维矩阵运算】多维（三维四维）矩阵向量运算-超强可视化 - 知乎 (zhihu.com)</a></p>
<h3 id="shift操作"><a href="#shift操作" class="headerlink" title="shift操作"></a>shift操作</h3><p>其实就是<strong>数据的平移</strong>或<strong>位移变换</strong>……</p>
<p><strong>卷积神经网络</strong>中用于<strong>特征图的移位</strong>，例如用于处理时空序列数据的位移。</p>
<p>在<strong>Transformer模型</strong>中，shift操作用于序列的偏移或位移，以实现特定的自回归或掩码机制。</p>
<h3 id="detach"><a href="#detach" class="headerlink" title="detach()"></a>detach()</h3><p>返回一个与当前 graph 分离的、不再需要梯度的新张量。</p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax:"></a>softmax:</h3><p>用于解决多分类问题，可以预测每个类别的概率。</p>
<p><img src="/images/Pron/softmax.png" alt="softmax"></p>
<p>Softmax 层会对上一层的原始数据归一化，转化为一个 (0 , 1) 之间的数值，这些数值可以被当做概率分布，用来作为多分类的目标预测值。</p>
<p>Softmax 函数一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。</p>
<h3 id="einsum"><a href="#einsum" class="headerlink" title="einsum:"></a>einsum:</h3><p>numpy 中一种优雅的矩阵矩阵计算方法，非常复杂。</p>
<p>具体参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/506843213">https://zhuanlan.zhihu.com/p/506843213</a></p>
<h3 id="消息传递神经网络（MPNN）"><a href="#消息传递神经网络（MPNN）" class="headerlink" title="消息传递神经网络（MPNN）"></a>消息传递神经网络（MPNN）</h3><p>消息传递神经网络（Message Passing Neural Network）是一类图神经网络，它通过在图的节点之间交换信息来学习节点的表示。它们基于以下步骤工作：</p>
<ol>
<li><strong>消息传递</strong>：每个节点接收其邻居节点的信息，并根据这些信息生成“消息”。</li>
<li><strong>聚合</strong>：将所有接收到的消息聚合成单个表示，这可以通过不同的函数实现，如求和、求平均或更复杂的操作。</li>
<li><strong>更新</strong>：使用聚合的信息来更新节点的状态。</li>
</ol>
<p>MPNN的核心思想是通过迭代这些步骤来精炼每个节点的表示，从而捕捉图的结构特征和节点之间的关系。</p>
<h3 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h3><p>GRU（gated recurrent unit）是循环神经网络（RNN）的一种变体，用于处理序列数据。与传统的RNN相比，GRU通过引入门控机制来解决梯度消失和梯度爆炸的问题，使得网络能够捕捉长距离依赖关系。GRU包含两个门：</p>
<ol>
<li><strong>更新门</strong>：决定状态信息应该如何更新。</li>
<li><strong>重置门</strong>：决定过去的状态信息在计算新状态时应保留多少。</li>
</ol>
<p>在每个时间步，GRU可以选择保留旧状态的信息并融入新输入的信息，这使得它在处理具有复杂依赖结构的数据时非常有效。</p>
<h3 id="EMA"><a href="#EMA" class="headerlink" title="EMA"></a>EMA</h3><p>EMA 模型，Exponential Moving Average model，通过计算模型参数的指数加权移动平均来平滑参数更新的过程。</p>
<p><img src="/images/Pron/ema.png" alt="image-20240325235724569"></p>
<p>EMA可以近似看成过去 $1/(1−\beta) $ 个时刻  v 值 的平均。</p>
<p>本质是对变量的一种加权平均。在训练过程中，原始模型的参数会不断更新，而 EMA 模型的参数则通过指数加权平均的方式来跟踪原始模型的参数变化。</p>
<h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><p><strong>前馈神经网络（FFN）</strong> 是每个 Transformer 层中的组成部分，它位于每次多头注意力操作之后，对从 Self-Attention 模块中获取的特征进行进一步处理。</p>
<p>FFN 由两个线性变换和一个激活函数（通常是 ReLU 或 GELU）组成。前馈神经网络中的非线性激活函数可以增强模型的表达能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),  <span class="comment"># 第一个线性层</span></span><br><span class="line">            nn.GELU(),                   <span class="comment"># 激活函数</span></span><br><span class="line">            nn.Dropout(dropout),          <span class="comment"># Dropout</span></span><br><span class="line">            nn.Linear(hidden_dim, dim),   <span class="comment"># 第二个线性层</span></span><br><span class="line">            nn.Dropout(dropout)           <span class="comment"># Dropout</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>

<p>gelu函数图像如下所示：<br><img src="/images/Pron/v2-149a8bc6ee90f65205a8d408f79a0017_1440w.webp" alt="GELU函数"></p>
<h3 id="NIAM"><a href="#NIAM" class="headerlink" title="NIAM"></a>NIAM</h3><blockquote>
<p>非常冷门……我怎么会写这个的……</p>
</blockquote>
<p>NIAM， Neural Implicit Attentive Model ，是一种与深度学习相关的模型。</p>
<p>一种基于注意力机制的神经网络模型，旨在处理具有隐式关系的数据。</p>
<p>隐式关系：不能直接观察到的关系，但可以通过数据间的相关性来推断。NIAM 模型可以自动地学习和捕捉这些隐式关系，从而在各种任务中表现出色，如推荐系统、自然语言处理和图像处理等。</p>
<h3 id="端到端end-to-end"><a href="#端到端end-to-end" class="headerlink" title="端到端end-to-end"></a>端到端end-to-end</h3><p>指从原始输入数据到最终输出结果的整个过程由一个统一的模型处理，即输入是原始数据，输出是最后的结果。</p>
<h3 id="2D-Feature-map"><a href="#2D-Feature-map" class="headerlink" title="2D Feature map"></a>2D Feature map</h3><p>通过卷积神经网络的卷积层和池化层处理后得到的二维矩阵。该矩阵包含了输入图像的某些特征。</p>
<h3 id="准确率、精确度、召回率、F1"><a href="#准确率、精确度、召回率、F1" class="headerlink" title="准确率、精确度、召回率、F1"></a>准确率、精确度、召回率、F1</h3><h4 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率 Accuracy"></a>准确率 Accuracy</h4><p>正例和负例中预测正确数量占总数量的比例<br>$$<br>ACC=\frac{TP+TN}{TP+TN+FP+FN}<br>$$</p>
<h4 id="精确度-Precision"><a href="#精确度-Precision" class="headerlink" title="精确度 Precision"></a>精确度 Precision</h4><p>以<strong>预测结果</strong>为判断依据，预测为正例的样本中预测正确的比例<br>$$<br>Precision=\frac{TP}{TP+FP}<br>$$</p>
<h4 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率 Recall"></a>召回率 Recall</h4><p>以<strong>实际样本</strong>为判断依据，实际为正例的样本中，被预测正确的正例占总实际正例样本的比例。<br>$$<br>Recall=\frac{TP}{TP+FN}<br>$$</p>
<h4 id="F1值"><a href="#F1值" class="headerlink" title="F1值"></a>F1值</h4><p>中和了精确率和召回率的指标<br>$$<br>F1=\frac{2PR}{R+P}<br>$$</p>
<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>变分自编码器VAE（Variational auto-encoder），可以通过编码解码的步骤，直接比较重建图片和原始图片的差异。VAE以概率的方式描述对潜在空间的观察，在数据生成方面表现出了巨大的应用价值。VAE一经提出就迅速获得了深度生成模型领域广泛的关注。</p>
<p>VAE 会将给定输入的每个潜在特征表示为概率分布。当从潜在状态解码时，VAE 将<strong>从每个潜在状态分布中随机采样</strong>，生成一个向量作为解码器模型的输入。</p>
<p>VAE通过编码器将输入数据映射到一个概率分布，通常假设为高斯分布。不同于传统自编码器生成一个确定的隐变量，VAE生成的是一组分布参数（均值μ和方差σ），并使用这些参数从该分布中采样隐变量。解码器从这些隐变量中重构输入数据。</p>
<p>同时，在损失函数中，VAE引入了交叉熵与KL散度（是不是很熟悉？），KL散度能够确保编码器生成的潜在分布接近标准正态分布，保持潜在空间的连续性和可操作性。这有助于生成新的样本，保证潜在空间的结构合理。</p>
<p>具体步骤：</p>
<p>1). 输入数据 X 输入编码器，得到潜在空间的分布参数（均值和标准差）</p>
<p>2). 使用重参数化技巧从潜在分布中采样潜在变量 z</p>
<p>3). 将采样的 z 输入解码器，生成数据 </p>
<p>4). 计算损失函数，包括重构误差和KL散度</p>
<p>5). 使用反向传播最小化损失函数，更新网络参数。</p>
<p>相比GAN等生成模型，VAE的训练更稳定，模型结构更为清晰，易于理解。尽管VAE的生成质量通常不如GAN。</p>
<h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p><img src="/images/Pron/image-20240912110124560.png" alt="GAN示意图"></p>
<p>GAN于2014年提出。具体而言，GAN的核心结构是生成器与判别器。</p>
<p>生成器的功能是根据真实图像样本<code>X</code>来调整分布，逐步将随机噪声分布<code>z</code>调整为与原始训练数据相似的模拟数据分布，从而实现生成的模拟图像样本<code>X*</code>能够以假乱真。</p>
<p>判别器则负责检测输入数据的真伪，判断输入数据为真实图像<code>X</code>还是模拟图像<code>X*</code>，并将判断结果作为监督信息反馈给生成器，生成器根据判别器的反馈信息来调整图像的生成，使得模拟图像能够更好地“骗过”生成器。</p>
<p>在训练过程中，生成器与判别器交替训练，二者相互竞争、相互制约，最终会达到一种动态平衡。生成器得以在判别器的制约下生成更为真实的数据，判别器也能够通过判断生成器产生的伪造数据，不断提升其鉴别能力。</p>
<p><img src="/images/Pron/image-20240912110753701.png" alt="优化目标函数"></p>
<p><strong>生成器</strong>通常是一个前馈神经网络，它接收一个随机噪声向量并生成一组样本（例如图像或文本）。生成器学习到的映射是从一个简单分布（如正态分布）到数据分布的转变。</p>
<p><strong>判别器</strong>也是一个前馈神经网络，接收一组样本并输出一个概率值，表示该样本是真实数据还是生成的数据。判别器的训练目标是最大化其区分真实和伪造样本的能力。</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>[【论文笔记】Order Matters （AAAI 20） - Yuhan’s blog (yuhan2001.github.io)](<a target="_blank" rel="noopener" href="https://yuhan2001.github.io/2024/05/04/Order">https://yuhan2001.github.io/2024/05/04/Order</a> Matters/)</p>
<p>一些解读Transformer的文章：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/fengdu78/article/details/104629336">关于Transformer，面试官们都怎么问？-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82391768https://zhuanlan.zhihu.com/p/82391768">关于Transformer的若干问题整理记录 - Adherer的文章</a></p>
<h3 id="Self-Attention（qkv）"><a href="#Self-Attention（qkv）" class="headerlink" title="Self-Attention（qkv）"></a>Self-Attention（qkv）</h3><p>Q、K、V 就是Self-Attention机制的核心。</p>
<p>Q: Query，查询，查询其他元素与自己相关的程度。        </p>
<p>K: Key，键，表示序列中的每个元素特征，用来判断是否与其他元素（即Query）有关。</p>
<p>V: Value，值，代表序列中每个元素的实际信息。当Query与某个Key的相似度较高时，这个Value就会对Query的输出产生更大的影响。</p>
<p>在Self-Attention机制中，输入的每个元素都会产生对应的Q、K、V，通过将查询 Q 与键 K 进行相关性计算，然后将这些相关性作为权重应用到值 V 上。</p>
<p>输入序列被用来计算查询 Q 、键 K 和值 V 。每个查询 Q 与所有键 K 计算得到相关性分数，然后通过 Softmax 函数将这些分数归一化为权重，最终这些权重被应用到相应的值 V 上，从而得到注意力表示。</p>
<p>Q、K 和 V 的作用可以简单概括如下：</p>
<ul>
<li><strong>Query（Q）</strong>：确定关注的内容，是用来“提问”的向量。它表示每个元素希望从其他元素中提取的信息，即当前元素与其他元素的相关性。</li>
<li><strong>Key（K）</strong>：提供与查询相关的上下文，是用来“回答”的向量。它表示每个元素本身的特征，其他元素的Query会根据这些Key来判断它们是否相关。</li>
<li><strong>Value（V）</strong>：是序列中的信息内容。Self-Attention 的最终输出就是根据Query和Key的相似度对这些Value进行加权求和后的结果。</li>
</ul>
<p>假设输入序列的维度是 $b×n×d$，其中 b 表示batch size，n 表示序列长度，d 表示每个元素的特征维度。</p>
<p>那么，Q K V 的维度均为 $b×n×d_k$。相关计算公式为 $\frac{QK^T}{\sqrt{d_k}}V$ ，$T$是转置操作，$d_k$是K的维度。</p>
<p><strong>除以$d_k$：在softmax之前除以$dk$ 可以将 $QK^T$ 的分布的方差纠正回接近1，这样一来，大部分数值都会分布在softmax梯度适当的位置，也就避免了梯度消失的问题。</strong></p>
<p><strong>大值的点积可能会导致 Softmax 进入梯度消失区域，因为极大的输入值会使 Softmax 函数的梯度接近 0。而通过除以 $\sqrt{d_k}$，可以避免进入这种极端情况，保持梯度稳定</strong></p>
<p>同时，由于add层（残差神经网络）与norm层（LayerNorm归一化），因此Encoder端和Decoder端每个子模块实际的输出为： $LayerNorm(x+Sublayer(x))$ ，其中 Sublayer(x)) 为子模块的输出。</p>
<h3 id="Linear-Attention"><a href="#Linear-Attention" class="headerlink" title="Linear-Attention"></a>Linear-Attention</h3><p><strong>Linear-Attention</strong> 是一种优化 Self-Attention 的变体，其核心思想是降低计算复杂度。</p>
<p>Linear-Attention 中，使用的是 <code>nn.Conv2d</code>从输入特征图中通过卷积层生成 <code>Q</code>、<code>K</code>、<code>V</code>，这表明它主要用于处理图像等二维数据。</p>
<p>同时，和 Self-Attention 的 $QK^T / \sqrt{d_k}$ 计算不同，Linear-Attention 通过 <code>einsum(&#39;bhdn,bhen-&gt;bhde&#39;, k, v)</code> 和 <code>einsum(&#39;bhde,bhdn-&gt;bhen&#39;, context, q)</code> 来进行计算。</p>
<ul>
<li>这里直接通过将 <code>K</code> 应用 <code>softmax</code>，然后将其与 <code>V</code> 通过 <code>einsum</code> 进行乘积，生成上下文信息。</li>
<li>后续的 <code>Q</code> 通过与上下文信息进行计算，生成输出。</li>
</ul>
<h3 id="Cross-Attention"><a href="#Cross-Attention" class="headerlink" title="Cross-Attention"></a>Cross-Attention</h3><p>Cross-Attention 用于处理两个不同输入之间的注意力计算。</p>
<p>Cross-Attention 在 Transformer 的编码器-解码器结构中有广泛应用，尤其是在任务中需要处理源数据和目标数据之间的依赖时，比如在机器翻译、图像生成等任务中。</p>
<p>在 Self-Attention 中，<code>Q</code>、<code>K</code> 和 <code>V</code> 来自同一个输入，而在 Cross-Attention 中，<code>Q</code> 和 <code>K</code>、<code>V</code> 可以来自不同的输入，常用于需要融合不同数据源的信息时。</p>
<p>Cross-Attention 的查询（Q）来自<strong>目标序列</strong>（通常是解码器的输入），而键（K）和值（V）来自源序列（通常是编码器的输出），公式仍然是和上面一致。</p>
<p>Cross-Attention 在机器翻译、图像生成、多模态等领域应用非常广泛。</p>
<h3 id="Muti-head-Attention"><a href="#Muti-head-Attention" class="headerlink" title="Muti-head-Attention"></a>Muti-head-Attention</h3><p>多头注意力机制在 Self-Attention 的基础上引入多个头（heads），即将注意力机制复制多次，每个注意力头使用不同的权重矩阵进行计算。这样做的目的是通过不同的子空间进行特征表示，从而捕捉不同层次的信息。</p>
<p>Q、K、V 通过不同的线性层，得到不同头的查询、键和值</p>
<p>$Q_i=W_i^QQ, K_i=W_i^KK,V_i=W_i^VV ,i=1,2,…h$ ，W是权重矩阵，h是头的数量。</p>
<p>然后，每个头都独立进行自注意力计算，得到每个头的注意力输出，公式同Self-Attention。</p>
<p>再将所有注意力头的输出拼接起来：</p>
<p>$MultiHead(Q,K,V)=Concat(head_1,head_2,…,head_h)$</p>
<p>最后再对拼接后的结果进行线性变换，得到最终的多头注意力输出：</p>
<p>$output=W^o⋅Concat(head_1,head_2,…,head_h)$</p>
<p> $W^O$  是输出层的权重矩阵。</p>
<p><strong>多头注意力机制保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。</strong></p>
<p>Decoder端的多头self-attention需要做mask，因为它在预测时，是“看不到未来的序列的”，所以要将当前预测的单词(token)及其之后的单词(token)全部mask掉。</p>
<h3 id="spatial-transformer"><a href="#spatial-transformer" class="headerlink" title="spatial_transformer"></a>spatial_transformer</h3><p>空间变换网络（Spatial Transformer Networks，简称STN）是一种深度学习模型，旨在增强网络对几何变换的适应能力。</p>
<p>在现代深度学习中，空间变换模块常常与自注意力机制结合使用，以便于模型能够关注输入数据的不同部分，并在处理过程中考虑这些部分之间的关系。</p>
<p>同时，在一些变体中，<code>SpatialTransformer</code> 可以结合条件输入（如上下文信息、标签等），使得生成的空间变换更具针对性和自适应性。</p>
<p>Diffusion中，SpatialTransformer非常常见。</p>
<h3 id="Vision-Transformer-ViT"><a href="#Vision-Transformer-ViT" class="headerlink" title="Vision Transformer(ViT)"></a>Vision Transformer(ViT)</h3><p>Vision Transformer（ViT）是将 Transformer 模型应用于图像分类的一种架构，旨在利用 Transformer 强大的全局建模能力来处理图像任务。</p>
<p>ViT 通过将图像分割成若干个小块<code>patch</code>，将它们看作序列输入到 Transformer 中，以捕捉全局依赖关系。</p>
<p>然后需要一项关键的内容：<strong>位置编码（Positional Embedding）</strong>，以保留图像的空间位置信息。</p>
<p>事实上，在最初的NLP领域，位置编码也是很重要的内容。位置编码一般采用正弦和余弦函数来生成<br>$$<br>PE_{pos,2i}=sin\frac{pos}{10000^{\frac{2i}{d_{model}}}}\<br>PE_{pos,2i+1}=cos\frac{pos}{10000^{\frac{2i}{d_{model}}}}<br>$$<br>其中 $d_{model}$ 是编码维度的总大小。</p>
<h4 id="FFN-1"><a href="#FFN-1" class="headerlink" title="FFN"></a>FFN</h4><h3 id="Bert："><a href="#Bert：" class="headerlink" title="Bert："></a>Bert：</h3><p>Bert，Bidirectional Encoder Representations from Transformers，是一种基于 Transformer 结构的语言模型，用来生成文本的深层双向表示。它<strong>彻底</strong>改变了自然语言处理（NLP）领域的研究方式。</p>
<p>BERT 的核心创新之一是双向编码器。在传统的语言模型  LSTM 或 RNN中，模型会顺序从左到右或从右到左预测下一个词。而 BERT 是通过同时考虑句子的左右上下文来学习词汇的含义。这种双向性使得 BERT 能够理解一个词或子词在句子中的更深层次的语义关系。</p>
<p><strong>Transformer架构</strong>：BERT 基于 Transformer 编码器，依赖于注意力机制（Self-Attention）来捕捉句子中不同词之间的关系。BERT 只使用 Transformer 的 Encoder 部分，它并不生成文本，而是通过理解现有文本进行分类或问答等任务。</p>
<p>传统语言模型中，单向上下文（仅前向或后向）导致了信息丢失。而BERT 的成功归功于两种新的预训练任务：</p>
<p>1). <strong>Masked Language Model (MLM)<strong>： 传统的语言模型通过预测下一个词来学习，而 BERT 使用了一种称为 Masked Language Model 的技术。在 MLM 中，</strong>输入的句子会随机遮盖（mask）一些词</strong>（通常是 15%），要求模型预测这些掩盖单词的原始值来实现。模型的目标是<strong>在训练过程中预测出这些被遮盖的词</strong>。</p>
<p>由于模型在训练时看不到某些词，它被迫通过句子的上下文来推测缺失的词，这有助于模型更好地理解词汇的语义和上下文关系。</p>
<p>例如： 输入句子：<code>[CLS] The cat sits on the [MASK]. [SEP]</code> 模型的任务是预测 <code>[MASK]</code> 的正确词（如 “mat”）。</p>
<p>这种训练方式使得BERT能够有效地学习单词的上下文关系和语义信息，从而更好地理解语言。</p>
<p>2). <strong>Next Sentence Prediction (NSP)<strong>： 除了预测遮盖的词，BERT 还引入了 Next Sentence Prediction 任务。这种任务的目标是</strong>使模型能够理解句子之间的关系</strong>。<strong>预测给定的两句话是否是连续的</strong>。通过这个任务，BERT 可以学会句子间的逻辑关系，从而在处理句子对相关的任务（如问答、自然语言推理等）时表现得更好。</p>
<p>具体来说，BERT 会接收两个句子，模型需要判断第二句话是否是第一句话的连续句。例如：</p>
<ul>
<li>输入：<code>[CLS] The cat is on the mat. [SEP] It is sleeping. [SEP]</code></li>
<li>输出：True（句子连贯）</li>
</ul>
<p>通过这种方式，BERT学习理解句子之间的逻辑和关系，增强对文本的整体理解能力。</p>
<h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h3 id="BEV"><a href="#BEV" class="headerlink" title="BEV:"></a>BEV:</h3><p><strong>BEV</strong>: 鸟瞰视角，Bird’s Eye View，是一种<strong>从上方观看</strong>对象或场景的视角，就像鸟在空中俯视地面一样。在自动驾驶和机器人领域，通过传感器（如 LiDAR 和摄像头）获取的数据通常会被转换成 BEV 表示，以便更好地进行<strong>物体检测、路径规划</strong>等任务。</p>
<h3 id="FPN："><a href="#FPN：" class="headerlink" title="FPN："></a>FPN：</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62604038">【目标检测】FPN(Feature Pyramid Network) - 知乎 (zhihu.com)</a></p>
<blockquote>
<p>整理完下面这些发现其实只看这一篇文章就够了。</p>
</blockquote>
<p><strong>FPN</strong>: 特征金字塔网络，Feature Pyramid Network。</p>
<p>传统方法使用图像金字塔，将图片 resize 到不同的大小，然后分别得到对应大小的特征，然后进行预测。这种方法可以一定程度上解决多尺度的问题，但带来的计算量也非常大。如下图所示：</p>
<p><img src="/images/Pron/%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94.jpg" alt="图像金字塔"></p>
<p>也有使用单个feature map进行检测，在网络的最后一层的特征图上进行预测。优点是计算速度快，缺点是最后一层的特征图分辨率低，不能准确的包含物体的位置信息。直接使用这种架构导致预测层的特征尺度比较单一，对小目标检测效果比较差。如下图所示：</p>
<p><img src="/images/Pron/featuremap.jpg" alt="featuremap"></p>
<p>为了使得不同尺度的特征都包含丰富的语义信息，同时又不使得计算成本过高，FPN 采用top down和lateral connection的方式，让低层高分辨率低语义的特征和高层低分辨率高语义的特征融合在一起。</p>
<p><img src="/images/Pron/FPN.jpg" alt="FPN"></p>
<p>三个结构：</p>
<p>Bottom-up：Bottom-up 的过程就是将图片输入到backbone ConvNet中提取特征的过程中。Backbone输出的feature map的尺寸有的是不变的，有的是成2倍的减小的。</p>
<p>Top-down：Top-down 的过程就是将高层得到的feature map进行上采样然后往下传递。高层的特征包含丰富的语义信息，经过top-down 这些语义信息能传播到低层特征上，使得低层特征也包含丰富的语义信息。</p>
<p>Lateral connection：3 个步骤，先 1*1的卷积降低维度，再将得到的特征和上一层采样得到特征图 $P_{n+1}$ 进行融合，直接相加。相加完后再进行一个 3*3 的卷积得到本层的特征输出 $P_n$。</p>
<h3 id="常见模型简称："><a href="#常见模型简称：" class="headerlink" title="常见模型简称："></a>常见模型简称：</h3><p>LC-GAN : 基于生成对抗网络的人脸数据隐私保护算法模型</p>
<p>CGAN : 条件生成对抗网络，是一种基于监督学习的神经网络深度学习模型</p>
<p>PP-GAN : 隐私保护网络，基于生成对抗网络，引入了针对人脸去识别问题而设计的验证器和调节模块</p>
<h3 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h3><p>Contrastive Language-Image Pre-Training 模型。一种多模态预训练神经网络，由OpenAI在2021年发布，是从自然语言监督中学习的一种有效且<strong>可扩展</strong>的方法。</p>
<p>CLIP模型有两个模态，一个是文本模态，一个是视觉模态，包括两个主要部分：</p>
<ol>
<li>Text Encoder：用于将文本转换为低维向量表示-Embeding。</li>
<li>Image Encoder：用于将图像转换为类似的向量表示-Embedding</li>
</ol>
<p>CLIP 通过计算文本和图像向量之间的<strong>余弦相似度</strong>来生成预测。这种模型特别适用于<strong>零样本学习</strong>任务，即模型不需要看到新的图像或文本的训练示例就能进行预测。CLIP模型在多个领域表现出色，如图像文本检索、图文生成等。</p>
<h3 id="Mamba"><a href="#Mamba" class="headerlink" title="Mamba"></a>Mamba</h3><blockquote>
<p>Mamba Out!</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/134923301">一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba_mamba模型-CSDN博客</a></p>
<h3 id="边缘监测："><a href="#边缘监测：" class="headerlink" title="边缘监测："></a>边缘监测：</h3><h4 id="Canny-Edge"><a href="#Canny-Edge" class="headerlink" title="Canny Edge"></a>Canny Edge</h4><p>一种经典的图像边缘检测算法。其核心思想是通过多阶段处理来提取图像的边缘，步骤包括高斯滤波、计算梯度、非极大值抑制以及双阈值检测。</p>
<h4 id="HED-Edge"><a href="#HED-Edge" class="headerlink" title="HED Edge"></a>HED Edge</h4><p>一种基于卷积神经网络（CNN）的边缘检测方法，旨在从图像中提取边缘信息。</p>
<h4 id="MLSD-Edge"><a href="#MLSD-Edge" class="headerlink" title="MLSD Edge"></a>MLSD Edge</h4><p>一种轻量化的线段检测方法，通常用于移动端或资源受限的设备上，专注于检测图像中的直线段。这种方法非常适合用于场景理解、几何建模以及三维重建等任务。</p>
<h4 id="MIDAS-Depth-and-Normal"><a href="#MIDAS-Depth-and-Normal" class="headerlink" title="MIDAS Depth and Normal"></a>MIDAS Depth and Normal</h4><p>一种基于神经网络的深度估计模型，能够从单张图像推测其深度信息（即每个像素距离相机的远近）。</p>
<h4 id="Openpose"><a href="#Openpose" class="headerlink" title="Openpose"></a>Openpose</h4><p>一种基于深度学习的人体姿态估计模型，它能够从图像或视频中检测人体的关节点（如头、肩、肘、膝等），并绘制骨架结构。</p>
<h4 id="Uniformer-Segmentation"><a href="#Uniformer-Segmentation" class="headerlink" title="Uniformer Segmentation"></a><strong>Uniformer Segmentation</strong></h4><p>一种结合 Transformer 和 CNN 的图像分割模型，旨在统一处理局部和全局信息。</p>
<p>通过 UniFormer 架构，图像分割模型能够更好地捕捉图像的全局上下文信息，同时保持对局部细节的敏感度。这种架构在语义分割任务中表现出色，能够有效地区分不同物体或区域。</p>
<h2 id="Diffusion"><a href="#Diffusion" class="headerlink" title="Diffusion"></a>Diffusion</h2><h3 id="LDM"><a href="#LDM" class="headerlink" title="LDM"></a>LDM</h3><p>Latent Diffusion Model，潜在扩散模型，一种生成模型，结合了扩散模型和潜在空间的思想，用于生成高质量的图像。通过逐步对噪声数据进行去噪，从而生成逼真的图像。</p>
<p>LDM 将数据x投射到一个潜在空间中，然后在这个潜在空间中进行扩散过程得到结果z，最后再将结果z映射回原始数据空间。</p>
<p>关键函数：ddpm.py中的 decode_first_stage 函数与 encode_first_stage 函数，get_first_stage_encoding函数</p>
<p>(这个代码不太好懂……慢慢来看)</p>
<h3 id="kl散度"><a href="#kl散度" class="headerlink" title="kl散度"></a>kl散度</h3><p>简而言之，kl散度用于计算两个分布之间的差异。<br>$$<br>D_{KL}(p∥q)= \sum_{i=1}^{n}p(x_i)log(\frac{p(x_i)}{q(x_i)})<br>$$<br>LDM中，normal_kl一般用于计算两个正态分布之间的差异。</p>
<p>LDM 会对数据进行潜在空间的编解码，而潜在空间中通常使用高斯分布对数据进行建模，因此对高斯分布进行采样、计算对数似然（nll）以及 KL 散度都是训练 LDM 时非常关键的操作。</p>
<h3 id="nll"><a href="#nll" class="headerlink" title="nll"></a>nll</h3><p>负对数似然损失（Negative Log-Likelihood，NLL），整个数据集的负对数似然损失就是对所有样本的损失进行求和或平均。<br>$$<br>NLL=\frac{1}{2}\sum_i[log(2\pi)+log(\sigma_i^2)+\frac{(x_i-\mu_i)^2}{\sigma_i^2}]<br>$$</p>
<h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><h3 id="噪声："><a href="#噪声：" class="headerlink" title="噪声："></a>噪声：</h3><p>图像噪声是指存在于图像数据中的<strong>不必要的或多余的干扰信息</strong>。</p>
<h3 id="仿射变换："><a href="#仿射变换：" class="headerlink" title="仿射变换："></a>仿射变换：</h3><p>Affine Transformation，线性变换与平移变化的叠加。</p>
<p>包括<strong>缩放</strong>(Scale)、<strong>平移</strong>(transform)、<strong>旋转</strong>(rotate)、<strong>反射</strong>（reflection）、<strong>错切</strong>(shear mapping）</p>
<p><img src="/images/Pron/affine.png" alt="affine transformations"></p>
<h3 id="DCT变换："><a href="#DCT变换：" class="headerlink" title="DCT变换："></a><strong>DCT变换</strong>：</h3><p>Discrete Cosine Transform，离散余弦变换。</p>
<p>离散余弦变换经常被信号处理和图像处理使用，<strong>用于对信号和图像(包括静止图像和运动图像)进行有损数据压缩</strong>。这是由于离散余弦变换具有很强的”能量集中”特性:大多数的自然信号(包括声音和图像)的能量都集中在离散余弦变换后的低频部分，而且当信号具有接近马尔科夫过程的统计特性时，离散余弦变换的去相关性接近于K-L变换 (Karhunen-Loève 变换，具有最优的去相关性) 的性能。</p>
<h3 id="全变分模型（Total-Variation-TV）"><a href="#全变分模型（Total-Variation-TV）" class="headerlink" title="全变分模型（Total Variation, TV）"></a>全变分模型（Total Variation, TV）</h3><p>图像处理领域中常用的一种正则化方法，尤其在图像去噪、图像修复和图像重建问题中广泛应用。</p>
<p>核心思想是通过最小化图像的全变分来保留图像中的边缘细节，同时去除噪声或平滑不必要的纹理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">total_variation</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Anisotropic TV.&quot;&quot;&quot;</span></span><br><span class="line">    dx = torch.mean(torch.<span class="built_in">abs</span>(x[:, :, :, :-<span class="number">1</span>] - x[:, :, :, <span class="number">1</span>:]))</span><br><span class="line">    dy = torch.mean(torch.<span class="built_in">abs</span>(x[:, :, :-<span class="number">1</span>, :] - x[:, :, <span class="number">1</span>:, :]))</span><br><span class="line">    <span class="keyword">return</span> dx + dy</span><br></pre></td></tr></table></figure>



<h3 id="量化："><a href="#量化：" class="headerlink" title="量化："></a><strong>量化</strong>：</h3><p>将图像像素点对应亮度的连续变换区间转换为单个特定值的过程，即<strong>将原始灰度图像的空间坐标幅度值离散化</strong>。量化等级越多，图像层次越丰富，灰度分辨率越高，图像的质量也越好；量化等级越少，图像层次欠丰富，灰度分辨率越低。</p>
<p>量化后，图像就被表示成一个整数矩阵，每个像素具有两个属性：位置和灰度。位置由行，列表示。灰度表示该像素位置上亮暗程度的整数。此数字矩阵M*N就作为计算机处理的对象了，灰度级一般为0-255（8bit量化）。如果量化等级为2，则将使用两种灰度级表示原始图像的像素（0-255），灰度值小于128的取0，大于等于128的取128；如果量化等级为4，则将使用四种灰度级表示原始图像的像素，新图像将分层为四种颜色，0-64区间取0,64-128区间取64,128-192区间的取128,192-255区间取192，依次类推。</p>
<p><a target="_blank" rel="noopener" href="http://t.zoukankan.com/wj-1314-p-12191084.html">OpenCV计算机视觉学习（12）——图像量化处理&amp;图像采样处理（K-Means聚类量化，局部马赛克处理） - 走看看 (zoukankan.com)</a></p>
<h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE:"></a>MSE:</h3><p>MSE , Mean Square Error , 均方误差，是衡量图像质量的指标之一。</p>
<p>原理为<strong>真实值与预测值的差值的平方然后求和再平均</strong></p>
<p><img src="/images/Pron/M.png" alt="MSE"></p>
<h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR:"></a>PSNR:</h3><p>PSNR , Peak Signal-to-Noise Ratio 峰值信噪比，定义为</p>
<p><img src="/images/Pron/psnr.png" alt="PSNR"></p>
<p>其中 $MAX_{I}^2$ 为图片可能的最大像素值。如果每个像素都由 8 位二进制来表示，那么就为 255。</p>
<p>这是针对灰度图像的计算方法，如果彩色图像，通常有三种方法来计算。</p>
<ul>
<li>分别计算 RGB 三个通道的 PSNR，然后取平均值。</li>
<li>计算 RGB 三通道的 MSE ，然后再除以 3 。</li>
<li>将图片转化为 YCbCr 格式，然后只计算 Y 分量也就是亮度分量的 PSNR。</li>
</ul>
<p>常见的是第二种与第三种。</p>
<h3 id="一系列评价指标："><a href="#一系列评价指标：" class="headerlink" title="一系列评价指标："></a>一系列评价指标：</h3><p>$\uparrow$ 表示越高效果越好</p>
<table>
<thead>
<tr>
<th><strong>信息熵</strong></th>
<th><strong>EN</strong></th>
<th>衡量图像中信息量的度量，表示图像复杂性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>空间频率</strong></td>
<td><strong>SF</strong></td>
<td>衡量图像细节和纹理变化的频率，反映图像的锐度</td>
</tr>
<tr>
<td><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%A0%87%E5%87%86%E5%B7%AE&zhida_source=entity&is_preview=1">标准差</a></strong></td>
<td><strong>SD</strong></td>
<td>图像像素强度的标准差，衡量图像的对比度。</td>
</tr>
<tr>
<td><strong>峰值信噪比</strong></td>
<td><strong>PSNR</strong></td>
<td>衡量图像重建质量的指标，用于比较原始图像和重建图像之间的差异</td>
</tr>
<tr>
<td><strong>均方误差</strong></td>
<td><strong>MSE</strong></td>
<td>衡量图像重建误差的指标，计算原始图像与重建图像之间的均方误差</td>
</tr>
<tr>
<td><strong>互信息</strong></td>
<td><strong>MI</strong></td>
<td>衡量两幅图像之间共享信息的量，用于图像配准或融合。</td>
</tr>
<tr>
<td><strong>视觉保真度</strong></td>
<td><strong>VIF</strong></td>
<td>根据人眼感知模型评估图像质量的指标，衡量图像保留的视觉信息量。</td>
</tr>
<tr>
<td><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%B9%B3%E5%9D%87%E6%A2%AF%E5%BA%A6&zhida_source=entity&is_preview=1">平均梯度</a></strong></td>
<td><strong>AG</strong></td>
<td>衡量图像梯度的平均值，表示图像的清晰度。</td>
</tr>
<tr>
<td><strong>相关系数</strong></td>
<td><strong>CC</strong></td>
<td>衡量两幅图像的相似度，常用于图像匹配和重建。</td>
</tr>
<tr>
<td><strong>差异相关和</strong></td>
<td><strong>SCD</strong></td>
<td>衡量图像结构的差异，评估图像之间的结构相似性。</td>
</tr>
<tr>
<td><strong>基于梯度的融合性能</strong></td>
<td><strong>F</strong></td>
<td>基于图像特征相似性的质量评估指标，考虑边缘和纹理特征。</td>
</tr>
<tr>
<td><strong>结构相似度测量</strong></td>
<td><strong>SSIM</strong></td>
<td>衡量两幅图像的结构相似度，考虑亮度、对比度和结构信息。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%BB%93%E6%9E%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%B5%8B%E9%87%8F&zhida_source=entity&is_preview=1">多尺度结构相似度测量</a></td>
<td><strong>MS-SSIM</strong></td>
<td>SSIM 的多尺度扩展版本，综合不同尺度的图像质量。</td>
</tr>
<tr>
<td><strong>基于噪声评估的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%9E%8D%E5%90%88%E6%80%A7%E8%83%BD&zhida_source=entity&is_preview=1">融合性能</a></strong></td>
<td><strong>Nabf</strong></td>
<td>一种无参考的图像质量评估指标，基于图像的特征提取</td>
</tr>
</tbody></table>
<p>性能评估指标主要分为四类，分别是<strong>基于信息论的评估指标</strong>，主要包括 EN、MI、PSNR、基于结构相似性的评估指标<strong>，主要包括</strong>SSIM、MS_SSIM、MSE<strong>、</strong>基于图像特征的评估指标<strong>， 主要包括</strong>SF、SD、AG<strong>，</strong>基于人类视觉感知的评估指标<strong>，主要包括</strong>VIF<strong>、</strong>以及基于源图像与生成图像的评估指标<strong>，主要包括</strong>CC、SCD、Qabf、Nabf**。</p>
<h3 id="空域图像："><a href="#空域图像：" class="headerlink" title="空域图像："></a>空域图像：</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Vulcan_Q/article/details/127876972">(62条消息) 【视觉入门】——空域图像增强(邻域运算部分）：图像卷积、多种滤波器；二值图像分析及形态学图像处理_Vulcan_Q的博客-CSDN博客_空域图像处理</a></p>
<h3 id="AUC："><a href="#AUC：" class="headerlink" title="AUC："></a>AUC：</h3><p>Area Under Curve，指接收器操作特征曲线下的面积，可以用来衡量分类器的性能。</p>
<h3 id="face-landmarks："><a href="#face-landmarks：" class="headerlink" title="face landmarks："></a>face landmarks：</h3><p>直译为面部地标，作为一种匹配标准，用于在真实图像生成训练数据过程中实现寻找。</p>
<p>mask：相应区域的掩码。可以理解为图像的某种属性。表现在图像上，是表示图像中某区域内的像素情况，可以用数字来表示。</p>
<h3 id="SBI："><a href="#SBI：" class="headerlink" title="SBI："></a>SBI：</h3><p>Self-Blended Images，自混合图像，混合来自单个原始图像的伪源图像和目标图像。</p>
<h2 id="LLM"><a href="#LLM" class="headerlink" title="LLM:"></a>LLM:</h2><p>注：这部分内容几乎全部摘自人大高瓴出版的那本书。</p>
<h3 id="大语言模型"><a href="#大语言模型" class="headerlink" title="大语言模型"></a>大语言模型</h3><p>大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型。</p>
<p>如 GPT-3 [23]，PaLM [33] 和 LLaMA [34]。</p>
<p>目前大语言模型所需要具有的最小参数规模还没有一个明确的参考标准，但是大语言模型通常是指参数规模达到百亿、千亿甚至万亿的模型。</p>
<h3 id="大语言模型的能力"><a href="#大语言模型的能力" class="headerlink" title="大语言模型的能力"></a>大语言模型的能力</h3><p>具有较为丰富的世界知识、具有较强的通用任务解决能力、具有较好的复杂任务推理能力、具有较强的人类指令遵循能力。</p>
<h3 id="大语言模型的预训练"><a href="#大语言模型的预训练" class="headerlink" title="大语言模型的预训练"></a>大语言模型的预训练</h3><p>由于参数规模巨大，需要使用大规模分布式训练算法优化大语言模型的神经网络参数。</p>
<p>在训练过程中，需要联合使用各种并行策略以及效率优化方法，包括 3D 并行（数据并行、流水线并行、张量并行）、ZeRO（内存冗余消除技术）等。</p>
<p>为了有效支持分布式训练，很多研究机构发布了专用的分布式优化框架来简化并行算法的实现与部署，其中具有代表性的分布式训练软件包括DeepSpeed 和 Megatron-LM。</p>
<h3 id="3H对齐标准"><a href="#3H对齐标准" class="headerlink" title="3H对齐标准"></a>3H对齐标准</h3><p>实践应用中，需要保证大语言模型能够较好地符合人类的价值观</p>
<p>3H标准即 Helpfulness（有用性）Honesty（诚实性）Harmlessness（无害性）。</p>
<p>为了解决这一问题，OpenAI 提出了基于人类反馈的强化学习算法（Reinforcement Learning from Human Feedback, RLHF），将人类偏好引入到大模型的对齐过程中。</p>
<h3 id="涌现能力"><a href="#涌现能力" class="headerlink" title="涌现能力"></a>涌现能力</h3><p>指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。</p>
<h2 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h2><h3 id="软硬编码"><a href="#软硬编码" class="headerlink" title="软硬编码"></a>软硬编码</h3><p>硬编码：简单来说，<strong>硬编码就是将某些值或参数直接写入代码中，而不是通过外部配置、用户输入或程序运行时动态获取</strong>。</p>
<p>硬编码有缺乏灵活性、维护困难、可读性差、不利于扩展等一系列问题。<br>以下是一些常用的方法避免硬编码：</p>
<ul>
<li>使用配置文件：将需要变动的值写入配置文件，程序在运行时读取配置文件获取这些值。这样，当需要修改值时，只需修改配置文件而无需修改代码。</li>
<li>使用环境变量：环境变量是一种在程序运行时动态获取值的方式。通过环境变量，我们可以将配置信息与代码分离，提高代码的灵活性和可维护性。</li>
<li>使用数据库或外部服务：对于需要频繁变动的数据，可以考虑使用数据库或外部服务进行存储和管理。程序通过API接口获取数据，实现与数据的解耦。</li>
</ul>
<p> 硬编码与软件设计原则中的一些概念是相悖的。例如，开放封闭原则（OCP）强调软件实体（类、模块、函数等）应该对扩展开放，对修改封闭。而硬编码往往导致软件实体对修改不封闭，因为每次修改硬编码的值都需要直接修改代码。</p>
<p>​    此外，单一职责原则（SRP）也要求我们将功能拆分到不同的类或模块中，每个类或模块只负责一项职责。硬编码可能会使得某个类或模块承担过多的职责，降低代码的可读性和可维护性。</p>
<p> 因此，在编写代码时，我们应该尽量避免硬编码，遵循软件设计原则，提高代码的质量和可维护性。</p>
<p>​    </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://lightdust02.github.io/2024/01/05/Pron/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag">计算机视觉</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2024/01/11/Java%E5%AD%A6%E4%B9%A0/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Java学习
          
        </div>
      </a>
    
    
      <a href="/2024/01/03/BAOYAN/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">2023计算机保研全历程</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.staticfile.org/valine/1.4.16/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023-2024
        <i class="ri-heart-fill heart_icon"></i> LightDust
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="http://www.beian.miit.gov.cn/" target="_black" rel="nofollow">浙ICP备88888888</a>
        </li>
        
    </ul>
    <ul>
      
      <li>
          <img src="/images/beian.png"></img>
          <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=01234567890123" target="_black" rel="nofollow">浙公网安备01234567890123号</a>
      </li>
        
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="LightDust"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>