<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>常见概念-科研向 |  LightDust</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-知识点-Pron"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  常见概念-科研向
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/01/05/%E7%9F%A5%E8%AF%86%E7%82%B9-Pron/" class="article-date">
  <time datetime="2024-01-05T08:35:00.000Z" itemprop="datePublished">2024-01-05</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">14.6k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">53 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>这部分内容记录了一系列科研过程中常到的概念，包括深度学习、图像处理、计算机视觉、多模态等方面的内容。</p>
<p>该博文会一直持续更新。</p>
<span id="more"></span>





<h2 id="常见参数"><a href="#常见参数" class="headerlink" title="常见参数"></a>常见参数</h2><p>dim：通道数</p>
<p>num_heads：Transformer中注意力头数量</p>
<p>input_resolution：输入分辨率</p>
<p>mlp_ratio：mlp 中隐藏层维度与输入层维度比例</p>
<h2 id="DeepLearning"><a href="#DeepLearning" class="headerlink" title="DeepLearning"></a>DeepLearning</h2><h3 id="卷积操作："><a href="#卷积操作：" class="headerlink" title="卷积操作："></a>卷积操作：</h3><p>说白了，卷积操作是利用卷积核，通过卷积操作来实现对图像特征的提取。</p>
<p><img src="/images/Pron/%E5%8D%B7%E7%A7%AF.png" alt="卷积"></p>
<h4 id="一些名词："><a href="#一些名词：" class="headerlink" title="一些名词："></a>一些名词：</h4><p><img src="/images/Pron/%E4%B8%80%E4%BA%9B%E5%90%8D%E8%AF%8D.png" alt="一些名词"></p>
<h3 id="空洞卷积："><a href="#空洞卷积：" class="headerlink" title="空洞卷积："></a>空洞卷积：</h3><p><img src="/images/Pron/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.png" alt="空洞卷积"></p>
<p>空洞卷积（也称为膨胀卷积或Atrous卷积）是一种在卷积核中引入空洞（Dilation Rate）的卷积操作。空洞卷积通过在卷积核的元素之间插入空洞，扩大卷积核的感受野，而不增加参数量。</p>
<p><strong>作用</strong>:</p>
<ul>
<li><strong>扩大感受野</strong>:<ul>
<li>空洞卷积通过引入空洞，可以在不增加参数量和计算量的情况下，扩大卷积核的感受野。</li>
<li>例如，一个3×3的卷积核，如果Dilation Rate为2，则其感受野相当于5×5的卷积核。</li>
</ul>
</li>
<li><strong>保持分辨率</strong>:<ul>
<li>空洞卷积可以在不进行下采样的情况下，保持输入特征图的空间分辨率，同时扩大感受野。</li>
</ul>
</li>
</ul>
<p><strong>应用场景</strong>:</p>
<ul>
<li><strong>语义分割</strong>:<ul>
<li>在语义分割任务中，空洞卷积被广泛使用，因为它可以在不降低分辨率的情况下，扩大感受野，从而捕捉更多的上下文信息。</li>
<li>例如，DeepLab系列模型中使用了空洞卷积来提高分割精度。</li>
</ul>
</li>
<li><strong>目标检测</strong>:<ul>
<li>在目标检测任务中，空洞卷积可以用于扩大感受野，从而更好地捕捉目标的上下文信息。</li>
</ul>
</li>
</ul>
<h3 id="1×1卷积核"><a href="#1×1卷积核" class="headerlink" title="1×1卷积核"></a>1×1卷积核</h3><p>1×1卷积核（也称为点卷积）在深度学习中具有多种重要作用，尤其是在卷积神经网络（CNN）中。</p>
<p><strong>具体作用</strong>:</p>
<ul>
<li><strong>通道降维/升维</strong>:<ul>
<li>1×1卷积核可以在不改变图像空间分辨率的情况下，对输入特征图的通道数进行降维或升维。</li>
<li>例如，如果输入特征图的通道数为256，使用1×1卷积核可以将通道数减少到64（降维），或者增加到512（升维）。</li>
<li>这种操作可以减少计算量，同时保留空间信息。</li>
</ul>
</li>
<li><strong>跨通道信息融合</strong>:<ul>
<li>1×1卷积核可以在不同通道之间进行信息融合，从而增强特征的表达能力。</li>
<li>通过1×1卷积，模型可以学习到跨通道的线性组合，从而提取更丰富的特征。</li>
</ul>
</li>
<li><strong>非线性映射</strong>:<ul>
<li>在1×1卷积之后，通常会接一个非线性激活函数（如ReLU），从而引入非线性映射，增强模型的表达能力。</li>
</ul>
</li>
</ul>
<p><strong>应用场景</strong>:</p>
<ul>
<li><strong>GoogleNet中的 Inception模块</strong>:<ul>
<li>在GoogleNet中，1×1卷积核被广泛用于Inception模块中，用于减少计算量和通道数，同时增强特征表达。</li>
</ul>
</li>
<li><strong>ResNet中的瓶颈结构</strong>:<ul>
<li>在ResNet中，1×1卷积核用于瓶颈结构（Bottleneck），通过降维和升维操作，减少计算量并提高模型的效率。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="反卷积（Deconvolution）"><a href="#反卷积（Deconvolution）" class="headerlink" title="反卷积（Deconvolution）"></a>反卷积（Deconvolution）</h3><p>反卷积（也称为转置卷积或上采样卷积）是一种用于将低分辨率特征图上采样为高分辨率特征图的卷积操作。反卷积通过在输入特征图中插入零值（Zero Padding），然后进行卷积操作，从而实现上采样。</p>
<p><strong>作用</strong>:</p>
<ul>
<li><strong>上采样</strong>:<ul>
<li>反卷积的主要作用是将低分辨率的特征图上采样为高分辨率的特征图。</li>
<li>例如，在图像生成任务中，反卷积可以用于将低分辨率的特征图恢复为原始分辨率的图像。</li>
</ul>
</li>
<li><strong>恢复空间分辨率</strong>:<ul>
<li>在某些任务中（如语义分割），反卷积用于恢复输入图像的空间分辨率，从而生成与输入图像相同分辨率的输出。</li>
</ul>
</li>
</ul>
<p><strong>应用场景</strong>:</p>
<ul>
<li><strong>图像生成</strong>:<ul>
<li>在图像生成任务中（如GAN的生成器），反卷积用于将低分辨率的特征图上采样为高分辨率的图像。</li>
</ul>
</li>
<li><strong>语义分割</strong>:<ul>
<li>在语义分割任务中，反卷积用于将低分辨率的特征图恢复为原始分辨率的分割图。</li>
</ul>
</li>
<li><strong>超分辨率重建</strong>:<ul>
<li>在超分辨率重建任务中，反卷积用于将低分辨率的图像上采样为高分辨率的图像。</li>
</ul>
</li>
</ul>
<h3 id="channels："><a href="#channels：" class="headerlink" title="channels："></a>channels：</h3><p>也被称为通道。</p>
<p>灰度图都是<code>1 channel</code>，彩色图都是<code>3 channels(R、G、B)</code>。</p>
<p><strong>卷积核中</strong>的 <code>in_channels</code> 与需要进行卷积操作的数据的 <code>channels</code>一致。</p>
<p>如下图所示，从左到右大小分别为6×6×3，3×3×3，4×4×1.</p>
<p><img src="/images/Pron/channel.png" alt="channel"></p>
<h3 id="梯度发散："><a href="#梯度发散：" class="headerlink" title="梯度发散："></a>梯度发散：</h3><p>采用一些激活函数（如ReLu）时，会出现随着卷积层层数的增加而导致导数指数级增长，即出现梯度发散现象。可以通过引入shortcut来解决：</p>
<h3 id="shortcut："><a href="#shortcut：" class="headerlink" title="shortcut："></a>shortcut：</h3><p>翻译为“捷径”，为了解决梯度发散问题，在两层之间增加了（带权的）shortcut 。结构如图所示：</p>
<p><img src="/images/Pron/shortcut.png" alt="shortcut"></p>
<p>通过 shortcut 可以直接将浅层的信息传递到深层，可以解决退化问题。</p>
<h3 id="dim"><a href="#dim" class="headerlink" title="dim:"></a>dim:</h3><p>在深度学习中的卷积操作中，1D通常用于处理序列数据，2D用于处理图像数据，3D则用于处理体积数据（如视频或医学影像中的三维扫描）。</p>
<p>具体而言：</p>
<p><strong>1D</strong>: 输入为一维信号（如时间序列）。</p>
<p><strong>2D</strong>: 输入为二维的空间数据（如常见的图片，形状为 <code>[batch_size, channels, height, width]</code>），即经典的<code>[N C H W]</code></p>
<blockquote>
<p>注意，有的2D数据格式会是[N H W C]，这就需要在代码中做出调整。</p>
</blockquote>
<p><strong>3D</strong>: 输入为三维的体积数据（如视频，形状为 <code>[batch_size, channels, depth, height, width]</code>），即经典的<code>[N C D H W]</code></p>
<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33378444">pytorch的计算图 - 知乎 (zhihu.com)</a></p>
<h3 id="numpy操作"><a href="#numpy操作" class="headerlink" title="numpy操作"></a>numpy操作</h3><p>numpy中有很多很有趣的操作：</p>
<p>array.flatten()：展平array</p>
<p>np.transpose(a)：转置 a ，等效于 a.T</p>
<p>多维矩阵问题：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337829793">【全面理解多维矩阵运算】多维（三维四维）矩阵向量运算-超强可视化 - 知乎 (zhihu.com)</a></p>
<h3 id="shift操作"><a href="#shift操作" class="headerlink" title="shift操作"></a>shift操作</h3><p>其实就是<strong>数据的平移</strong>或<strong>位移变换</strong>……</p>
<p><strong>卷积神经网络</strong>中用于<strong>特征图的移位</strong>，例如用于处理时空序列数据的位移。</p>
<p>在<strong>Transformer模型</strong>中，shift操作用于序列的偏移或位移，以实现特定的自回归或掩码机制。</p>
<h3 id="detach"><a href="#detach" class="headerlink" title="detach()"></a>detach()</h3><p>返回一个与当前 graph 分离的、不再需要梯度的新张量。</p>
<h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax:"></a>softmax:</h3><p>用于解决多分类问题，可以预测每个类别的概率。</p>
<p><img src="/images/Pron/softmax.png" alt="softmax"></p>
<p>Softmax 层会对上一层的原始数据归一化，转化为一个 (0 , 1) 之间的数值，这些数值可以被当做概率分布，用来作为多分类的目标预测值。</p>
<p>Softmax 函数一般作为神经网络的最后一层，接受来自上一层网络的输入值，然后将其转化为概率。</p>
<h3 id="einsum"><a href="#einsum" class="headerlink" title="einsum:"></a>einsum:</h3><p>numpy 中一种优雅的矩阵矩阵计算方法，非常复杂。</p>
<p>具体参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/506843213">https://zhuanlan.zhihu.com/p/506843213</a></p>
<h3 id="消息传递神经网络（MPNN）"><a href="#消息传递神经网络（MPNN）" class="headerlink" title="消息传递神经网络（MPNN）"></a>消息传递神经网络（MPNN）</h3><p>消息传递神经网络（Message Passing Neural Network）是一类图神经网络，它通过在图的节点之间交换信息来学习节点的表示。它们基于以下步骤工作：</p>
<ol>
<li><strong>消息传递</strong>：每个节点接收其邻居节点的信息，并根据这些信息生成“消息”。</li>
<li><strong>聚合</strong>：将所有接收到的消息聚合成单个表示，这可以通过不同的函数实现，如求和、求平均或更复杂的操作。</li>
<li><strong>更新</strong>：使用聚合的信息来更新节点的状态。</li>
</ol>
<p>MPNN的核心思想是通过迭代这些步骤来精炼每个节点的表示，从而捕捉图的结构特征和节点之间的关系。</p>
<h3 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h3><p>GRU（gated recurrent unit）是循环神经网络（RNN）的一种变体，用于处理序列数据。与传统的RNN相比，GRU通过引入门控机制来解决梯度消失和梯度爆炸的问题，使得网络能够捕捉长距离依赖关系。GRU包含两个门：</p>
<ol>
<li><strong>更新门</strong>：决定状态信息应该如何更新。</li>
<li><strong>重置门</strong>：决定过去的状态信息在计算新状态时应保留多少。</li>
</ol>
<p>在每个时间步，GRU可以选择保留旧状态的信息并融入新输入的信息，这使得它在处理具有复杂依赖结构的数据时非常有效。</p>
<h3 id="EMA"><a href="#EMA" class="headerlink" title="EMA"></a>EMA</h3><p>EMA 模型，Exponential Moving Average model，通过计算模型参数的指数加权移动平均来平滑参数更新的过程。</p>
<p><img src="/images/Pron/ema.png" alt="image-20240325235724569"></p>
<p>EMA可以近似看成过去 $1/(1−\beta) $ 个时刻  v 值 的平均。</p>
<p>本质是对变量的一种加权平均。在训练过程中，原始模型的参数会不断更新，而 EMA 模型的参数则通过指数加权平均的方式来跟踪原始模型的参数变化。</p>
<h3 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h3><p><strong>前馈神经网络（FFN）</strong> 是每个 Transformer 层中的组成部分，它位于每次多头注意力操作之后，对从 Self-Attention 模块中获取的特征进行进一步处理。</p>
<p>FFN 由两个线性变换和一个激活函数（通常是 ReLU 或 GELU）组成。前馈神经网络中的非线性激活函数可以增强模型的表达能力。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, hidden_dim, dropout = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, hidden_dim),   <span class="comment"># 第一个线性层</span></span><br><span class="line">            nn.GELU(),                    <span class="comment"># 激活函数</span></span><br><span class="line">            nn.Dropout(dropout),          <span class="comment"># Dropout</span></span><br><span class="line">            nn.Linear(hidden_dim, dim),   <span class="comment"># 第二个线性层</span></span><br><span class="line">            nn.Dropout(dropout)           <span class="comment"># Dropout</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>



<p>gelu函数图像如下所示：<br><img src="/images/Pron/v2-149a8bc6ee90f65205a8d408f79a0017_1440w.webp" alt="GELU函数"></p>
<h3 id="NIAM"><a href="#NIAM" class="headerlink" title="NIAM"></a>NIAM</h3><blockquote>
<p>非常冷门……我怎么会写这个的……</p>
</blockquote>
<p>NIAM， Neural Implicit Attentive Model ，是一种与深度学习相关的模型。</p>
<p>一种基于注意力机制的神经网络模型，旨在处理具有隐式关系的数据。</p>
<p>隐式关系：不能直接观察到的关系，但可以通过数据间的相关性来推断。NIAM 模型可以自动地学习和捕捉这些隐式关系，从而在各种任务中表现出色，如推荐系统、自然语言处理和图像处理等。</p>
<h3 id="端到端end-to-end"><a href="#端到端end-to-end" class="headerlink" title="端到端end-to-end"></a>端到端end-to-end</h3><p>指从原始输入数据到最终输出结果的整个过程由一个统一的模型处理，即输入是原始数据，输出是最后的结果。</p>
<h3 id="2D-Feature-map"><a href="#2D-Feature-map" class="headerlink" title="2D Feature map"></a>2D Feature map</h3><p>通过卷积神经网络的卷积层和池化层处理后得到的二维矩阵。该矩阵包含了输入图像的某些特征。</p>
<h3 id="准确率、精确度、召回率、F1"><a href="#准确率、精确度、召回率、F1" class="headerlink" title="准确率、精确度、召回率、F1"></a>准确率、精确度、召回率、F1</h3><h4 id="准确率-Accuracy"><a href="#准确率-Accuracy" class="headerlink" title="准确率 Accuracy"></a>准确率 Accuracy</h4><p>正例和负例中预测正确数量占总数量的比例<br>$$<br>ACC=\frac{TP+TN}{TP+TN+FP+FN}<br>$$</p>
<h4 id="精确度-Precision"><a href="#精确度-Precision" class="headerlink" title="精确度 Precision"></a>精确度 Precision</h4><p>以<strong>预测结果</strong>为判断依据，预测为正例的样本中预测正确的比例<br>$$<br>Precision=\frac{TP}{TP+FP}<br>$$</p>
<h4 id="召回率-Recall"><a href="#召回率-Recall" class="headerlink" title="召回率 Recall"></a>召回率 Recall</h4><p>以<strong>实际样本</strong>为判断依据，实际为正例的样本中，被预测正确的正例占总实际正例样本的比例。<br>$$<br>Recall=\frac{TP}{TP+FN}<br>$$</p>
<h4 id="F1值"><a href="#F1值" class="headerlink" title="F1值"></a>F1值</h4><p>中和了精确率和召回率的指标<br>$$<br>F1=\frac{2PR}{R+P}<br>$$</p>
<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>变分自编码器 VAE（Variational auto-encoder），可以通过编码解码的步骤，直接比较重建图片和原始图片的差异。VAE以概率的方式描述对潜在空间的观察，在数据生成方面表现出了巨大的应用价值。VAE一经提出就迅速获得了深度生成模型领域广泛的关注。</p>
<p>VAE 会将给定输入的每个潜在特征表示为概率分布。当从潜在状态解码时，VAE 将<strong>从每个潜在状态分布中随机采样</strong>，生成一个向量作为解码器模型的输入。</p>
<p>VAE通过编码器将输入数据映射到一个概率分布，通常假设为高斯分布。不同于传统自编码器生成一个确定的隐变量，VAE生成的是一组分布参数（均值μ和方差σ），并使用这些参数从该分布中采样隐变量。解码器从这些隐变量中重构输入数据。</p>
<p>同时，在损失函数中，VAE引入了交叉熵与KL散度（是不是很熟悉？），KL散度能够确保编码器生成的潜在分布接近标准正态分布，保持潜在空间的连续性和可操作性。这有助于生成新的样本，保证潜在空间的结构合理。</p>
<p>具体步骤：</p>
<p>1). 输入数据 X 输入编码器，得到潜在空间的分布参数（均值和标准差）</p>
<p>2). 使用重参数化技巧从潜在分布中采样潜在变量 z</p>
<p>3). 将采样的 z 输入解码器，生成数据 </p>
<p>4). 计算损失函数，包括重构误差和KL散度</p>
<p>5). 使用反向传播最小化损失函数，更新网络参数。</p>
<p>相比GAN等生成模型，VAE的训练更稳定，模型结构更为清晰，易于理解。尽管VAE的生成质量通常不如GAN。</p>
<h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p><img src="/images/Pron/image-20240912110124560.png" alt="GAN示意图"></p>
<p>GAN于 2014 年提出。具体而言，GAN的核心结构是生成器与判别器。</p>
<p>生成器的功能是根据真实图像样本<code>X</code>来调整分布，逐步将随机噪声分布<code>z</code>调整为与原始训练数据相似的模拟数据分布，从而实现生成的模拟图像样本<code>X*</code>能够以假乱真。</p>
<p>判别器则负责检测输入数据的真伪，判断输入数据为真实图像<code>X</code>还是模拟图像<code>X*</code>，并将判断结果作为监督信息反馈给生成器，生成器根据判别器的反馈信息来调整图像的生成，使得模拟图像能够更好地“骗过”生成器。</p>
<p>在训练过程中，生成器与判别器交替训练，二者相互竞争、相互制约，最终会达到一种动态平衡。生成器得以在判别器的制约下生成更为真实的数据，判别器也能够通过判断生成器产生的伪造数据，不断提升其鉴别能力。</p>
<p><img src="/images/Pron/image-20240912110753701.png" alt="优化目标函数"></p>
<p><strong>生成器</strong>通常是一个前馈神经网络，它接收一个随机噪声向量并生成一组样本（例如图像或文本）。生成器学习到的映射是从一个简单分布（如正态分布）到数据分布的转变。</p>
<p><strong>判别器</strong>也是一个前馈神经网络，接收一组样本并输出一个概率值，表示该样本是真实数据还是生成的数据。判别器的训练目标是最大化其区分真实和伪造样本的能力。</p>
<p>GAN 的问题：</p>
<ul>
<li><p><strong>模式崩溃（Mode Collapse）</strong>: 生成器可能只生成少数几种样本，而无法覆盖数据分布的所有模式。</p>
</li>
<li><p><strong>训练不稳定</strong>: 生成器和判别器之间的对抗可能导致训练不稳定，难以收敛。</p>
</li>
<li><p><strong>计算成本高</strong>: GAN的训练通常需要大量的计算资源，尤其是在生成高分辨率图像时。</p>
</li>
</ul>
<h2 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h2><p><strong>ImageNet</strong> 是一个用于计算机视觉研究的<strong>大规模图像数据库</strong>。ImageNet 数据集包含超过<strong>1400 万张图像</strong>，这些图像涵盖了 20,000 多个类别，大部分图像都经过人工标注，每张图像都附带类别标签，有些还标注了图像中的物体边界框。</p>
<p>在图像超分辨率（Super-Resolution, SR）等图像处理任务中，使用 ImageNet 预训练模型具有以下优势：</p>
<ul>
<li><strong>泛化性强</strong>：因为 ImageNet 数据集涵盖了丰富的图像类别，预训练模型在不同任务中表现出很强的泛化能力。</li>
<li><strong>特征提取</strong>：预训练的卷积神经网络能够提取高质量的视觉特征，减少超分模型的训练时间。</li>
</ul>
<p>在图像超分中，训练超分模型通常需要大量高质量的图像数据，但构建大型数据集并进行高效训练是一个耗时且计算资源密集的过程。因此，许多研究者利用 <strong>ImageNet 预训练权重</strong> 来提升模型性能</p>
<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><p>[【论文笔记】Order Matters （AAAI 20） - Yuhan’s blog (yuhan2001.github.io)](<a target="_blank" rel="noopener" href="https://yuhan2001.github.io/2024/05/04/Order">https://yuhan2001.github.io/2024/05/04/Order</a> Matters/)</p>
<p>一些解读Transformer的文章：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/fengdu78/article/details/104629336">关于Transformer，面试官们都怎么问？-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/82391768https://zhuanlan.zhihu.com/p/82391768">关于Transformer的若干问题整理记录 - Adherer的文章</a></p>
<h3 id="一系列常见操作"><a href="#一系列常见操作" class="headerlink" title="一系列常见操作"></a>一系列常见操作</h3><h4 id="Proj-in"><a href="#Proj-in" class="headerlink" title="Proj_in"></a>Proj_in</h4><p>在Self-Attention部分，经常能看到 Proj_in ，投影操作。</p>
<p>投影操作用于将输入图像的特征维度从 <code>in_channels</code> 转换为 <code>inner_dim</code>。通过这种转换，可以将原始的图像数据嵌入到一个更丰富的特征空间，使其能够参与到更高维度的计算中。这个转换有助于捕捉更细致的特征</p>
<h4 id="上下文信息-context"><a href="#上下文信息-context" class="headerlink" title="上下文信息 context"></a>上下文信息 context</h4><p>在 transformer 结构中，<code>x</code> 通常代表当前层级（或模块）的输入特征，它通常是图像特征或编码特征。而 <code>context</code> 则表示上下文信息，通常用于指导或辅助主输入特征的计算。具体来说：</p>
<ul>
<li><code>x</code> 表示输入的图像特征，是网络的主输入。它包含了图像中的空间特征，例如像素信息、纹理等。</li>
<li><code>context</code> 则提供了额外的上下文特征。在跨模态任务（如图像-文本跨模态任务）中，<code>context</code> 可能来自于文本信息；在同模态任务中，<code>context</code> 可以是先前网络层生成的特征，或是不同区域的图像特征。这里的设计使用 <code>context</code> 来辅助输入特征的增强，这种方式可以提高模型在不同模态或特征组合中的表现能力。</li>
</ul>
<p>通过将 <code>x</code> 和 <code>context</code> 输入到 transformer block，可以将 <code>context</code> 的信息结合到 <code>x</code> 中，从而产生更丰富的输出特征。</p>
<h4 id="重要参数"><a href="#重要参数" class="headerlink" title="重要参数"></a>重要参数</h4><p><strong><code>n_heads</code>（注意力头数量）</strong>：表示在 transformer 中，输入数据将被分成多少个不同的注意力头来进行平行计算。多头注意力通过并行多个注意力头，可以学习数据的不同特征，同时增强注意力机制的表达能力。这样可以让模型更全面地捕捉到不同位置和特征之间的关系。</p>
<p><strong><code>d_head</code>（每个注意力头的特征维度）</strong>：表示每个头中查询、键和值的特征维度。这个参数控制了单个注意力头中特征向量的长度。通过调整 <code>d_head</code>，可以控制注意力计算的精度和每个头所学习的特征细节。</p>
<p><strong><code>inner_dim</code>（特征维度）</strong>：<code>inner_dim</code> 是通过 <strong><code>n_heads * d_head</code></strong> 计算得到的，即总的特征维度。这个维度表示所有注意力头的特征合并后的总特征数量。</p>
<h3 id="Self-Attention（qkv）"><a href="#Self-Attention（qkv）" class="headerlink" title="Self-Attention（qkv）"></a>Self-Attention（qkv）</h3><p>Q、K、V 就是Self-Attention机制的核心。</p>
<p>Q: Query，查询，查询其他元素与自己相关的程度。        </p>
<p>K: Key，键，表示序列中的每个元素特征，用来判断是否与其他元素（即Query）有关。</p>
<p>V: Value，值，代表序列中每个元素的实际信息。当Query与某个Key的相似度较高时，这个Value就会对Query的输出产生更大的影响。</p>
<p>在Self-Attention机制中，输入的每个元素都会产生对应的 Q、K、V，通过将查询 Q 与键 K 进行相关性计算，然后将这些相关性作为权重应用到值 V 上。</p>
<p>输入序列被用来计算查询 Q 、键 K 和值 V 。每个查询 Q 与所有键 K 计算得到相关性分数，然后通过 Softmax 函数将这些分数归一化为权重，最终这些权重被应用到相应的值 V 上，从而得到注意力表示。</p>
<p>Q、K 和 V 的作用可以简单概括如下：</p>
<ul>
<li><strong>Query（Q）</strong>：确定关注的内容，是用来“提问”的向量。它表示每个元素希望从其他元素中提取的信息，即当前元素与其他元素的相关性。</li>
<li><strong>Key（K）</strong>：提供与查询相关的上下文，是用来“回答”的向量。它表示每个元素本身的特征，其他元素的Query会根据这些Key来判断它们是否相关。</li>
<li><strong>Value（V）</strong>：是序列中的信息内容。Self-Attention 的最终输出就是根据Query和Key的相似度对这些Value进行加权求和后的结果。</li>
</ul>
<p>假设输入序列的维度是 $b×n×d$，其中 b 表示batch size，n 表示序列长度，d 表示每个元素的特征维度。</p>
<p>那么，Q K V 的维度均为 $b×n×d_k$。相关计算公式为 $\frac{QK^T}{\sqrt{d_k}}V$ ，$T$是转置操作，$d_k$是K的维度。</p>
<p><strong>除以$d_k$：在softmax之前除以$dk$ 可以将 $QK^T$ 的分布的方差纠正回接近1，这样一来，大部分数值都会分布在softmax梯度适当的位置，也就避免了梯度消失的问题。</strong></p>
<p><strong>大值的点积可能会导致 Softmax 进入梯度消失区域，因为极大的输入值会使 Softmax 函数的梯度接近 0。而通过除以 $\sqrt{d_k}$，可以避免进入这种极端情况，保持梯度稳定</strong></p>
<p>同时，由于add层（残差神经网络）与norm层（LayerNorm归一化），因此Encoder端和Decoder端每个子模块实际的输出为： $LayerNorm(x+Sublayer(x))$ ，其中 Sublayer(x)) 为子模块的输出。</p>
<h3 id="Linear-Attention"><a href="#Linear-Attention" class="headerlink" title="Linear-Attention"></a>Linear-Attention</h3><p><strong>Linear-Attention</strong> 是一种优化 Self-Attention 的变体，其核心思想是降低计算复杂度。</p>
<p>Linear-Attention 中，使用的是 <code>nn.Conv2d</code>从输入特征图中通过卷积层生成 <code>Q</code>、<code>K</code>、<code>V</code>，这表明它主要用于处理图像等二维数据。</p>
<p>同时，和 Self-Attention 的 $QK^T / \sqrt{d_k}$ 计算不同，Linear-Attention 通过 <code>einsum(&#39;bhdn,bhen-&gt;bhde&#39;, k, v)</code> 和 <code>einsum(&#39;bhde,bhdn-&gt;bhen&#39;, context, q)</code> 来进行计算。</p>
<ul>
<li>这里直接通过将 <code>K</code> 应用 <code>softmax</code>，然后将其与 <code>V</code> 通过 <code>einsum</code> 进行乘积，生成上下文信息。</li>
<li>后续的 <code>Q</code> 通过与上下文信息进行计算，生成输出。</li>
</ul>
<h3 id="Cross-Attention"><a href="#Cross-Attention" class="headerlink" title="Cross-Attention"></a>Cross-Attention</h3><p>Cross-Attention 用于处理两个不同输入之间的注意力计算。</p>
<p>Cross-Attention 在 Transformer 的编码器-解码器结构中有广泛应用，尤其是在任务中需要处理源数据和目标数据之间的依赖时，比如在机器翻译、图像生成等任务中。</p>
<p>在 Self-Attention 中，<code>Q</code>、<code>K</code> 和 <code>V</code> 来自同一个输入，而在 Cross-Attention 中，<code>Q</code> 和 <code>K</code>、<code>V</code> 可以来自不同的输入，常用于需要融合不同数据源的信息时。</p>
<p>Cross-Attention 的查询（Q）来自<strong>目标序列</strong>（通常是解码器的输入），而键（K）和值（V）来自源序列（通常是编码器的输出），公式仍然是和上面一致。</p>
<p>Cross-Attention 在机器翻译、图像生成、多模态等领域应用非常广泛。</p>
<h3 id="Muti-head-Attention"><a href="#Muti-head-Attention" class="headerlink" title="Muti-head-Attention"></a>Muti-head-Attention</h3><p>多头注意力机制在 Self-Attention 的基础上引入多个头（heads），即将注意力机制复制多次，每个注意力头使用不同的权重矩阵进行计算。这样做的目的是通过不同的子空间进行特征表示，从而捕捉不同层次的信息。</p>
<p>Q、K、V 通过不同的线性层，得到不同头的查询、键和值</p>
<p>$Q_i=W_i^QQ, K_i=W_i^KK,V_i=W_i^VV ,i=1,2,…h$ ，W是权重矩阵，h是头的数量。</p>
<p>然后，每个头都独立进行自注意力计算，得到每个头的注意力输出，公式同Self-Attention。</p>
<p>再将所有注意力头的输出拼接起来：</p>
<p>$MultiHead(Q,K,V)=Concat(head_1,head_2,…,head_h)$</p>
<p>最后再对拼接后的结果进行线性变换，得到最终的多头注意力输出：</p>
<p>$output=W^o⋅Concat(head_1,head_2,…,head_h)$</p>
<p> $W^O$  是输出层的权重矩阵。</p>
<p><strong>多头注意力机制保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。</strong></p>
<p>Decoder端的多头self-attention需要做mask，因为它在预测时，是“看不到未来的序列的”，所以要将当前预测的单词(token)及其之后的单词(token)全部mask掉。</p>
<h3 id="Spatial-transformer"><a href="#Spatial-transformer" class="headerlink" title="Spatial_transformer"></a>Spatial_transformer</h3><p>空间变换网络（Spatial Transformer Networks，简称STN）是一种深度学习模型，旨在增强网络对几何变换的适应能力。</p>
<p>在现代深度学习中，空间变换模块常常与自注意力机制结合使用，以便于模型能够关注输入数据的不同部分，并在处理过程中考虑这些部分之间的关系。</p>
<p>同时，在一些变体中，<code>SpatialTransformer</code> 可以结合条件输入（如上下文信息、标签等），使得生成的空间变换更具针对性和自适应性。</p>
<p>Diffusion中，SpatialTransformer非常常见。</p>
<h3 id="Vision-Transformer-ViT"><a href="#Vision-Transformer-ViT" class="headerlink" title="Vision Transformer(ViT)"></a>Vision Transformer(ViT)</h3><p>Vision Transformer（ViT）是将 Transformer 模型应用于图像分类的一种架构，旨在利用 Transformer 强大的全局建模能力来处理图像任务。</p>
<p>ViT 通过将图像分割成若干个小块<code>patch</code>，将它们看作序列输入到 Transformer 中，以捕捉全局依赖关系。</p>
<p>然后需要一项关键的内容：<strong>位置编码（Positional Embedding）</strong>，以保留图像的空间位置信息。</p>
<p>事实上，在最初的NLP领域，位置编码也是很重要的内容。位置编码一般采用正弦和余弦函数来生成<br>$$<br>PE_{pos,2i}=sin\frac{pos}{10000^{\frac{2i}{d_{model}}}}\<br>PE_{pos,2i+1}=cos\frac{pos}{10000^{\frac{2i}{d_{model}}}}<br>$$<br>其中 $d_{model}$ 是编码维度的总大小。</p>
<h4 id="Patch-embed-与-Patch-unembed"><a href="#Patch-embed-与-Patch-unembed" class="headerlink" title="Patch_embed 与 Patch_unembed"></a>Patch_embed 与 Patch_unembed</h4><p><code>PatchEmbed</code> 和 <code>PatchUnembed</code> 是两个关键的函数，分别用于将输入图像划分为多个小块（patches）以及将处理后的特征重新组合回图像。</p>
<p>这两个函数在 ViT 的架构中起到了桥梁的作用，帮助模型在图像和特征表示之间进行转换。</p>
<p><code>PatchEmbed</code> 的主要作用是将输入的图像划分为多个 patches ，并将这些 patches 展平为向量，然后通过线性变换将它们映射到嵌入空间。这个过程可以理解为将图像从像素空间转换为特征空间。</p>
<p><strong>输入：图像</strong>：形状为 <code>(batch_size, channels, height, width)</code>，例如 <code>(b, c, H, W)</code>。</p>
<p><strong>输出：嵌入特征</strong>：形状为 <code>(batch_size, num_patches, embed_dim)</code>，例如 <code>(b, N, C)</code>，其中 <code>N</code> 是图像被划分为的 patches 数量，<code>C</code> 是嵌入维度。</p>
<p>步骤如下所示：</p>
<ol>
<li><strong>图像划分</strong>：<ul>
<li>将输入图像划分为多个大小为 <code>patch_size x patch_size</code> 的小块（patches）。</li>
<li>例如，如果输入图像大小为 <code>(H, W)</code>，patch 大小为 <code>(P, P)</code>，则 patches 的数量为 <code>(H/P) * (W/P)</code>。</li>
</ul>
</li>
<li><strong>展平 patches</strong>：<ul>
<li>将每个 patch 展平为一个向量。</li>
<li>例如，一个 <code>(P, P, C)</code> 的 patch 会被展平为 <code>(P*P*C)</code> 的向量。</li>
</ul>
</li>
<li><strong>线性变换</strong>：<ul>
<li>使用一个线性层（全连接层）将展平后的 patches 映射到嵌入维度 <code>embed_dim</code>。</li>
<li>例如，如果展平后的 patch 向量维度为 <code>(P*P*C)</code>，则线性层将其映射为 <code>(embed_dim)</code>。</li>
</ul>
</li>
<li><strong>输出</strong>：<ul>
<li>最终输出为形状为 <code>(batch_size, num_patches, embed_dim)</code> 的嵌入特征</li>
</ul>
</li>
</ol>
<p><code>PatchUnembed</code> 的主要作用是将处理后的特征（通常是 Transformer 的输出）重新组合回图像空间。这个过程可以理解为将特征表示转换回像素空间，以便进行后续的图像重建或输出。</p>
<p><strong>输入：特征</strong>：形状为 <code>(batch_size, num_patches, embed_dim)</code>，例如 <code>(b, N, C)</code>。</p>
<p><strong>输出：图像</strong>：形状为 <code>(batch_size, channels, height, width)</code>，例如 <code>(b, 3, H, W)</code>。</p>
<p>核心步骤如下：</p>
<ol>
<li><strong>特征重塑</strong>：<ul>
<li>将输入特征从 <code>(batch_size, num_patches, embed_dim)</code> 重塑为 <code>(batch_size, embed_dim, H/P, W/P)</code>，其中 <code>H/P</code> 和 <code>W/P</code> 是 patches 的数量。</li>
</ul>
</li>
<li><strong>反向线性变换</strong>：<ul>
<li>使用一个反向线性层（通常是转置卷积或插值）将特征映射回原始图像的通道数和分辨率。</li>
</ul>
</li>
<li><strong>输出</strong>：<ul>
<li>最终输出为形状为 <code>(batch_size, channels, height, width)</code> 的图像。</li>
</ul>
</li>
</ol>
<h4 id="windows-partition"><a href="#windows-partition" class="headerlink" title="windows_partition"></a>windows_partition</h4><p>我经常会将 <code>windows_partition</code> 与 <code>Patch_embed</code> 相弄混，因此特地记录一下。</p>
<p><code>window_partition</code> 的功能是将输入的特征图划分为多个固定大小的窗口 windows，以便在每个窗口内进行局部注意力计算。这个函数通常用于 <strong>窗口注意力机制（Window Attention）</strong>，例如在 SwinTransformer 中，窗口注意力机制通过将输入特征划分为多个窗口来减少计算复杂度。</p>
<p><strong>输入：</strong></p>
<ul>
<li><strong>特征图</strong>：形状为 <code>(batch_size, height, width, channels)</code>，例如 <code>(b, H, W, C)</code>。</li>
<li><strong>窗口大小</strong>：一个整数或元组，表示窗口的高度和宽度，例如 <code>window_size = 16</code>。</li>
</ul>
<p><strong>输出</strong></p>
<ul>
<li><strong>窗口特征</strong>：形状为 <code>(num_windows * batch_size, window_size, window_size, channels)</code>，例如 <code>(nw * b, P, P, C)</code>，其中 <code>nw</code> 是窗口的数量。</li>
</ul>
<p><strong>核心步骤</strong></p>
<ol>
<li><strong>窗口划分</strong>：<ul>
<li>将输入特征图划分为多个大小为 <code>window_size x window_size</code> 的窗口。</li>
<li>例如，如果输入特征图大小为 <code>(H, W)</code>，窗口大小为 <code>(P, P)</code>，则窗口的数量为 <code>(H/P) * (W/P)</code>。</li>
</ul>
</li>
<li><strong>重塑和排列</strong>：<ul>
<li>将划分后的窗口重塑为一个新的张量，形状为 <code>(num_windows * batch_size, window_size, window_size, channels)</code>。</li>
<li>例如，如果输入特征图大小为 <code>(H, W)</code>，窗口大小为 <code>(P, P)</code>，则输出形状为 <code>(nw * b, P, P, C)</code>。</li>
</ul>
</li>
</ol>
<h3 id="FFN-1"><a href="#FFN-1" class="headerlink" title="FFN"></a>FFN</h3><p>前馈神经网络是一种最简单的神经网络结构，数据从输入层流向输出层，中间经过一个或多个隐藏层，且信息流动是单向的（没有循环或反馈）。</p>
<h3 id="Bert："><a href="#Bert：" class="headerlink" title="Bert："></a>Bert：</h3><p>Bert，Bidirectional Encoder Representations from Transformers，是一种基于 Transformer 结构的语言模型，用来生成文本的深层双向表示。它<strong>彻底</strong>改变了自然语言处理（NLP）领域的研究方式。</p>
<p>BERT 的核心创新之一是双向编码器。在传统的语言模型  LSTM 或 RNN中，模型会顺序从左到右或从右到左预测下一个词。而 BERT 是通过同时考虑句子的左右上下文来学习词汇的含义。这种双向性使得 BERT 能够理解一个词或子词在句子中的更深层次的语义关系。</p>
<p><strong>Transformer架构</strong>：BERT 基于 Transformer 编码器，依赖于注意力机制（Self-Attention）来捕捉句子中不同词之间的关系。BERT 只使用 Transformer 的 Encoder 部分，它并不生成文本，而是通过理解现有文本进行分类或问答等任务。</p>
<p>传统语言模型中，单向上下文（仅前向或后向）导致了信息丢失。而BERT 的成功归功于两种新的预训练任务：</p>
<p>1). <strong>Masked Language Model (MLM)<strong>： 传统的语言模型通过预测下一个词来学习，而 BERT 使用了一种称为 Masked Language Model 的技术。在 MLM 中，</strong>输入的句子会随机遮盖（mask）一些词</strong>（通常是 15%），要求模型预测这些掩盖单词的原始值来实现。模型的目标是<strong>在训练过程中预测出这些被遮盖的词</strong>。</p>
<p>由于模型在训练时看不到某些词，它被迫通过句子的上下文来推测缺失的词，这有助于模型更好地理解词汇的语义和上下文关系。</p>
<p>例如： 输入句子：<code>[CLS] The cat sits on the [MASK]. [SEP]</code> 模型的任务是预测 <code>[MASK]</code> 的正确词（如 “mat”）。</p>
<p>这种训练方式使得BERT能够有效地学习单词的上下文关系和语义信息，从而更好地理解语言。</p>
<p>2). <strong>Next Sentence Prediction (NSP)<strong>： 除了预测遮盖的词，BERT 还引入了 Next Sentence Prediction 任务。这种任务的目标是</strong>使模型能够理解句子之间的关系</strong>。<strong>预测给定的两句话是否是连续的</strong>。通过这个任务，BERT 可以学会句子间的逻辑关系，从而在处理句子对相关的任务（如问答、自然语言推理等）时表现得更好。</p>
<p>具体来说，BERT 会接收两个句子，模型需要判断第二句话是否是第一句话的连续句。例如：</p>
<ul>
<li>输入：<code>[CLS] The cat is on the mat. [SEP] It is sleeping. [SEP]</code></li>
<li>输出：True（句子连贯）</li>
</ul>
<p>通过这种方式，BERT学习理解句子之间的逻辑和关系，增强对文本的整体理解能力。</p>
<h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h3 id="BEV"><a href="#BEV" class="headerlink" title="BEV:"></a>BEV:</h3><p><strong>BEV</strong>: 鸟瞰视角，Bird’s Eye View，是一种<strong>从上方观看</strong>对象或场景的视角，就像鸟在空中俯视地面一样。在自动驾驶和机器人领域，通过传感器（如 LiDAR 和摄像头）获取的数据通常会被转换成 BEV 表示，以便更好地进行<strong>物体检测、路径规划</strong>等任务。</p>
<h3 id="FPN："><a href="#FPN：" class="headerlink" title="FPN："></a>FPN：</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62604038">【目标检测】FPN(Feature Pyramid Network) - 知乎 (zhihu.com)</a></p>
<blockquote>
<p>整理完下面这些发现其实只看这一篇文章就够了。</p>
</blockquote>
<p><strong>FPN</strong>: 特征金字塔网络，Feature Pyramid Network。</p>
<p>传统方法使用图像金字塔，将图片 resize 到不同的大小，然后分别得到对应大小的特征，然后进行预测。这种方法可以一定程度上解决多尺度的问题，但带来的计算量也非常大。如下图所示：</p>
<p><img src="/images/Pron/%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94.jpg" alt="图像金字塔"></p>
<p>也有使用单个feature map进行检测，在网络的最后一层的特征图上进行预测。优点是计算速度快，缺点是最后一层的特征图分辨率低，不能准确的包含物体的位置信息。直接使用这种架构导致预测层的特征尺度比较单一，对小目标检测效果比较差。如下图所示：</p>
<p><img src="/images/Pron/featuremap.jpg" alt="featuremap"></p>
<p>为了使得不同尺度的特征都包含丰富的语义信息，同时又不使得计算成本过高，FPN 采用top down和lateral connection的方式，让低层高分辨率低语义的特征和高层低分辨率高语义的特征融合在一起。</p>
<p><img src="/images/Pron/FPN.jpg" alt="FPN"></p>
<p>三个结构：</p>
<p>Bottom-up：Bottom-up 的过程就是将图片输入到backbone ConvNet中提取特征的过程中。Backbone输出的feature map的尺寸有的是不变的，有的是成2倍的减小的。</p>
<p>Top-down：Top-down 的过程就是将高层得到的feature map进行上采样然后往下传递。高层的特征包含丰富的语义信息，经过top-down 这些语义信息能传播到低层特征上，使得低层特征也包含丰富的语义信息。</p>
<p>Lateral connection：3 个步骤，先 1*1的卷积降低维度，再将得到的特征和上一层采样得到特征图 $P_{n+1}$ 进行融合，直接相加。相加完后再进行一个 3*3 的卷积得到本层的特征输出 $P_n$。</p>
<h3 id="常见模型简称："><a href="#常见模型简称：" class="headerlink" title="常见模型简称："></a>常见模型简称：</h3><p>LC-GAN : 基于生成对抗网络的人脸数据隐私保护算法模型</p>
<p>CGAN : 条件生成对抗网络，是一种基于监督学习的神经网络深度学习模型</p>
<p>PP-GAN : 隐私保护网络，基于生成对抗网络，引入了针对人脸去识别问题而设计的验证器和调节模块</p>
<h3 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h3><p>Contrastive Language-Image Pre-Training 模型。一种多模态预训练神经网络，由OpenAI在2021年发布，是从自然语言监督中学习的一种有效且<strong>可扩展</strong>的方法。</p>
<p>CLIP模型有两个模态，一个是文本模态，一个是视觉模态，包括两个主要部分：</p>
<ol>
<li>Text Encoder：用于将文本转换为低维向量表示-Embeding。</li>
<li>Image Encoder：用于将图像转换为类似的向量表示-Embedding</li>
</ol>
<p>CLIP 通过计算文本和图像向量之间的<strong>余弦相似度</strong>来生成预测。这种模型特别适用于<strong>零样本学习</strong>任务，即模型不需要看到新的图像或文本的训练示例就能进行预测。CLIP模型在多个领域表现出色，如图像文本检索、图文生成等。</p>
<h3 id="Mamba"><a href="#Mamba" class="headerlink" title="Mamba"></a>Mamba</h3><blockquote>
<p>Mamba Out!</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/134923301">一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba_mamba模型-CSDN博客</a></p>
<h3 id="RRDB"><a href="#RRDB" class="headerlink" title="RRDB"></a>RRDB</h3><p><img src="/images/Pron/image-20241118144746361.png" alt="RRDB结构图"></p>
<p>2018年提出。RRDB（Residual in Residual Dense Block）是 ESRGAN（Enhanced Super-Resolution Generative Adversarial Network）中的一个核心模块，用于高质量图像超分辨率任务。它基于传统的残差网络（ResNet）和密集网络（DenseNet）的思想，进一步提升了模型的性能和稳定性。</p>
<p>RRDB 的主要特点是 <strong>“残差中的残差结构”</strong>，整体结构如下：</p>
<ul>
<li><strong>外层残差结构</strong>：<ul>
<li>RRDB 是一个残差块（Residual Block），整个块的输入会直接通过跳跃连接（skip connection）加到输出上。</li>
<li>跳跃连接缓解了深度网络中的梯度消失问题，增强了训练稳定性。</li>
</ul>
</li>
<li><strong>内部残差结构</strong>：<ul>
<li>每个 RRDB 内包含多个密集块（Dense Block），每个密集块通过 <strong>密集连接</strong> 提高了信息流动效率。</li>
<li>在这些 Dense Block 之间存在递归式的残差连接（即 “残差中的残差”），这是一种更深层次的信息融合方式。</li>
</ul>
</li>
<li><strong>无批归一化（Batch Normalization）</strong>：<ul>
<li>ESRGAN 中去除了传统的 Batch Normalization（BN）层，因为研究表明 BN 会导致生成模型的失真，从而影响图像质量。</li>
</ul>
</li>
</ul>
<p>一个典型的 RRDB 由以下部分组成：</p>
<p><strong>1 Dense Block</strong></p>
<ul>
<li>每个 Dense Block 包括多个卷积层（通常为 3-5 层），每一层接收前面所有层的输出作为输入。</li>
<li>公式表示： $x_{l+1} = H_l([x_0, x_1, …, x_l])$,，其中，$x_0, x_1, …, x_l$ 表示前面所有层的特征图拼接，$H_l$ 是第 l层的卷积操作。</li>
</ul>
<p><strong>2 残差连接</strong></p>
<ul>
<li>Dense Block 的输出与输入通过残差连接相加： $y = x + G(x)$  其中 $G(x)$ 是 Dense Block 的输出。</li>
</ul>
<p><strong>3 残差中的残差</strong></p>
<ul>
<li>多个 Dense Block 被串联起来，每个 Dense Block 都有自己的残差连接。</li>
<li>整个 RRDB 的输入会通过外层残差连接加到输出上：$y = x + G_{\text{RRDB}}(x)$, 其中 $G_{\text{RRDB}}(x)$ 是多个 Dense Block 的组合输出。</li>
</ul>
<p><strong>4 无归一化</strong></p>
<ul>
<li>RRDB 中直接使用 ReLU 激活函数而不使用 BN 层，这样可以避免生成模型中 BN 引入的偏移问题。</li>
</ul>
<h3 id="SwinIR"><a href="#SwinIR" class="headerlink" title="SwinIR"></a>SwinIR</h3><p>2021年提出，<strong>SwinIR  (Swin Transformer for Image Restoration)</strong> 是一种基于 Swin Transformer 的高效模型，专门用于图像超分辨率（Super-Resolution）、去噪（Denoising）、去模糊（Deblurring）等图像恢复任务。它通过 Transformer 的强大建模能力捕获图像中的长程依赖关系，同时通过滑动窗口机制提高效率，是近年来图像恢复领域的代表性模型之一。</p>
<p><img src="/images/Pron/image-20241118154419267.png" alt="SwinIR结构图"></p>
<ol>
<li><p><strong>浅层特征提取模块</strong>（Shallow Feature Extractor）。</p>
</li>
<li><p>深层特征提取模块</p>
<p>（Deep Feature Extractor）：</p>
<ul>
<li>使用了多个 Swin Transformer 块（STL）。</li>
</ul>
</li>
<li><p>重建模块</p>
<p>（Reconstruction Module）：</p>
<ul>
<li>将提取的特征映射为最终输出图像。</li>
</ul>
</li>
</ol>
<p><strong>2.2 模块细节</strong></p>
<p><strong>(1) 浅层特征提取模块</strong></p>
<ul>
<li>通过一个卷积层提取输入图像的初始特征。</li>
</ul>
<p>$F_0 = H_{\text{conv}}(I_{\text{LR}})$</p>
<p>其中，$I_{\text{LR}}$ 是低分辨率输入图像，$F_0$ 是提取的初始特征。</p>
<hr>
<p><strong>(2) 深层特征提取模块</strong></p>
<ul>
<li>SwinIR 的核心部分，包含多个 Swin Transformer Layer。</li>
<li>每个 Swin Transformer Layer 的基本单元包括：<ul>
<li><strong>窗口多头自注意力机制（Window-based Multi-head Self-Attention, W-MSA）</strong>：对局部窗口中的像素进行自注意力计算。</li>
<li><strong>滑动窗口机制（Shifted Window Mechanism, SW-MSA）</strong>：通过移动窗口位置，实现跨窗口的全局信息交互。</li>
</ul>
</li>
</ul>
<p>多层 Swin Transformer 层堆叠后，可以有效捕获局部和全局信息。</p>
<hr>
<p><strong>(3) 重建模块</strong></p>
<ul>
<li>最后的卷积层对深层特征进行处理，生成高分辨率或恢复后的图像。</li>
<li>在超分辨率任务中，通常使用 $PixelShuffle$ 来实现图像上采样。</li>
</ul>
<hr>
<h3 id><a href="#" class="headerlink" title></a></h3><p><strong>1 Swin Transformer 的窗口机制</strong></p>
<ul>
<li><p><strong>局部窗口自注意力（W-MSA）</strong>：输入图像被划分为多个固定大小的窗口（如 $8\times8$），在每个窗口内计算自注意力。</p>
</li>
<li><p>$\text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$</p>
<p>这样可以减少计算开销，使其与图像分辨率呈线性关系。</p>
</li>
<li><p><strong>滑动窗口机制（Shifted Window Mechanism）</strong>：</p>
<ul>
<li>为了解决窗口之间的独立性问题，SwinIR 将窗口在某些层中移动一部分像素，使得特征能够跨窗口交互。</li>
</ul>
</li>
</ul>
<p><strong>2 SwinIR 的信息交互</strong></p>
<ul>
<li>Swin Transformer 提取的局部窗口特征通过滑动窗口实现全局特征的逐层融合，进而捕获图像中的长程依赖和纹理信息。</li>
<li>在深层模块中，交替堆叠 W-MSA 和 SW-MSA，使得 SwinIR 的感受野逐渐扩大。</li>
</ul>
<h3 id="边缘监测："><a href="#边缘监测：" class="headerlink" title="边缘监测："></a>边缘监测：</h3><h4 id="Canny-Edge"><a href="#Canny-Edge" class="headerlink" title="Canny Edge"></a>Canny Edge</h4><p>一种经典的图像边缘检测算法。其核心思想是通过多阶段处理来提取图像的边缘，步骤包括高斯滤波、计算梯度、非极大值抑制以及双阈值检测。</p>
<h4 id="HED-Edge"><a href="#HED-Edge" class="headerlink" title="HED Edge"></a>HED Edge</h4><p>一种基于卷积神经网络（CNN）的边缘检测方法，旨在从图像中提取边缘信息。</p>
<h4 id="MLSD-Edge"><a href="#MLSD-Edge" class="headerlink" title="MLSD Edge"></a>MLSD Edge</h4><p>一种轻量化的线段检测方法，通常用于移动端或资源受限的设备上，专注于检测图像中的直线段。这种方法非常适合用于场景理解、几何建模以及三维重建等任务。</p>
<h4 id="MIDAS-Depth-and-Normal"><a href="#MIDAS-Depth-and-Normal" class="headerlink" title="MIDAS Depth and Normal"></a>MIDAS Depth and Normal</h4><p>一种基于神经网络的深度估计模型，能够从单张图像推测其深度信息（即每个像素距离相机的远近）。</p>
<h4 id="Openpose"><a href="#Openpose" class="headerlink" title="Openpose"></a>Openpose</h4><p>一种基于深度学习的人体姿态估计模型，它能够从图像或视频中检测人体的关节点（如头、肩、肘、膝等），并绘制骨架结构。</p>
<h4 id="Uniformer-Segmentation"><a href="#Uniformer-Segmentation" class="headerlink" title="Uniformer Segmentation"></a><strong>Uniformer Segmentation</strong></h4><p>一种结合 Transformer 和 CNN 的图像分割模型，旨在统一处理局部和全局信息。</p>
<p>通过 UniFormer 架构，图像分割模型能够更好地捕捉图像的全局上下文信息，同时保持对局部细节的敏感度。这种架构在语义分割任务中表现出色，能够有效地区分不同物体或区域。</p>
<h2 id="Diffusion"><a href="#Diffusion" class="headerlink" title="Diffusion"></a>Diffusion</h2><h3 id="LDM"><a href="#LDM" class="headerlink" title="LDM"></a>LDM</h3><p>Latent Diffusion Model，潜在扩散模型，一种生成模型，结合了扩散模型和潜在空间的思想，用于生成高质量的图像。通过逐步对噪声数据进行去噪，从而生成逼真的图像。</p>
<p>LDM 将数据x投射到一个潜在空间中，然后在这个潜在空间中进行扩散过程得到结果z，最后再将结果z映射回原始数据空间。</p>
<p>关键函数：ddpm.py中的 decode_first_stage 函数与 encode_first_stage 函数，get_first_stage_encoding函数</p>
<p>(这个代码不太好懂……慢慢来看)</p>
<h3 id="kl散度"><a href="#kl散度" class="headerlink" title="kl散度"></a>kl散度</h3><p>简而言之，kl散度用于计算两个分布之间的差异。<br>$$<br>D_{KL}(p∥q)= \sum_{i=1}^{n}p(x_i)log(\frac{p(x_i)}{q(x_i)})<br>$$<br>LDM中，normal_kl一般用于计算两个正态分布之间的差异。</p>
<p>LDM 会对数据进行潜在空间的编解码，而潜在空间中通常使用高斯分布对数据进行建模，因此对高斯分布进行采样、计算对数似然（nll）以及 KL 散度都是训练 LDM 时非常关键的操作。</p>
<h3 id="nll"><a href="#nll" class="headerlink" title="nll"></a>nll</h3><p>负对数似然损失（Negative Log-Likelihood，NLL），整个数据集的负对数似然损失就是对所有样本的损失进行求和或平均。<br>$$<br>NLL=\frac{1}{2}\sum_i[log(2\pi)+log(\sigma_i^2)+\frac{(x_i-\mu_i)^2}{\sigma_i^2}]<br>$$</p>
<h2 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h2><h3 id="上采样与下采样："><a href="#上采样与下采样：" class="headerlink" title="上采样与下采样："></a>上采样与下采样：</h3><p>上采样（Upsampling）会增加数据的空间分辨率，通过插值或其他方法填充更多的样本点。它可以用于恢复或生成更高分辨率的图像。</p>
<p>上采样会<strong>放大图像数据，增加图像的空间分辨率</strong>，使得输出图像的尺寸大于输入图像的尺寸。</p>
<p>上采样一般通过插值、转置卷积、PixelShuffle实现。</p>
<p>下采样（Downsampling）是指减少数据的空间分辨率，通常通过减少样本的数量来实现。它可以用于降低计算负担，减少模型的内存占用，或者在某些情况下减少噪声。</p>
<p>下采样会<strong>缩小图像数据，减少图像的空间分辨率</strong>，使得输出图像的尺寸小于输入图像的尺寸。</p>
<p>下采样一般通过池化与空洞卷积实现。</p>
<p>Unet中先是下采样，再是上采样。</p>
<h3 id="噪声："><a href="#噪声：" class="headerlink" title="噪声："></a>噪声：</h3><p>图像噪声是指存在于图像数据中的<strong>不必要的或多余的干扰信息</strong>。</p>
<h3 id="仿射变换："><a href="#仿射变换：" class="headerlink" title="仿射变换："></a>仿射变换：</h3><p>Affine Transformation，线性变换与平移变化的叠加。</p>
<p>包括<strong>缩放</strong>(Scale)、<strong>平移</strong>(transform)、<strong>旋转</strong>(rotate)、<strong>反射</strong>（reflection）、<strong>错切</strong>(shear mapping）</p>
<p><img src="/images/Pron/affine.png" alt="affine transformations"></p>
<h3 id="DCT变换："><a href="#DCT变换：" class="headerlink" title="DCT变换："></a><strong>DCT变换</strong>：</h3><p>Discrete Cosine Transform，离散余弦变换。</p>
<p>离散余弦变换经常被信号处理和图像处理使用，<strong>用于对信号和图像(包括静止图像和运动图像)进行有损数据压缩</strong>。这是由于离散余弦变换具有很强的”能量集中”特性:大多数的自然信号(包括声音和图像)的能量都集中在离散余弦变换后的低频部分，而且当信号具有接近马尔科夫过程的统计特性时，离散余弦变换的去相关性接近于K-L变换 (Karhunen-Loève 变换，具有最优的去相关性) 的性能。</p>
<h3 id="全变分模型（Total-Variation-TV）"><a href="#全变分模型（Total-Variation-TV）" class="headerlink" title="全变分模型（Total Variation, TV）"></a>全变分模型（Total Variation, TV）</h3><p>图像处理领域中常用的一种正则化方法，尤其在图像去噪、图像修复和图像重建问题中广泛应用。</p>
<p>核心思想是通过最小化图像的全变分来保留图像中的边缘细节，同时去除噪声或平滑不必要的纹理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">total_variation</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Anisotropic TV.&quot;&quot;&quot;</span></span><br><span class="line">    dx = torch.mean(torch.<span class="built_in">abs</span>(x[:, :, :, :-<span class="number">1</span>] - x[:, :, :, <span class="number">1</span>:]))</span><br><span class="line">    dy = torch.mean(torch.<span class="built_in">abs</span>(x[:, :, :-<span class="number">1</span>, :] - x[:, :, <span class="number">1</span>:, :]))</span><br><span class="line">    <span class="keyword">return</span> dx + dy</span><br></pre></td></tr></table></figure>



<h3 id="量化："><a href="#量化：" class="headerlink" title="量化："></a><strong>量化</strong>：</h3><p>将图像像素点对应亮度的连续变换区间转换为单个特定值的过程，即<strong>将原始灰度图像的空间坐标幅度值离散化</strong>。量化等级越多，图像层次越丰富，灰度分辨率越高，图像的质量也越好；量化等级越少，图像层次欠丰富，灰度分辨率越低。</p>
<p>量化后，图像就被表示成一个整数矩阵，每个像素具有两个属性：位置和灰度。位置由行，列表示。灰度表示该像素位置上亮暗程度的整数。此数字矩阵M*N就作为计算机处理的对象了，灰度级一般为0-255（8bit量化）。如果量化等级为2，则将使用两种灰度级表示原始图像的像素（0-255），灰度值小于128的取0，大于等于128的取128；如果量化等级为4，则将使用四种灰度级表示原始图像的像素，新图像将分层为四种颜色，0-64区间取0,64-128区间取64,128-192区间的取128,192-255区间取192，依次类推。</p>
<p><a target="_blank" rel="noopener" href="http://t.zoukankan.com/wj-1314-p-12191084.html">OpenCV计算机视觉学习（12）——图像量化处理&amp;图像采样处理（K-Means聚类量化，局部马赛克处理） - 走看看 (zoukankan.com)</a></p>
<h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE:"></a>MSE:</h3><p>MSE , Mean Square Error , 均方误差，是衡量图像质量的指标之一。</p>
<p>原理为<strong>真实值与预测值的差值的平方然后求和再平均</strong></p>
<p><img src="/images/Pron/M.png" alt="MSE"></p>
<h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR:"></a>PSNR:</h3><p>PSNR , Peak Signal-to-Noise Ratio 峰值信噪比，定义为</p>
<p><img src="/images/Pron/psnr.png" alt="PSNR"></p>
<p>其中 $MAX_{I}^2$ 为图片可能的最大像素值。如果每个像素都由 8 位二进制来表示，那么就为 255。</p>
<p>这是针对灰度图像的计算方法，如果彩色图像，通常有三种方法来计算。</p>
<ul>
<li>分别计算 RGB 三个通道的 PSNR，然后取平均值。</li>
<li>计算 RGB 三通道的 MSE ，然后再除以 3 。</li>
<li>将图片转化为 YCbCr 格式，然后只计算 Y 分量也就是亮度分量的 PSNR。</li>
</ul>
<p>常见的是第二种与第三种。</p>
<h3 id="一系列评价指标："><a href="#一系列评价指标：" class="headerlink" title="一系列评价指标："></a>一系列评价指标：</h3><p>$\uparrow$ 表示越高效果越好</p>
<table>
<thead>
<tr>
<th><strong>信息熵</strong></th>
<th><strong>EN</strong></th>
<th>衡量图像中信息量的度量，表示图像复杂性</th>
</tr>
</thead>
<tbody><tr>
<td><strong>空间频率</strong></td>
<td><strong>SF</strong></td>
<td>衡量图像细节和纹理变化的频率，反映图像的锐度</td>
</tr>
<tr>
<td><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E6%A0%87%E5%87%86%E5%B7%AE&zhida_source=entity&is_preview=1">标准差</a></strong></td>
<td><strong>SD</strong></td>
<td>图像像素强度的标准差，衡量图像的对比度。</td>
</tr>
<tr>
<td><strong>峰值信噪比</strong></td>
<td><strong>PSNR</strong></td>
<td>衡量图像重建质量的指标，用于比较原始图像和重建图像之间的差异</td>
</tr>
<tr>
<td><strong>均方误差</strong></td>
<td><strong>MSE</strong></td>
<td>衡量图像重建误差的指标，计算原始图像与重建图像之间的均方误差</td>
</tr>
<tr>
<td><strong>互信息</strong></td>
<td><strong>MI</strong></td>
<td>衡量两幅图像之间共享信息的量，用于图像配准或融合。</td>
</tr>
<tr>
<td><strong>视觉保真度</strong></td>
<td><strong>VIF</strong></td>
<td>根据人眼感知模型评估图像质量的指标，衡量图像保留的视觉信息量。</td>
</tr>
<tr>
<td><strong><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%B9%B3%E5%9D%87%E6%A2%AF%E5%BA%A6&zhida_source=entity&is_preview=1">平均梯度</a></strong></td>
<td><strong>AG</strong></td>
<td>衡量图像梯度的平均值，表示图像的清晰度。</td>
</tr>
<tr>
<td><strong>相关系数</strong></td>
<td><strong>CC</strong></td>
<td>衡量两幅图像的相似度，常用于图像匹配和重建。</td>
</tr>
<tr>
<td><strong>差异相关和</strong></td>
<td><strong>SCD</strong></td>
<td>衡量图像结构的差异，评估图像之间的结构相似性。</td>
</tr>
<tr>
<td><strong>基于梯度的融合性能</strong></td>
<td><strong>F</strong></td>
<td>基于图像特征相似性的质量评估指标，考虑边缘和纹理特征。</td>
</tr>
<tr>
<td><strong>结构相似度测量</strong></td>
<td><strong>SSIM</strong></td>
<td>衡量两幅图像的结构相似度，考虑亮度、对比度和结构信息。</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%BB%93%E6%9E%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%B5%8B%E9%87%8F&zhida_source=entity&is_preview=1">多尺度结构相似度测量</a></td>
<td><strong>MS-SSIM</strong></td>
<td>SSIM 的多尺度扩展版本，综合不同尺度的图像质量。</td>
</tr>
<tr>
<td><strong>基于噪声评估的<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?q=%E8%9E%8D%E5%90%88%E6%80%A7%E8%83%BD&zhida_source=entity&is_preview=1">融合性能</a></strong></td>
<td><strong>Nabf</strong></td>
<td>一种无参考的图像质量评估指标，基于图像的特征提取</td>
</tr>
</tbody></table>
<p>性能评估指标主要分为四类，分别是<strong>基于信息论的评估指标</strong>，主要包括 EN、MI、PSNR、基于结构相似性的评估指标<strong>，主要包括</strong>SSIM、MS_SSIM、MSE<strong>、</strong>基于图像特征的评估指标<strong>， 主要包括</strong>SF、SD、AG<strong>，</strong>基于人类视觉感知的评估指标<strong>，主要包括</strong>VIF<strong>、</strong>以及基于源图像与生成图像的评估指标<strong>，主要包括</strong>CC、SCD、Qabf、Nabf**。</p>
<h3 id="空域图像："><a href="#空域图像：" class="headerlink" title="空域图像："></a>空域图像：</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Vulcan_Q/article/details/127876972">(62条消息) 【视觉入门】——空域图像增强(邻域运算部分）：图像卷积、多种滤波器；二值图像分析及形态学图像处理_Vulcan_Q的博客-CSDN博客_空域图像处理</a></p>
<h3 id="AUC："><a href="#AUC：" class="headerlink" title="AUC："></a>AUC：</h3><p>Area Under Curve，指接收器操作特征曲线下的面积，可以用来衡量分类器的性能。</p>
<h3 id="face-landmarks："><a href="#face-landmarks：" class="headerlink" title="face landmarks："></a>face landmarks：</h3><p>直译为面部地标，作为一种匹配标准，用于在真实图像生成训练数据过程中实现寻找。</p>
<p>mask：相应区域的掩码。可以理解为图像的某种属性。表现在图像上，是表示图像中某区域内的像素情况，可以用数字来表示。</p>
<h3 id="SBI："><a href="#SBI：" class="headerlink" title="SBI："></a>SBI：</h3><p>Self-Blended Images，自混合图像，混合来自单个原始图像的伪源图像和目标图像。</p>
<h2 id="LLM"><a href="#LLM" class="headerlink" title="LLM:"></a>LLM:</h2><p>注：这部分内容几乎全部摘自人大高瓴出版的那本书。</p>
<h3 id="大语言模型"><a href="#大语言模型" class="headerlink" title="大语言模型"></a>大语言模型</h3><p>大语言模型是指在海量无标注文本数据上进行预训练得到的大型预训练语言模型。</p>
<p>如 GPT-3 [23]，PaLM [33] 和 LLaMA [34]。</p>
<p>目前大语言模型所需要具有的最小参数规模还没有一个明确的参考标准，但是大语言模型通常是指参数规模达到百亿、千亿甚至万亿的模型。</p>
<h3 id="大语言模型的能力"><a href="#大语言模型的能力" class="headerlink" title="大语言模型的能力"></a>大语言模型的能力</h3><p>具有较为丰富的世界知识、具有较强的通用任务解决能力、具有较好的复杂任务推理能力、具有较强的人类指令遵循能力。</p>
<h3 id="大语言模型的预训练"><a href="#大语言模型的预训练" class="headerlink" title="大语言模型的预训练"></a>大语言模型的预训练</h3><p>由于参数规模巨大，需要使用大规模分布式训练算法优化大语言模型的神经网络参数。</p>
<p>在训练过程中，需要联合使用各种并行策略以及效率优化方法，包括 3D 并行（数据并行、流水线并行、张量并行）、ZeRO（内存冗余消除技术）等。</p>
<p>为了有效支持分布式训练，很多研究机构发布了专用的分布式优化框架来简化并行算法的实现与部署，其中具有代表性的分布式训练软件包括DeepSpeed 和 Megatron-LM。</p>
<h3 id="3H对齐标准"><a href="#3H对齐标准" class="headerlink" title="3H对齐标准"></a>3H对齐标准</h3><p>实践应用中，需要保证大语言模型能够较好地符合人类的价值观</p>
<p>3H标准即 Helpfulness（有用性）Honesty（诚实性）Harmlessness（无害性）。</p>
<p>为了解决这一问题，OpenAI 提出了基于人类反馈的强化学习算法（Reinforcement Learning from Human Feedback, RLHF），将人类偏好引入到大模型的对齐过程中。</p>
<h3 id="涌现能力"><a href="#涌现能力" class="headerlink" title="涌现能力"></a>涌现能力</h3><p>指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。</p>
<h2 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h2><h3 id="软硬编码"><a href="#软硬编码" class="headerlink" title="软硬编码"></a>软硬编码</h3><p>硬编码：简单来说，<strong>硬编码就是将某些值或参数直接写入代码中，而不是通过外部配置、用户输入或程序运行时动态获取</strong>。</p>
<p>硬编码有缺乏灵活性、维护困难、可读性差、不利于扩展等一系列问题。<br>以下是一些常用的方法避免硬编码：</p>
<ul>
<li>使用配置文件：将需要变动的值写入配置文件，程序在运行时读取配置文件获取这些值。这样，当需要修改值时，只需修改配置文件而无需修改代码。</li>
<li>使用环境变量：环境变量是一种在程序运行时动态获取值的方式。通过环境变量，我们可以将配置信息与代码分离，提高代码的灵活性和可维护性。</li>
<li>使用数据库或外部服务：对于需要频繁变动的数据，可以考虑使用数据库或外部服务进行存储和管理。程序通过API接口获取数据，实现与数据的解耦。</li>
</ul>
<p> 硬编码与软件设计原则中的一些概念是相悖的。例如，开放封闭原则（OCP）强调软件实体（类、模块、函数等）应该对扩展开放，对修改封闭。而硬编码往往导致软件实体对修改不封闭，因为每次修改硬编码的值都需要直接修改代码。</p>
<p>​    此外，单一职责原则（SRP）也要求我们将功能拆分到不同的类或模块中，每个类或模块只负责一项职责。硬编码可能会使得某个类或模块承担过多的职责，降低代码的可读性和可维护性。</p>
<p> 因此，在编写代码时，我们应该尽量避免硬编码，遵循软件设计原则，提高代码的质量和可维护性。</p>
<p>​    </p>
<h2 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h2><h3 id="Rust"><a href="#Rust" class="headerlink" title="Rust"></a>Rust</h3><h4 id="Lifetime"><a href="#Lifetime" class="headerlink" title="Lifetime"></a>Lifetime</h4><ol>
<li><strong>什么是Lifetime？</strong></li>
</ol>
<p>在Rust中，<strong>Lifetime</strong>（生命周期）是用来管理引用有效性的概念。Rust的所有权系统确保内存安全，而Lifetime则是这个系统的一部分，用于确保引用不会在它们所指向的数据被释放后仍然存在。</p>
<p>简单来说，Lifetime标记了引用的有效范围，确保引用不会“悬空”（即指向已经被释放的内存）。</p>
<ol start="2">
<li><strong>为什么需要Lifetime？</strong></li>
</ol>
<p>Rust的所有权系统要求编译器在编译时就能确定所有引用的有效性。如果没有Lifetime，编译器无法确定某个引用是否在它所指向的数据被释放后仍然存在，这会导致潜在的内存安全问题。</p>
<p>例如：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">main</span>() &#123;</span><br><span class="line">    <span class="keyword">let</span> <span class="variable">r</span>;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">let</span> <span class="variable">x</span> = <span class="number">5</span>;</span><br><span class="line">        r = &amp;x; <span class="comment">// 错误：x的生命周期太短</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">println!</span>(<span class="string">&quot;r: &#123;&#125;&quot;</span>, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>r</code>引用了<code>x</code>，但<code>x</code>的生命周期在内部块结束时已经结束，因此<code>r</code>会变成一个悬空引用。Rust编译器会通过Lifetime检查来阻止这种情况。</p>
<ol start="3">
<li><strong>如何使用Lifetime？</strong></li>
</ol>
<p>Lifetime是通过在函数签名或结构体定义中显式标记来使用的。例如：</p>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fn</span> <span class="title function_">longest</span>&lt;<span class="symbol">&#x27;a</span>&gt;(x: &amp;<span class="symbol">&#x27;a</span> <span class="type">str</span>, y: &amp;<span class="symbol">&#x27;a</span> <span class="type">str</span>) <span class="punctuation">-&gt;</span> &amp;<span class="symbol">&#x27;a</span> <span class="type">str</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> x.<span class="title function_ invoke__">len</span>() &gt; y.<span class="title function_ invoke__">len</span>() &#123;</span><br><span class="line">        x</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        y</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>&#39;a</code>是一个Lifetime参数，表示<code>x</code>和<code>y</code>的引用必须至少在<code>&#39;a</code>的生命周期内有效。返回值的Lifetime也必须是<code>&#39;a</code>，以确保返回的引用不会悬空。</p>
<ol start="4">
<li><strong>Lifetime的自动推断</strong></li>
</ol>
<p>在许多情况下，Rust编译器可以自动推断Lifetime，而不需要显式标记。只有在编译器无法确定Lifetime时，才需要手动标记。</p>
<ol start="5">
<li><strong>Lifetime与所有权的关系</strong></li>
</ol>
<p>Lifetime是Rust所有权系统的一部分，但它只与引用相关。所有权本身不需要Lifetime，因为所有权是明确的（要么拥有数据，要么不拥有）。</p>
<p>总之，<strong>Rust的 Lifetime</strong> 用于确保引用的有效性，防止悬空引用，是Rust所有权系统的一部分。</p>
<h3 id="Golang"><a href="#Golang" class="headerlink" title="Golang"></a>Golang</h3><h4 id="defer"><a href="#defer" class="headerlink" title="defer"></a>defer</h4><p> <strong>什么是defer？</strong></p>
<p>在Golang中，<strong>defer</strong>是一种延迟执行的机制。通过<code>defer</code>关键字，可以将一个函数调用推迟到当前函数返回之前执行。</p>
<ol start="2">
<li><strong>为什么需要defer？</strong></li>
</ol>
<p><code>defer</code>通常用于资源管理，例如关闭文件、释放锁、释放内存等。它确保在函数退出时，无论函数是正常返回还是因错误退出，资源都能被正确释放。</p>
<p>例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(<span class="string">&quot;World&quot;</span>)</span><br><span class="line">    fmt.Println(<span class="string">&quot;Hello&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello</span><br><span class="line">World</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>defer</code>推迟了<code>fmt.Println(&quot;World&quot;)</code>的执行，直到<code>main</code>函数返回之前才执行。</p>
<ol start="3">
<li><strong>defer的执行顺序</strong></li>
</ol>
<p>多个<code>defer</code>语句按照<strong>后进先出（LIFO）</strong>的顺序执行。例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(<span class="string">&quot;1&quot;</span>)</span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(<span class="string">&quot;2&quot;</span>)</span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(<span class="string">&quot;3&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><strong>defer与错误处理</strong></li>
</ol>
<p><code>defer</code>常用于错误处理，确保在函数因错误退出时，资源仍能被正确释放。例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;fmt&quot;</span></span><br><span class="line">    <span class="string">&quot;os&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    file, err := os.Open(<span class="string">&quot;example.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">        fmt.Println(<span class="string">&quot;Error:&quot;</span>, err)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">defer</span> file.Close() <span class="comment">// 确保文件在函数返回前关闭</span></span><br><span class="line">    <span class="comment">// 读取文件内容</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这个例子中，无论函数是否因错误退出，<code>file.Close()</code>都会在函数返回前执行。</p>
<ol start="5">
<li><strong>defer的注意事项</strong></li>
</ol>
<p><code>defer</code>语句中的表达式是在<code>defer</code>语句执行时计算的，而不是在函数返回时计算的。例如：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    i := <span class="number">0</span></span><br><span class="line">    <span class="keyword">defer</span> fmt.Println(i) <span class="comment">// 这里的i是0</span></span><br><span class="line">    i = <span class="number">10</span></span><br><span class="line">    fmt.Println(<span class="string">&quot;End&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">End</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>总之，<strong>Golang的defer</strong>：用于延迟执行函数调用，通常用于资源管理，确保在函数退出时执行清理操作。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://lightdust02.github.io/2024/01/05/%E7%9F%A5%E8%AF%86%E7%82%B9-Pron/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag">计算机视觉</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2024/01/11/Java%E5%AD%A6%E4%B9%A0/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Java学习
          
        </div>
      </a>
    
    
      <a href="/2024/01/03/BAOYAN/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">2023计算机保研全历程</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.staticfile.org/valine/1.4.16/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023-2025
        <i class="ri-heart-fill heart_icon"></i> LightDust
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="http://www.beian.miit.gov.cn/" target="_black" rel="nofollow">浙ICP备88888888</a>
        </li>
        
    </ul>
    <ul>
      
      <li>
          <img src="/images/beian.png"></img>
          <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=01234567890123" target="_black" rel="nofollow">浙公网安备01234567890123号</a>
      </li>
        
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="LightDust"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>