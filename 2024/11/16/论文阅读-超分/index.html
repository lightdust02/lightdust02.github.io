<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>论文阅读笔记-图像超分与复原 |  LightDust</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-论文阅读-超分"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  论文阅读笔记-图像超分与复原
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/" class="article-date">
  <time datetime="2024-11-16T09:35:00.000Z" itemprop="datePublished">2024-11-16</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%A7%91%E7%A0%94/">科研</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">24k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">85 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>阅读一系列论文时的笔记。</p>
<p>和导师沟通后，确定了小方向走图像超分/复原这条路。于是开始读该领域的一系列论文。</p>
<span id="more"></span>

<h1 id="Image-Processing-GNN-Breaking-Rigidity-in-Super-Resolution"><a href="#Image-Processing-GNN-Breaking-Rigidity-in-Super-Resolution" class="headerlink" title="Image Processing GNN: Breaking Rigidity in Super-Resolution"></a>Image Processing GNN: Breaking Rigidity in Super-Resolution</h1><p>CVPR 2024</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>超分领域传统的神经网络和窗口注意方法要求比较严格，这两种操作中，每个像素收集相同数量的相邻像素，这阻碍了它们在SR任务中的有效性。</p>
<p>该论文利用图的灵活性，提出了<strong>图像处理GNN（IPG）模型</strong>。</p>
<p>通过为细节丰富的图像节点分配更高的节点度来利用度的灵活性。然后，为了构造SR有效聚集图，我们将图像视为 pixel node sets 而不是 patch nodes。</p>
<p>最后，为了采集局部信息和全局信息，需要灵活地从局部和全局两个尺度上有效地收集像素信息，我们在邻近区域内寻找节点连接以构造局部图；在整个图像的一个步长采样空间内寻找连接，构造全局图。</p>
<p>图的灵活性提高了IPG模型的SR性能。不同数据集的实验结果表明，IPG优于baseline。</p>
<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>主流SR模型以相当严格的方式处理所有像素是理所当然的。例如，基于CNN的SR模型VDSR的卷积层中，相同的卷积核扫描特征图的所有像素，即，每个像素被严格指定为与其最近的邻居通信；在基于Transformer的模型SwinIR 中，所有像素被分配到相等大小的 attention grids 以进行 self-attention 操作。</p>
<p>最近的一些SR研究试图通过扩大 self-attention grids 和设计条纹全局注意窗口来增强SR模型。但是这些改进也引入了很大的计算负担。</p>
<p>其他一些工作利用了图的灵活性，并提出了基于图的方法。<strong>利用基于内容的图构造图块节点，在卷积和窗口注意中，每个图像块可以聚集硬局部边界之外的top-k个相关但距离较远的部分</strong>。</p>
<p>现有的基于k-最近邻图的方法平等地对待所有图像节点。换句话说，所有节点共享相同的预设度k，不考虑SR的不平衡性质。事实上，当我们从图形的角度检查SR中的规范操作范例时，我们发现“度等价”这一特点体现在卷积和窗口注意力中：在这些范例中，图像上的每个像素聚合相同数量的像素，而不管图像内容如何，因此，在图论中共享“相等的度”（每个像素块的 度 是一样的）。这也意味着这些方法的僵化。<strong>严格分配给节点或像素的相等节点度与SR中的不相等重建需求不匹配，从而影响SR性能。</strong></p>
<p><strong>问题：基于Transformer的改进引入了很大的计算负担。而原有的基于图的方法较为僵化，与 SR 中的不相等重建需求不匹配。</strong></p>
<h2 id="二、相关工作"><a href="#二、相关工作" class="headerlink" title="二、相关工作"></a>二、相关工作</h2><p>图与视觉：图已广泛应用于视觉任务，包括点云分割，人类动作识别[42]和图像分类。以前的一些 low-level 工作也将图像视为图。大多数工作会利用图来聚合非局部信息，无论是 intra-scale 还是 cross-scale。图上的其他low-level vision work 包括跨层 cross-layer或图像间信息聚合inter-image information aggregation。在这些 work中，从相邻节点的边缘条件聚合是最优选的。</p>
<p>在从图像构建图方面，大多数作品使用k-最近邻。由于所有图像节点都具有相同的度，KNN图不够灵活；且在low-level vision task中图像节点的不平衡性也被忽视。此外，大多数graph工作将 patches 视为image nodes，忽略了low-level问题中潜在的 patch 未对齐问题。至于图的规模，一些工作在全局范围内搜索以构造图;其他工作从局部框构造图。这些作品中的图不够灵活，因为它们未能兼顾局部和全局方面。</p>
<h2 id="三、方法"><a href="#三、方法" class="headerlink" title="三、方法"></a>三、方法</h2><p>卷积和窗口注意力是 SR 的两个主要途径。尽管在SR中使用得很普遍，但并不灵活。它们根深蒂固的僵化可能会阻碍SR的性能。在一个卷积运算中，每个输出像素在一个微小的 kernel 大小的窗口内收集信息；每个像素只能访问其相邻像素。例如，在标准的3 × 3卷积中，单个像素的感知场被限制在一个微小的3 × 3框中。图像上的所有像素从8个相应的邻居和它们自己收集信息。类似的僵化也适用于窗口注意力：尽管通常采用更大的窗口大小（与卷积相比），但窗口注意力的感知范围仍然局限于硬窗口边界内。在一个8 × 8窗口注意场景中，所有像素在它们所属的窗口内聚集64个像素。</p>
<p>因此，除了卷积和窗口注意力机制的僵化之外，一些工作跳出固有思维，在 SR 模型中采用图Graph。与信息聚合受限于框的卷积和窗口注意不同，这些基于图的工作在空间上更加灵活：每个节点都可以从其最喜欢的前 k 个节点聚合信息。因此，图不限于预设的僵化。与卷积和窗口注意力机制相比，它更具动态性和可扩展性。</p>
<p>已有的基于图的方法突破了硬聚合边界，但图的灵活性在SR任务中没有得到充分的利用。SR重建需求在不同图像部分之间不平衡。问题包括：</p>
<ol>
<li>先前的方法中，图像上的所有像素pixels节点nodes聚合相同数量的像素或节点：即，它们共享相同的度。</li>
<li>先前基于图的作品都是 patch-rigid 的。虽然 patch 通常被视为图像节点nodes，但 patch 聚合通常需要严格的像素对齐。patch 中可能存在的 low-level 特征错位会使SR模型的性能变差。</li>
<li>以往的基于图的工作，要么使用全局上的图，要么使用局部尺度上的图，但实际上这两个尺度上的信息对于SR重构都是潜在重要的。这些僵化的方面正在阻碍SR模型的性能。</li>
</ol>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241016153055257.png" alt="不同方法间“度”的对比"></p>
<h3 id="论文中图的构建："><a href="#论文中图的构建：" class="headerlink" title="论文中图的构建："></a>论文中图的构建：</h3><p>在IPG模型的局部和全局尺度上构建了度灵活的像素图。通过这种方式，我们可以充分利用图的灵活性，并在SR任务中获得出色的性能。</p>
<p><strong>度的灵活性：</strong>首先，基于SR任务的“不平衡性”，提出了一种 “degree-flexible” 的求解方法。SR重建图像特征不平衡。SR是一个长尾问题，其中只有一小部分高频像素需要重建，其余图像部分只需要最小限度的恢复。然而，许多工作通过设计训练损失来解决这个问题，而不是从模型设计的角度重新思考。在不平衡的超分问题中，等效处理图像上的所有或部分像素是不适宜的、效率低下的。而这个问题长期被忽视。</p>
<p>我们选择基于标记需要更多重建工作的像素的细节丰富的指示符来为像素分配各种节点度。</p>
<p>具体而言，给定特征图 $F∈R^{H×W×C}$ ，下采样率 s。</p>
<p>每个像素的<strong>细节丰富度量指标</strong>为$D_F ∈ R^{H×W}$ 指定为双线性下采样和上采样特征图与特征图本身之间的绝对差：<br>$$<br>D_F:=\sum_C|F-F_{\downarrow s\uparrow s}|<br>$$<br>s 取为2以避免严重的信息丢失(? )。一些可解释的SR作品提出了度量输出中某个部分的重要性的指标。然而，这些措施是基于梯度的，这需要昂贵的backward。相比之下，所提出的度量$D_F$是成本低的，因为它只需要两次双线性插值两次。我们还在附录中提供了FLOPs统计数据。（FLOPs，floating point operations，浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。）</p>
<p>在 $D_F$ 的基础上，整体度的预算会分配给每个像素。特征图上的像素节点 $v∈F$ 的度与其在 $D_F$ 处的对应像素值成比例：$deg (v) ∝ D_F (v)$ 。细节丰富度指标 $D_F$ 是可视化的。可以看出的是，边缘和角落具有高$D_F$，而平坦色块的$D_F$较低。可视化结果表明，该方法能够有效地将图像中的高频部分反映到各层中。</p>
<p><strong>像素节点灵活性：</strong> 我们面对的是图中顶点的公式化。在以前的基于图的视觉工作中，<strong>图节点通常被设置为图像块 patches 。在图形聚合期间，以逐像素方式对像素块 patches 进行加权求和。</strong>然而，在聚合期间像素的强制元素对齐不适合特征图具有丰富的 low-level 特征的SR任务。low-level任务中的对象移动和旋转是导致 patch 未对齐的两个主要问题。至于对象移位shift，对象在 patch 内的位置可能会有所不同;位置未对齐的 patch 对象会引入噪声。对象旋转是 patch 聚合的另一个错误方面。由于 patch 对齐期间 patch 形状为刚性矩形，因此 patch 的方向总会被忽略。</p>
<p>为了避免节点聚合过程中出现上述问题，我们认为在 low-level 场景中更细粒度(finer-grained)的像素节点是更好的解决方案。像素图更加灵活：每个像素节点可以直接在聚合中找到它的相关像素，从而避免了像素错位。</p>
<p>与 pixel graph 相比，patch graph可能具有更大的感知域，且由于节点总数更少而更容易构造，这意味着用于边连接的搜索空间更小。因此，我们需要一种有效的方法来构造一个灵活有效的 pixel graph。</p>
<p><strong>空间灵活性：</strong> 我们通过局部和全局像素节点连接的有效搜索来开发 IPG 图的空间灵活性。我们提出局部和全局信息对于SR重建都是至关重要的：虽然有损图像部分可以从局部邻域中重建自己，但它们也可以从遥远但相似的特征中学习以进行细化。</p>
<p>现有的基于图的方法已经展示了某种空间灵活性。如图所示，与卷积和窗口关注选项（像素始终关注固定的相邻区域）相比，图聚合可以<strong>灵活地关注关键图像部分</strong>，而不受空间限制。然而，现有的基于图的方法在空间上不够灵活，不能兼顾局部和全局的特点。</p>
<p>实际上，计算考虑是连接搜索的主要关注点。通过搜索所有图像节点来构造全局图的代价很高。作为一种补救措施，通常采用步长采样(stride sampling)。但结果是，局部区域 local regions 被忽视了。像素作为图节点的选择使得图构造更加具有挑战性，因为节点空间被进一步扩大，并且搜索所有像素以用于图构造几乎是不切实际的。</p>
<p>为了提高效率，并收集有助于SR任务中细节重建的局部周围特征和全局远距离特征，我们使用两种类型的采样来聚合局部和全局信息：<strong>对于局部采样，选择某个节点周围的邻域作为局部尺度的搜索空间，在其上建立图；对于全局采样，采样节点 sampled nodes 以扩张的模式跨越整个图像。</strong></p>
<p>现在构建了灵活的图，执行图聚合，使得每个节点可以与其连接的邻居进行通信，并使用它们的信息在SR中进行自细化。在图的视觉应用中，最大池化 aggregation in max-pooling 或边条件形式的聚合 edge-conditioned 是最优选的。我们倾向于采用边缘调节聚合，因为最大值合并可能会导致相邻像素的信息大量丢失，而这些信息对低级视觉至关重要。由于超分辨率图像的像素重建依赖于丰富的邻域信息，采用边缘条件聚合算法，既考虑了像素间的相互关系，又保留了更多的邻域信息，以保证重建的有效性。</p>
<p>边缘条件聚合算法数学语言表达如下：在IPG中的第 $k$ 层，给定节点特征$h^{k-1}$作为输入，相邻节点集合$N(v)$，节点v输出$h^k_v$计算为：<br>$$<br>h^k_v=\frac{1}{C^k}\sum_{v\in N(v)}exp(f^k(u,v))h^{k-1}<em>v<br>$$<br>其中$f^k：R^d × R^d → R$是衡量节点对 (u，v) 之间相关性的参数化函数；$C^k：=\sum</em>{v\in N(v)}exp(f^k(u,v))h^{k-1}_v$ 是归一化常数。例子中，余弦相似性被采用作为相关性度量。</p>
<p>尽管图聚合具有灵活性，但我们担心在图聚合过程中空间信息会被破坏：由于所有节点都被平等对待，模型将掌握很少的节点位置知识。因此，受Liu等人的启发。在聚合之前向节点特征添加相对位置编码以增强位置信息。</p>
<h3 id="模型的结构"><a href="#模型的结构" class="headerlink" title="模型的结构"></a>模型的结构</h3><p>团队将所提出的图构造和图聚合机制作为 IPG模型合并到有效的 MetaFormer 架构中。IPG的详细结构如图所示。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241017102420504.png" alt="IPG结构"></p>
<p>IPG架构。IPG的总体架构遵循主流SR模型。当LR图像 (低分辨率图像) 被输入到模型中时，它首先被传递到Conv Layer以提取浅层特征。然后将特征通过一系列的多尺度图聚合块 (Multiscale Graph-aggregation Blocks ，MGB)，利用 flexible graph 进行有效的深度特征提取。每个MGB由图聚合层 (Graph Aggregation Layer,GAL) 组成，其在本地和全局尺度上执行图聚合。最后，通过像素混淆上采样器对图像进行空间重构。</p>
<p>MGB 块。多尺度图聚合块（MGB）同时收集局部和全局尺度的信息，实现有效的图像SR重建。局部和全局像素图都是基于块输入按块计算的。逐块图计算可在整个模型中定期更新图形。构造步骤详见3.2节，分别进行局部采样和全局采样，进行局部或全局图构造。然后，将两种类型的图（局部/全局）分布到整个块中的GAL以进行聚合操作。局部和全局图以顺序交替的方式分布，以确保局部和全局尺度的信息都被充分地聚合。</p>
<p>GAL层。图形聚合层（GAL）的设计遵循一般的类MetaFormer结构[44]。负责图聚合（如3.3节中所介绍的）的 Graphers 被用作 token混合器。Grapher 根据收到的图类型收集局部或全局信息。接下来，我们利用高效的通道注意力 (Coordinate Attention,CA) 模块和 ConvFFN 块来提高我们模型的 SR 性能。</p>
<h2 id="四、实验部分"><a href="#四、实验部分" class="headerlink" title="四、实验部分"></a>四、实验部分</h2><p>采用最近SR工作[5，6，23]中的一般训练设置，以进行公平比较。我们使用DIV2K+Flickr2K（$D_F$2K）作为训练集（DIV2K用于轻量级模型）。将尺寸为64 × 64的裁剪  patches  送到模型中。训练数据通过随机翻转和随机旋转角度[0，90，180，270]来扩充。</p>
<p>使用Adam Optimizer（β1 = 0.9，β2 = 0.99）以2e−4的学习速率训练 500K 次迭代。训练输入大小设定为64 × 64。采用 MultistepLR ，其中 lr 在迭代[250000，400000，450000，475000]时减半。批处理大小设置为32。</p>
<p>优化器 Adam，0.9 ，0.99</p>
<p>学习率 2e-4</p>
<p>iters 500000</p>
<p>batch_size 32</p>
<h3 id="实验表现："><a href="#实验表现：" class="headerlink" title="实验表现："></a>实验表现：</h3><p>比较了我们的模型与表1中Set 5 [3]，Set 14 [45]，BSDS 100 [33]，Urban 100 [13]和Manga 109 [34]基准的各种SR baseline的性能。选择的 baseline 如下：IGNN 是基于 patches 图的SR方法; ELAN ，IPT ，SwinIR ，CAT-A，ART ，GRL-B 和 HAT是基于 Transformer 的方法。通过PSNR和SSIM测试了SR ×2、×3、×4模型的性能。“+”标记代表具有<strong>自集成策略</strong>的 baseline ，这进一步提高了SR性能。IPG可以大幅超越其他模型。特别是，IPG在Urban100 × 4任务上达到 28.13dB 的PSNR，在标准训练设置下超过现有SOTA 0.1dB以上。IPG在PSNR与理论FLOPs图中具有出色的性能，如图5所示。但值得注意的是，由于缺乏硬件支持，IPG 的运行速度比以前的方法慢；IPG 占用的峰值内存略有优势。除了 PSNR/SSIM 指标的定量比较外，我们还提供了 IPG 重建的<strong>困难细节的视觉结果</strong>。如图 6 所示，在 SR ×4 任务上，将提出的 IPG 模型与最近的顶级 SR baseline进行了比较。我们的 IPG 模型在视觉上可以胜过其他方法。</p>
<p>还提供了IPG的一些轻量化变体。虽然IPG-S和IPG-Tiny的架构与基础版本保持相同，但其深度和尺寸宽度对于计算受限的应用程序而言显著降低。变体配置详情见附录。我们选择了规范的SR baseline，包括  CARN[1]、IMDN [14]、LAPAR-A [19]、LatticeNet [31]和ESRT [30]；其他 baseline采用MetaFormer架构，包括SwinIR-light [23]、ELAN [48]、SwinIRNG [53]和SRFormer-light [53]。IPG可以以相似的计算成本胜过共享相似MetaFormer [44]架构的SR baseline。具体而言，与其他轻量级基准相比，IPG-Tiny在Urban 100上实现了超过0.1dB的PSNR改善。</p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>Pixel Nodes vs. Patch Nodes：如第3.2节所述，作为图像图形节点的 patches 可能会在对象移动和对象旋转等情况下受到影响。 patches 像素的强制逐元素对齐会在很大程度上阻碍SR性能。为了研究 patches 节点的负面影响，我们提供了一个修补版本的IPG如下：不使用像素节点，使用2 × 2大小的 patches 作为节点在这个修补的变体。为了公平比较，其他模型设置保持不变，包括模型深度、特征尺寸、MLP比率和采样区域大小。然而，我们知道，与像素节点相比， patches 节点聚合的计算成本可能不同。因此，我们调整 patches 变量的聚合次数，以确保 patches 节点的聚合具有与像素节点聚合相似的成本。消融结果如表2所示。</p>
<p>对于使用大 patches 作为节点的IGNN， patches 对齐带来了麻烦：模型对纹理方向感到困惑，因此在纹理重建过程中对齐模式相似但方向不同的 patches 会出错。对于修补的IPG，未对齐的方向也会导致问题：线纹理扭曲。相比之下，IPG中的像素聚合避免了未对准并且表现良好。</p>
<p>KNN：在视觉任务的典型图形应用中，通常采用 k-最近邻 (KNN) 图来构建图形。然而，由于所有节点都连接到固定数量的邻居节点，KNN仍然受到度 rigidity 的影响。为了提高度的灵活性，我们考虑了SR任务中像素需求的不平衡性，提出了一种度灵活的图构造方案（在3.2节中介绍）。我们在相同的设置下，比较了KNN和度灵活图方案在IPG中的性能。根据表3，Degree-Flex可以胜过IPG的KNN变体。有趣的是，我们的Degree-Flex图可以胜过计算量大的全连接图（见附录），这进一步表明了我们的DegreeFlex图的有效性。</p>
<p>如图8所示，在局部尺度上直观地比较了细节贫乏像素和细节丰富像素的连通区域。细节不佳的像素通常位于图像的平坦、不变部分；这些像素需要最少的节点连接进行重建。相反，细节丰富的像素节点通常位于边缘和角上，这些边缘和角被分配了较高的节点度，从而具有较大的感知区域。在这个意义上，细节丰富的指标度量$D_F$可以赋予IPG程度灵活性.</p>
<p>局部/全局图：在 IPG 的设计中，我们基于局部和全局像素节点采样从局部和全局尺度聚合信息。尽管许多 SR 工作都强调了局部或非局部聚合的重要性，但我们证明这两个方面对于 SR 性能实际上都至关重要。我们仅在相同的设置下使用局部或全局比例图进行实验，并将其与IPG的原始版本进行比较。当局部和全局聚合都进行时，可以达到最佳性能。</p>
<h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>现有的SR测度在空间和度上都是 rigid 的。为了打破SR的僵化，提出了IPG –一种充分利用图的灵活性的SR模型。首先，我们知道，SR是一项任务，其中细节丰富的图像部分需要不平衡的重建工作。因此，我们设计了一个度可变的图构造方案，而不是k-最近邻。然后，我们采用像素而不是 patches 来进行图聚合。为了达到最大的空间灵活性，节点在局部和全局尺度上都被采样。与SR基线相比，IPG达到了最先进的性能。</p>
<h1 id="SeD-Semantic-Aware-Discriminator-for-Image-Super-Resolution"><a href="#SeD-Semantic-Aware-Discriminator-for-Image-Super-Resolution" class="headerlink" title="SeD: Semantic-Aware Discriminator for Image Super-Resolution"></a>SeD: Semantic-Aware Discriminator for Image Super-Resolution</h1><h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>生成对抗网络（Generative Adversarial Networks，GANs）可广泛应用于图像超分辨率（SuperResolution，简称SR）中的纹理恢复。具体地，利用判别器，使得SR网络能够以对抗训练的方式学习真实世界的高质量图像的分布。然而，分布学习过于<strong>粗粒度</strong>，容易受到<strong>虚拟纹理</strong>的影响，并导致违反直觉的生成结果。</p>
<p>为缓解这一问题，我们提出了一种简单有效的语义感知鉴别器（Semantic-aware Discriminator，简称为SeD），它通过引入图像的语义作为条件，促使SR网络学习图像的细粒度分布。具体地说，我们的目标是从一个经过良好训练的语义提取器中<strong>挖掘图像的语义</strong>。在不同语义下，鉴别器能够自适应地对真假图像进行区分，从而指导SR网络学习更细粒度的语义感知纹理。为了获得准确和丰富的语义，我们充分利用了最近流行的<strong>具有大量数据集的预训练视觉模型（PVMs）</strong>，然后通过设计良好的<strong>空间交叉注意模块</strong>将其语义特征融入到鉴别器中。通过这种方式，我们提出的语义识别器使SR网络能够产生更逼真和令人愉快的图像。在两个典型的任务上进行了广泛的实验，仿真结果验证了所提方法的有效性。</p>
<h2 id="一、介绍-1"><a href="#一、介绍-1" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>深度学习加速了<strong>单图像超分辨率（SISR）</strong>的巨大发展，其目的是从退化的低分辨率（LR）图像中恢复生动的高分辨率（HR）图像。</p>
<p>SISR追求的目标有两个，即：客观和主观质量。为了追求更高的客观质量，已经提出了一系列基于CNN，Transformer 的框架，以通过逐像素损失函数的约束来提高SR网络的表示能力（例如，L1损失和MSE损失）。</p>
<p>但逐像素损失函数不能使SR网络具有有希望的生成能力，这使得这些方法在纹理生成上表现不佳，且主观质量也不好。</p>
<p>为了提高图像的主观质量，需要对失真图像生成令人愉悦的纹理。受生成对抗网络 GAN 的启发，许多工作将SR网络作为生成器，然后引入鉴别器，使SR网络具有真实感知纹理生成能力。SR网络有三种典型的判别器： 逐图像鉴别器 image-wise discriminator、逐拼贴元鉴别器 patch-wise discriminator 、逐像素鉴别器 pixel-wise discriminator。具体地，图像式鉴别器的目的是从全局分布中区分真实/假图像，如VGG样鉴别器。但一般来说，图像方式的分布是粗粒度的，使得网络产生非理想的局部纹理。</p>
<p>为了增强局部纹理，一些工作利用逐块判别器确定逐块分布，并根据不同大小块的接受域调整块大小。此外，逐像素判别器以逐像素的方式区分真假分布，但计算成本较高。然而，上述工作忽略了一个事实：一幅图像的纹理应该符合其语义的分布。实现 SR 的<strong>细粒度语义纹理生成</strong>是必要的。</p>
<p>为了生成语义感知的纹理，一种直观的方法是将图像的<strong>语义信息集成到纹理生成器</strong>中，使生成器具有语义自适应能力。实际上，这给语义感知的纹理生成带来了两个明显的缺陷：</p>
<ol>
<li><p> 低质量的图像可能导致语义提取的质量变差，甚至错误，从而阻碍了合理的纹理生成。</p>
</li>
<li><p> 在推理阶段，复杂语义抽取器会导致SR网络的计算量和模型复杂度的灾难性增长。相比之下，我们的目标是从另一个角度实现语义感知的SR纹理生成，即，鉴别器。</p>
</li>
</ol>
<p>本文提出了第一个用于 SR 的<strong>语义感知鉴别器，称为SeD</strong>。值得注意的是，鉴别器通过区分图像/补丁/像素是真实的的还是假的来工作。通过对抗训练，能够测量生成的图像与其参考图像之间的分布距离。如图1所示，vanilla 鉴别器测量了分布距离却忽略了语义，这容易受到粗粒度平均纹理（如噪声）的影响。为了缓解这一问题，我们引入了最近流行的<strong>预训练视觉模型（PVM）</strong>（如 ResNet 50或CLIP）中提取的语义作为鉴别器的条件，这使得鉴别器能够针对不同的语义单独地和自适应地测量分布距离。在语义感知鉴别器的约束下，SR网络的生成器能够实现更细粒度语义感知纹理生成。</p>
<p>实现SeD有一个关键步骤，即：<strong>如何提取语义并将其合并到一个XML中。</strong>一种简单的策略是直接从预训练的分类网络中引入<strong>最后一层的特征作为语义</strong>，并在鉴别器中将它们连接起来作为条件。然而，该方法没有考虑SR的特点，不利于对鉴别器进行有效的语义指导。为了实现细粒度的语义感知纹理生成，一种直觉是，由于不同区域的语义可能不同，因此语义需要是逐像素的。考虑到这一点，我们从PVM的中间特征中提取语义。</p>
<p>除此之外，我们还设计了一个<strong>语义感知融合块（SeFB）</strong>来更好地将语义融入到鉴别器中。在 SeFB中，我们将从PVM 中提取的语义作为查询，通过交叉注意力将语义感知的图像特征扭曲到鉴别器中，从而实现了更好的语义引导。我们在两个典型的SR任务上验证了我们所提出的SeD的有效性：经典和真实世界图像SR任务。我们的SeD是通用的，适用于基于GAN的SR的不同基准，例如，ESRGAN、RealESRGAN 和 BSRGAN 。</p>
<p><strong>问题：原有的分布学习粗粒度，容易受到虚拟纹理的影响，并导致违反直觉的生成结果。但是，一幅图像的纹理应该符合其语义的分布。实现 SR 的细粒度语义纹理生成是必要的。</strong></p>
<p><strong>本文主要工作：</strong></p>
<ul>
<li>指出了细粒度语义纹理生成技术在SR任务中的重要性，并首次提出了一种面向SR任务的语义识别器（Semantic-aware Discriminator，SeD），<strong>将预训练视觉模型（Pretrained Vision Model，PVM）的语义引入到识别器中</strong>。</li>
<li>为了更好地融合语义对鉴别器的指导作用，提出了<strong>基于语义感知的SeD融合块</strong>（Semantic-aware fusion block，<strong>SeFB</strong>），该融合块<strong>提取像素级语义，并通过交叉注意的方式将语义感知的图像特征引入鉴别器</strong>。</li>
<li>在两个典型的SR任务上进行了广泛的实验，即：经典的和真实世界的图像SR已经揭示了我们所提出的SeD的有效性.此外，我们的SeD可以以即插即用的方式轻松地集成到基于GAN的SR方法的许多基准测试中。</li>
</ul>
<p>简而言之： SeD + PVM + SeFB</p>
<h2 id="二、相关工作-1"><a href="#二、相关工作-1" class="headerlink" title="二、相关工作"></a>二、相关工作</h2><p>SRCNN 将CNN引入到 SISR 中，并通过最小化超分辨率图像（SR）与其对应的高分辨率图像（HR）之间的均方误差（MSE）来优化网络。随后，许多工作从不同的角度进一步提高了网络的表示能力，如引入剩余连接、稠密连接、多尺度表示等。具体地，一些工作采用注意机制来进一步增强高频纹理。然而，这些方法缺乏对图像中存在的长相关性的利用，从而限制了它们在SR上的性能。随着 ViT 的出现，一些研究[37，42，69]将像素视为token，并利用基于窗口的自我注意的长程依赖性，适用于经典与真实世界SR任务。</p>
<p>为了生成更吸引人的图像，一些工作旨在利用生成模型的出色能力来增强SR网络的主观质量。引入对抗性损失，使SR网络能够学习真实世界高质量图像的分布。SRGAN 作为基于GAN的SR的第一项工作，系统研究了GAN对SR的优势。在此之后，一系列工作开始细化SR网络的架构（即，生成器），实现稳定的训练和更真实的纹理。尽管如此，很少有工作考虑优化GAN中的另一个核心组件：<strong>该算法用于确定SR网络学习到的分布是否与真实世界的高质量图像一致？</strong>由此，提出的语义感知鉴别器（SeD）有望使SR网络实现更细粒度的语义感知纹理生成。</p>
<p>受益于大规模数据集，如ImageNet 22 K [11]，JFT-300 M [60]，LAION-5 B [58]和精心设计的预训练策略[12，21]，预训练视觉模型（PVM），例如，ViT [7，14]，Swin Transformer [40，41]已经证明了它们在各个领域的巨大潜力，例如，分类，视觉语言任务，生成任务。与较小的模型相比，PVM对大多数下游任务都很友好，这要归功于它们强大而鲁棒的表示能力。最近，大型视觉语言模型 CLIP 由于利用大量收集的视觉语言对来学习更细粒度的语义空间而引起了极大的兴趣。因此，一系列工作将 CLIP 的这个强大的特征提取器纳入各种任务中，包括提示学习，生成等。受此启发，我们的目标是从 CLIP 中提取更细粒度的语义来指导我们的 SeD。</p>
<h2 id="三、方法-1"><a href="#三、方法-1" class="headerlink" title="三、方法"></a>三、方法</h2><p>首先回顾典型的基于 GAN 的 SR 方法，然后描述该方法与它在整体框架中的差异。接下来将详细阐述我们提出的语义感知鉴别器。最后，我们描述如何将我们的 SeD 集成到现有的逐块鉴别器和逐像素鉴别器中。</p>
<h3 id="SeD框架"><a href="#SeD框架" class="headerlink" title="SeD框架"></a>SeD框架</h3><p>我们提出的语义感知鉴别器（SeD）的整体框架如图2所示。给定低分辨率图像$Ii$，我们可以首先获得超分辨率图像$Is$。然后，使用判别器 D来区分$Is$和高分辨率图像$I_h$，这强制SR网络生成类真实图像（即$P(I_s) = P(I_h)$）。然而，普通的分类器只考虑了图像的粗粒度分布，而忽略了图像的语义。这将导致SR网络产生虚假甚至更差的纹理。一个有前途的纹理生成应该满足其语义信息。因此，我们的目标是实现语义感知的语义，它利用高分辨率图像$I_h$的语义作为条件。在这里，我们将大型视觉模型中的语义提取器表示为 $ϕ$，我们的目标是实现更细粒度的语义感知纹理生成。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241105153507152.png" alt="原始GAN、SeD、SeFB、Patch-wise SeD"></p>
<p>如图所示，高分辨率图像$I_h$将被送到来自型视觉模型的固定的预训练语义提取器中以获得语义$ϕ(I_h)$，然后使用语义感知融合块SeFB，通过将语义视为查询，将超分辨率图像特征$f^s$和高分辨率图像特征$f^h$进一步变形为超分辨率图像特征$f^s_s$，$f^h_s$。基于语义感知的特征，该算法可以实现语义感知的分布度量。值得注意的是，这个过程不会增加SR网络在推理阶段的参数或计算成本，因为不需要递归。</p>
<h3 id="SeD具体结构"><a href="#SeD具体结构" class="headerlink" title="SeD具体结构"></a>SeD具体结构</h3><p>我们的语义感知鉴别器由两个基本组件组成，即语义特征抽取器和语义感知融合块。通过将这些成分结合到流行的鉴别器中，我们可以得到语义感知的鉴别器。</p>
<h4 id="3-3-1语义挖掘"><a href="#3-3-1语义挖掘" class="headerlink" title="3.3.1语义挖掘"></a>3.3.1语义挖掘</h4><p>成功的语义挖掘在语义识别中起着不可替代的作用。直觉是，更细粒度的语义将促进鉴别器测量更细粒度的分布。因此，最近流行的预训练视觉模型(PVM)具有更强的表示能力，适合于语义挖掘。其中，CLIP模型已成为突出的中坚力量，并已在各种任务中得到应用。因此，我们采用预先训练的片段 “RN50” 模型作为语义抽取器，因为我们需要对 SR 进行像素级的语义信息。</p>
<p>具体地说，RN50 由四层组成，随着层的增加和语义的变得更加抽象，特征的分辨率被下采样。为了考察哪一层更适合我们的语义挖掘，我们对这四层进行了系统的实验，实验发现<strong>第三层的语义特征是最优的</strong>。这里，我们将语义抽取器表示为ϕ，并通过将高分辨率图像$I_h$送入ϕ来提取语义特征f。</p>
<h4 id="3-3-2-语义感知融合"><a href="#3-3-2-语义感知融合" class="headerlink" title="3.3.2 语义感知融合"></a>3.3.2 语义感知融合</h4><p>在获得语义$S_h$之后，我们利用我们提出的语义感知融合块（SeFB）来实现判别器的语义引导。考虑到超分辨率/高质量图像$f^s f^h$的特征，我们的目标是将语义感知的纹理从图像扭曲到纹理，从而强制纹理集中于语义感知的纹理的分布。因此，在图2（c）中，语义$S_h$被传递到自注意模块，然后，作为查询被馈送到交叉注意力模块：$Q = LN(SA(LN(GN(S_h))))$，其中LN、GN、SA表示层归一化、组归一化和自注意模块。然后，超分辨率/高质量图像的特征$f^s f^h$被传递到卷积层，并被视为键值 $K^s K^h$和值$Q^s Q^h$。最后，具有交叉注意力的变形语义感知图像特征与原始增强特征级联以形成最终特征$f^s_s f^h_s$，如下：</p>
<h3 id="扩展到其他鉴别器"><a href="#扩展到其他鉴别器" class="headerlink" title="扩展到其他鉴别器"></a>扩展到其他鉴别器</h3><p>在本文中，我们将我们提出的SeD结合到两种流行的鉴别器中，包括patch-wise鉴别器和pixel-wise鉴别器。如图2（d）所示，patch-wise鉴别器由<strong>三个SeFB和两个卷积层</strong>组成。对于pixel-wise鉴别器，我们遵循[66]中的方法，并利用 U-Net 架构作为主干。在浅层特征提取阶段，我们用我们提出的SeFBs代替原始卷积层。</p>
<h2 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h2><h3 id="4-1-实验设置"><a href="#4-1-实验设置" class="headerlink" title="4.1 实验设置"></a>4.1 实验设置</h3><p>网络结构：对于生成器G，我们使用竞争的RRDB 作为主干。</p>
<p>同时还采用了流行的基于Transformer的 SR 网络SwinIR 作为我们的生成器。对于鉴别器，我们将我们的SeD结合到两种类型的鉴别器中，即，patch-wise 鉴别器和 pixel-wise 鉴别器，即 P+SeD和U+SeD。我们还在基于VGG 的图像鉴别器上验证了我们的SeD，称为V+SeD，见附录。</p>
<p>经典图像SR的实现细节：我们使用SeD在两个典型的SR任务上进行了实验，包括经典图像SR [62]和真实世界图像SR [66]。</p>
<p>经典的图像超分辨算法是利用双三次下采样技术实现低分辨率图像的超分辨。在前面的工作之后，我们使用来自DF2K [1，61]的3450个图像来训练我们的网络。我们在五个基准测试集上测试了我们的方法：Set 5 [2]、Set 14 [75]、Urban 100 [22]、Manga 109 [47]和DIV 2K验证集[1]。我们使用MATLAB的双三次下采样核来合成所有的×4的训练样本。我们按照先前的技术[32，37，71]，对所有训练样本应用相同的数据扩充。HR图像的patch大小为256 × 256。我们使用 <strong>PyTorch 在4个NVIDIA V100 GPU</strong>上进行了实验，每个设备上的 <strong>batch_size 为8</strong>。该方法采用面向 PSNR 的预训练模型对发生器参数进行初始化。我们使用学习率为<strong>1e-4的Adam</strong>优化器来优化我们的网络。<strong>总的训练迭代次数为 300000</strong>。</p>
<p>真实世界图像SR的实现细节：我们还对真实世界图像SR进行了实验。我们将性能与原始的SR三种最先进的方法：Real-ESRGAN [66]，LDL [38]和SwinIR [37]。在此之后，我们评估了几个常用的真实世界低分辨率数据集的性能，包括DPED [23]，OST 300 [64]和RealSRSet [77]。</p>
<p>这三种方法的训练策略和数据集略有不同。为了保持公平，我们遵循他们原始的训练设置，用我们的SeD训练生成器，并将视觉质量与他们原始的基于GAN的结果进行比较。此外，我们采用众所周知的非参考度量NIQE [48]进行定量比较，因为 ground-truth 图像在真实的世界中不可用。</p>
<h3 id="4-2-经典图像SR的结果"><a href="#4-2-经典图像SR的结果" class="headerlink" title="4.2 经典图像SR的结果"></a>4.2 经典图像SR的结果</h3><p>对SeD和最先进的基于GAN的SR方法进行了定量比较。SFTGAN [64]结合了从输入LR图像中提取的语义映射，以更好地恢复纹理。ESRGAN [65]和USRGAN [76]都使用RRDB [65]作为生成器的主干。LDL [38]使用RRDB将伪影贴图应用到RRDB中，以更好地重建更精细的纹理。</p>
<p>因此，我们用RRDB作为生成器，与它们进行公平的比较。为了验证泛化能力，我们还使用另一种生成器架构来验证我们的SeD，即，基于transformer的SR骨架，SwinIR 。</p>
<p>如表1所示，我们的SeD在感知度量（即，LPIPS），包括SFTGAN [64]，ESRGAN [65]，USRGAN [76]，LDL [38]和DualFormer [44]，具有可比的甚至更高的客观质量。值得注意的是，对于表1的最后几行，我们通过将SeD合并到不同的生成器（即，RRDB [65]和SwinIR [37]）和鉴别器（分块鉴别，即，“+P”和U形像素级的像素级，即，“+U”）。</p>
<p>“RRDB+P”，可以观察到我们的 SeD（即“RRDB+P+SeD”）在任何情况下都优于 LPIPS 中的原始鉴别器，甚至在 Manga109 数据集中 PSNR 增益高达 1.0dB。</p>
<p>这表明在 SeD中引入语义指导的优越性，因为它可以在训练过程中区分更多的细粒度纹理，从而获得更好的感知质量。注意，我们的SeD在基于Transformer的SR方法SwinIR上也表现良好（即，“SwinIR+P+SeD”和“SwinIR+U+SeD”）。这证明了SeD在不同生成器架构上的泛化能力。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241107105416564.png" alt="表1,三个内容分别是LPIPS↓/PSNR↑/SSIM↑"></p>
<p>我注意到，其他方法恢复的图像纹理不自然。如最后一排将汽车前面的点状纹理误认为条纹等。与其他方法相比，我们的SeD生成了视觉上令人愉快的结果，而不引入伪影。这证明了通过SeD结合语义指导可以使用户能够区分更精细的细节。我们在附录中提供了更多的视觉结果。</p>
<h3 id="4-3-真实世界图像SR的结果"><a href="#4-3-真实世界图像SR的结果" class="headerlink" title="4.3 真实世界图像SR的结果"></a>4.3 真实世界图像SR的结果</h3><p>我们通过将其纳入最先进的方法（包括Real-ESRGAN 和LDL ）来评估SeD在真实世界图像SR问题上的性能。通常，Real-ESRGAN和LDL都使用RRDB作为生成器，但LDL用 artifact 的 U-Net替代了普通的U-Net。我们还使用更强大的基于Transformer的发生器SwinIR [37]进行实验，SwinIR的定性结果见附录。所有上述方法都使用像素级U-Net算法，因此，我们使用像素级U-Net算法“U+SeD”进行实验。</p>
<p>表2中展示了定量比较。我们的SeD在不同的骨干和训练策略方面超过了大多数真实世界数据集上的普通鉴别器。结果表明，我们的SeD是能够区分严重失真的现实世界的图像，并学习细粒度的纹理生成能力。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241107110320854.png" alt="表2"></p>
<h3 id="4-4-消融研究SeD"><a href="#4-4-消融研究SeD" class="headerlink" title="4.4 消融研究SeD"></a>4.4 消融研究SeD</h3><p>我们进行消融实验，以调查最合适的方式引入语义指导到神经网络。我们主要关注三个方面：i）在SeFB中融合语义特征的最佳方式是什么？ii）CLIP的哪一层包含最有用的语义信息用于纹理鉴别？iii）哪种预训练模型更适合作为语义提取器？</p>
<p>不同融合方法的效果：有一系列的融合方法来引入先验信息，包括特征级联，空间注意，或通道注意。比较我们提出的SeFB（即，SeD-Our）与不同融合策略的融合。信道注意力（channel attention即，SeD-B）破坏了语义的空间信息，性能最差。级联操作的性能（concatenation operation 即，SeD-A）和空间注意力（spatial attention即，SeD-C）是可比的，这是明显低于我们提出的语义感知融合块（SeFB）。更多细节可以在附录中找到，以实现上述融合策略。</p>
<p>不同层的语义：值得注意的是，CLIP 不同层的语义是不同的。随着层数的增加，语义的代表性会更强，但分辨率会降低。有效的语义引导的本质在于提取足够深的语义特征，同时保留必要的空间信息，以确保恢复引导的质量。CLIP语义提取器内部共有四层，每层后空间分辨率减半。为了研究哪一层是指导的最佳选择，我们进行了表4中的实验。</p>
<p>从表中可以看出，在第一层和第二层中，语义特征具有相对较大的空间分辨率，但其缺乏集中性可能导致用于区分过程的次优语义指导。最后一层空间分辨率太小，可能会导致忽略像素一致性。结果表明，第三层语义的性能最好，这是我们在本文中使用的。</p>
<p>不同语义提取器的效果：为了更深入地研究更细粒度的语义理解是否能提高指导质量的问题，我们比较了两种不同的语义提取器。具体而言，我们使用ImageNet数据集对比来自预训练CLIP的“RN 50”模型对预训练ResNet-50 [20]进行了评估，该模型使用视觉语言数据集，详见表5。实验结果表明，用视觉语言数据预训练的“RN 50”模型比前者具有更好的识别效果。CLIP训练方案中的语言描述提供了比传统分类任务中使用的数字标签更全面、更详细的语义上下文，这是其上级性能的主要原因。这一发现强调了<strong>语言增强模型在实现对语义的更精细理解方面的潜力</strong>，从而为区分过程提供了更有效的指导。</p>
<h3 id="4-5-地形特征的T-SNE可视化"><a href="#4-5-地形特征的T-SNE可视化" class="headerlink" title="4.5.地形特征的T-SNE可视化"></a>4.5.地形特征的T-SNE可视化</h3><p>为了验证该语义指导是否促进了SeD执行更细粒度的区分，我们使用T-SNE可视化了普通SeD和SeD的中间特征[63]。我们从ImageNet中随机选择了7个类别，每个类别100张图像。然后，我们将所有图像输入到普通PatchGAN和我们的P+SeD中。我们分别可视化SeD的第一个SeFB之后的特征和PatchGAN的第一个BN层之后的特征。如图5所示，SeD的特征是良好聚类的，而普通Patch判别器的特征是混乱的。这提供了证据，证明在语义的指导下，SeD能够执行更细粒度的区分，正如论文中所声称的那样。wu</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>提出了一种简单的、有效的图像超分辨率语义识别器（SeD）。我们发现，以往的判别器的分布学习过于粗粒化，导致了虚拟纹理或违反直觉的纹理。为了缓解这一问题，我们引入了预训练视觉模型（PVM）的图像语义作为鉴别器的条件，并提出了语义感知融合块（SeFB），以鼓励SR网络学习细粒度的语义感知纹理。此外，SeD算法易于与大多数基于GAN的SR算法相结合，并取得了良好的性能。在经典SR和真实SR上的大量实验证明了该方法的有效性.</p>
<h1 id="APISR-Anime-Production-Inspired-Real-World-Anime-Super-Resolution"><a href="#APISR-Anime-Production-Inspired-Real-World-Anime-Super-Resolution" class="headerlink" title="APISR: Anime Production Inspired Real-World Anime Super-Resolution"></a><strong>APISR: Anime Production Inspired Real-World Anime Super-Resolution</strong></h1><blockquote>
<p>唉，二次元</p>
</blockquote>
<h2 id="摘要-2"><a href="#摘要-2" class="headerlink" title="摘要"></a>摘要</h2><p>真实世界的动画超分辨率（SR）在SR社区中得到了越来越多的关注，但现有的方法仍然采用来自真实感领域的技术。该论文分析了动漫生产的工作流程，并重新思考如何应用它的特点，为了现实世界的动漫SR。首先，我们认为，视频网络和数据集是没有必要的动漫SR由于重复使用的手绘帧。相反，我们提出了一个动漫图像收集管道（anime image collection pipeline），选择最少的压缩和最丰富的帧从视频源。在此基础上，我们介绍了面向动画制作的图像（API）数据集。</p>
<p>此外，我们确定了两个动画特定的挑战，扭曲和模糊的手绘线（distorted and faint hand-drawn lines）和不需要的颜色文物（unwanted color artifacts）。我们通过在图像退化模型中引入面向预测的压缩模块和具有增强的手绘线的伪地面实况准备来解决第一个问题。此外，我们还引入了平衡的双感知损失（the balanced twin perceptual loss combining both anime and photorealistic high-level features），结合了动画和真实感的高级功能，以减轻不必要的颜色伪影并提高视觉清晰度。我们通过在公共基准上进行广泛的实验来评估我们的方法，表明我们的方法优于最先进的动漫游戏训练方法。</p>
<h2 id="一、介绍-2"><a href="#一、介绍-2" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>动漫SR专注于将低质量的低分辨率（LR）动漫视觉艺术图像和视频恢复和增强为高质量的高分辨率（HR）形式。它在娱乐和商业领域已表现出重大的实际影响[46、48、54、56、61]。一个新兴的工作线已经解决了这个问题，通过扩展SR网络来捕获多尺度信息或学习自适应退化模型。</p>
<p>在本文中，我们彻底分析了动漫制作过程，探索如何利用其独特的方面在动漫SR的实际应用。制作工作流程首先从纸上手绘草图开始，然后通过计算机生成图像（CGI）处理进行着色和增强[69]。然后，这些处理过的草图被连接成一个视频。由于绘制过程极其劳动密集型并且人眼对运动不敏感[10，37]，因此在形成视频时<strong>跨多个连续帧重复使用单个图像</strong>是标准做法。这一生产过程促使我们重新思考在动漫领域使用视频网络和视频数据集来训练SR网络是否必要和有效。</p>
<p>为此，我们将探索使用<strong>基于图像的方法和数据集作为动漫图像和视频的统一超分辨率和恢复框架</strong>。创建图像数据集使我们能够更灵活地专门选择压缩程度最低的视频帧作为我们的潜在数据集池，而不是收集包含时间失真的连续帧来创建视频数据集。</p>
<p>此外，通过形成图像数据集，我们可以选择性地聚焦于信息量最大的帧，因为动画视频通常比照片级真实感视频拥有更少的信息。如果我们从动画图像中随机裁剪一个 Patch，则很有可能它是表示缺少信息的单色区域。针对这些现象，本文提出了一种基于关键帧的动画图像采集流水线，沿着一种基于图像复杂度评估的选择标准 (an anime image collection pipeline that focuses on keyframes in video, along with an image complexity assessment-based selection criteria)。该方法的目的是从视频源中识别和选择压缩程度最低、信息量最大的图像。使用我们的管道，我们提出了面向动画制作的图像（API）数据集用于SR训练。此外，我们还发现了现实世界SR任务的两个新的动画特定挑战。首先，在动漫制作中，手绘线条的清晰度是一个非常强调的细节[6，26，56]，如图2a所示，但由于网络传输中的压缩和制作中的物理老化，手绘线条很容易被削弱。线的边缘处的这种劣化对视觉效果施加了相当大的负面影响。为了解决这个问题，我们从修复和增强的角度出发。具体地，我们在图像退化模型中提出了一个面向预测的压缩模块来模拟网络传输中的压缩过程，使得用这种自监督方法训练的模型能够恢复手绘线条失真。此外，我们提出一种地面实况（ground-truth，GT）增强方法，借由合并从过度锐化的GT影像中所撷取的手绘线来增强模糊、老化的手绘线。</p>
<p>其次，我们意识到动漫图像中不需要的颜色伪影问题，这是采用基于GAN的SR网络的结果[17]（见图2 B）。这些伪影表现为<strong>形状不规则、强度不同</strong>的彩色斑点，随机散布在生成的图像中，这会显著破坏视觉感知。我们将此问题归因于感知损失的图像特征是在真实感图像数据集上训练的，这在动漫领域是不一致的。为了缓解这个问题，我们进行了全面的研究，感知损失，并引入平衡的双感知损失（balanced twin perceptual loss），组装感知功能，从真实感域和动画域的平衡层缩放分布。</p>
<p>问题：提出了一个动漫图像收集管道</p>
<p><strong>贡献如下：</strong></p>
<ul>
<li><p>提出了一个图像退化模型来处理更难的压缩恢复挑战，特别是手绘线失真在动漫领域，用心加强微弱的手绘线条。</p>
</li>
<li><p>认识到并解决了在基于GAN的SR网络训练中由于感知损失的域不一致而引起的不必要的颜色伪影。</p>
</li>
<li><p>在真实世界的动漫SR数据集上彻底评估了我们的方法，并表明我们的方法比最先进的动漫机器人训练的SR方法性能更好，训练样本复杂度仅为先前工作的13.3%。</p>
</li>
<li><p>我们提出了一种新的动漫数据集策展管道，能够从视频源中收集压缩最少，信息量最大的动漫图像。</p>
</li>
</ul>
<h2 id="二、相关工作-2"><a href="#二、相关工作-2" class="headerlink" title="二、相关工作"></a>二、相关工作</h2><p>真实世界的超分辨率：经典的SR方法通常采用直接的方法，使用单个双三次下采样操作将高分辨率（HR）实况（GT）图像转换成它们的低分辨率（LR）对应物。经典的图像恢复方法为不同的任务训练不同的权重。相比之下，真实世界SR致力于通过一个模型权重来实现复杂的退化模型，以恢复在真实世界场景中发现的各种退化，例如模糊、噪声和压缩。</p>
<p>一般而言，退化模型设计可大致分为两类：显式模型和隐式模型。显式退化模型采用核函数和数学公式来模拟真实世界的退化过程。另一方面，隐式退化模型专注于训练神经网络以捕捉真实世界退化的分布。然而，隐式模型面临着可解释性和可扩展性的挑战。隐式模型的有效性缺乏明确的理论基础，而将其适应新的领域需要创建定制的数据集和额外的训练复杂性。</p>
<p>动画处理：动画代表了视觉艺术的一种独特形式，其特点往往是夸张的视觉表现。动画的创作者通常从素描线条艺术开始，然后是2D和3D动画技术，其中包括着色、CGI效果和帧内插。值得注意的是，最近在动画领域的研究已经获得了大量的关注，具有动画内容的AI绘画、动画图像的矢量化、动画插值和中间插入、动画草图着色、3D表示和动画领域适应。</p>
<p><strong>AnimeSR</strong>（NeurIPS 2022）和 <strong>VQD-SR</strong>（ICCV 2023）是现实世界动画超分辨率任务领域中的两项最新代表性研究。然而，他们并没有完全解决低级别动画修复的独特挑战。这包括<strong>在基于GAN的网络训练中的模糊手绘线和域不一致性</strong>。本文对动漫SR领域的几种精雕细琢的方法进行了全面的探索。</p>
<h2 id="三、提出的方法"><a href="#三、提出的方法" class="headerlink" title="三、提出的方法"></a>三、提出的方法</h2><h3 id="3-1-面向动画制作的图像SR数据集"><a href="#3-1-面向动画制作的图像SR数据集" class="headerlink" title="3.1.面向动画制作的图像SR数据集"></a>3.1.面向动画制作的图像SR数据集</h3><p>在本节中，我们将介绍APISR数据集及其工作流。此工作流利用动漫视频的特性来选择压缩最少和信息量最大的帧。基于I-Frame的图像收集。AnimeSR引入了<strong>AVC-Train</strong>，这是第一个基于视频的动漫SR数据集，但他们忽略了收集过程中<strong>压缩的影响</strong>，这导致 <strong>VQD-SR 提出了一种后处理技术来增强数据集</strong>。</p>
<p>相反，我们提出一个一种新的方法，以最小的努力从源级选择压缩最少的帧。</p>
<p>互联网上的所有视频都用视频压缩标准（例如，H.264 和H.265），以在质量和数据大小之间进行权衡。有许多视频压缩标准，每个都有一个复杂的工程系统，但它们共享一个类似的主干设计。这个特性促使我们找到分配给每个帧的压缩质量不同的模式。视频压缩将一些关键帧（称为 I-Frames）指定为单独的压缩单位。从经验上讲，I 帧是场景变化场景的第一帧。这些 I 帧被分配有高数据大小预算。相反，更高的压缩比要求非 I-Frames，即 P-Frames和 B-Frames，在压缩期间以 I-Frames作为参考，这引入了时间失真。如图3a所示，<strong>在我们收集的动漫视频中，I帧平均具有比其他非I帧高得多的数据大小，这真正代表了更高的质量。</strong>因此，我们使用视频处理工具 ffmpeg 从视频源中<strong>提取所有I帧作为初始池</strong>。</p>
<p><strong>基于图像复杂度的选择</strong>：为了进一步从I-Frames池中选择理想化的图像，我们需要一些标准。一种简单的方法是遵循AVCTrain使用图像质量评估（IQA）来排名和选择具有更好分数的帧。然而，IQA排名并不喜欢带有CGI效果的动画图像，而是喜欢信息量很少的简单场景（见图4）。因此，我们认为，图像复杂性评估（ICA）是一个更好的选择，在动漫领域。</p>
<p>ICA 通过对存在的细节的数量和种类进行评分来评估图像中的复杂程度。与IQA相比，ICA 对饱和度、闪电、对比度和运动模糊的变化具有更强的鲁棒性。我们使用的ICA指标是最近兴起的分析网络IC 9600 [16]。在动画领域，采用 ICA 呈现两个主要优点。首先，动漫视频中的许多场景都是典型的单调场景（如图4所示），其中大多数像素在训练中缺乏重要信息。IQA倾向于这些简单的图像，并且与其他图像相比给出了更高的分数，但是ICA使得排除这些场景，这又有助于降低训练样本复杂度。</p>
<p>第二，ICA 更擅长于识别动画制作中有意义的场景，尤其是那些具有CGI效果的场景，如图4右侧的黑暗场景。在这些情况下，IQA方法通常会出现问题。通过收集各种场景，网络训练可以在处理复杂的真实世界动画输入方面变得更加健壮。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241113194508775.png" alt="图4 单调场景，使用HyperIQA和Rechique进行图像质量评估（IQA），对比使用IC9600 [16]进行图像复杂性评估（ICA）。IQA喜欢简单的场景，并对CGI较强的图像给予较低的分数。但ICA恰恰相反。"></p>
<p>API数据集：我们开始手动采集 562个高质量的动画视频。从这些中，我们提取所有 I-Frames 作为初始选择池。利用上述图像复杂度评估方法，我们从每个视频的 I-Frames池中选择得分最高的前10个帧。在丢弃不适当的图像（例如，裸体、暴力、异常和与照片真实感内容混合的动画图像），获得3740个高质量图像作为我们提出的数据集。示例图像如图5所示。此外，如图B所示，我们的API数据集的高图像复杂度评分帧的密度明显上级AVC-Train。更多的分析和数据可以在补充材料中找到。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241113194936859.png" alt="图5 API超分辨率数据集示例。API包括多功能CGI效果场景（例如，不同的闪电和特殊效果）并呈现高图像复杂度。"></p>
<p><strong>720P恢复原始分辨率：</strong>在研究动画制作管道时，我们观察到大多数动画制作遵循720P格式（图像高度为720像素）。然而，在现实世界中，为了标准化多媒体格式，动画经常被错误地放大到1080 P或其他格式。我们的经验发现，重新缩放所有动画图像回到原来的720 P可以提供的特征密度所设想的创作者与更紧凑的动画手绘线和CGI信息。</p>
<h3 id="3-2-一种实用的动漫退化模型"><a href="#3-2-一种实用的动漫退化模型" class="headerlink" title="3.2.一种实用的动漫退化模型"></a>3.2.一种实用的动漫退化模型</h3><p>在现实世界的SR中，退化模型的设计是非常重要的。基于高阶退化模型和最近的基于图像的视频压缩恢复模型，我们提出了两项改进，以恢复扭曲的手绘线和多功能压缩伪影，并增强退化模型的表示。退化模型如图6a所示。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241113195856610.png" alt="图6 退化模型，概述我们提出的方法"></p>
<p>面向预测的压缩：利用图像退化模型在视频压缩伪影的动画恢复中提出了挑战。这是因为先前的真实世界图像SR方法采用JPEG（一种旧的但广泛使用的图像压缩标准）作为图像退化模型中的唯一压缩模块。JPEG对所有编码单元进行重复且独立的压缩，而不考虑其他单元的存在。然而，为了获得更高的压缩比，视频压缩算法会应用预测算法来搜索具有相似像素内容的参考，并仅压缩它们的差异（残差），从而降低信息熵。预测算法可以在空间（帧内预测）或时间（帧间预测）上搜索其参考。无论属于哪种类别，失真的内在原因都来自于由于预测限制而导致的残差错位。</p>
<p>因此，我们认为等同于真实世界视频压缩伪影的伪影<strong>可以使用单个图像输入结合面向预测的压缩算法</strong>（例如，WebP 和H.264）。不需要真正连续的帧。为<strong>此，我们在图像退化模型中设计了一个面向预测的压缩模块</strong>。此模块需要视频压缩算法，以单帧为基础压缩输入。与VCISR [48]相比，一次执行压缩不需要多个帧。从工程应用的角度看，该方法在理论上是合理的，在实践中是可行的。<strong>在单帧输入的情况下，视频压缩简单地应用帧内预测来压缩帧，而不使用其帧间预测功能。</strong>利用这种方法，图像退化模型能够合成类似于在图7所示的传统多帧视频压缩中观察到的压缩伪像。随后，通过将这些合成图像馈送到图像SR网络中，系统可以有效地学习各种压缩伪影的模式并参与恢复。</p>
<p>随机调整模块大小：真实世界SR域中的退化模型考虑模糊、调整大小、噪声和压缩模块。模糊、噪声和压缩是真实世界中的伪像，可以使用清晰的数学模型或算法进行合成。但是，调整大小模块的逻辑完全不同。调整大小不是自然图像生成的一部分，而是仅出于SR配对数据集的目的而引入的。考虑到这个概念，我们认为以前的固定大小调整模块不是很适合。我们提出了一种更稳健和有效的解决方案，该方案包括<strong>在退化模型中以不同的顺序随机放置调整大小操作</strong>。</p>
<h3 id="3-3-动画手绘线增强"><a href="#3-3-动画手绘线增强" class="headerlink" title="3.3.动画手绘线增强"></a>3.3.动画手绘线增强</h3><p>为了增强模糊的手绘线，直接采用全局方法（如修改退化模型或锐化整个GT）并不是理想的方法，因为网络无法在注意手绘线变化的情况下进行学习。因此，我们选择提取锐化的手绘线信息，并将其与GT合并，形成伪GT。通过将这种精心增强的伪GT引入SR训练，网络可以生成锐化的手绘线，而不需要引入额外的神经网络模块或单独的后处理网络。</p>
<p>为了提取手绘线，一种直接的方法是应用草图提取模型。然而，当前基于学习的草图提取通常以风格转移到参考图像为特征，这会扭曲手绘线条细节并包含不相关的像素内容（例如，CGI 效果的阴影和边缘）。因此，我们需要一个更细粒度的，逐像素的方法来提取手绘线。因此，我们利用XDoG [55]，一种基于像素比像素高斯核的草图提取算法，从锐化的GT中提取边缘图。然而，XDoG边缘图被过多的噪声破坏，包含离群像素和碎片线表示。为了解决这个不适定的问题，我们提出了一个离群值过滤技术，加上一个定制设计的被动膨胀方法（在补充材料中详细介绍）。通过这种方式，我们产生了一个更连贯和不受干扰的手绘线表示。</p>
<p>我们经验性地发现，过度锐化的预处理GT使得手绘线边缘比其他不相关的阴影边缘细节更明显，这使得离群值过滤器更容易区分它们的差异。因此，我们提出了三轮unsharp掩蔽GT。综上所述，公式如下：<br>$$<br>I_{sharp}=f^n(I_{GT}) \<br>I_{Map} = h(g(I_{Sharp})) \<br>I_{pseudo-GT} = I_{Sharp} · I_{Map} + I_{GT} · (1 − I_{Map}),<br>$$</p>
<p>其中f是递归执行n次的锐化函数，g表示XDoG边缘检测，h表示具有离群值滤波的被动膨胀的后处理技术。IMap是一个二进制值映射。</p>
<h3 id="3-4-动漫的平衡双感知损失"><a href="#3-4-动漫的平衡双感知损失" class="headerlink" title="3.4.动漫的平衡双感知损失"></a>3.4.动漫的平衡双感知损失</h3><p>不需要的颜色伪影的存在归因于生成器和感知损失之间的训练中的不一致的数据集域。目前，大多数使用GAN训练的SR模型，包括AnimeSR和VQD-SR，使用相同的ImageNet [14]预训练VGG [39]网络作为感知损失。然而，动漫内容，尤其是混合了 CGI 和大量插图的内容，与 ImageNet 中的照片级真实感特征存在显著不同。为了解决这个问题，我们调查感知损失和随后的改进。感知损失背后的核心思想是利用高级特征（例如，分割、分类、识别）以通过比较中间层特征输出来补充低级像素特征。在这方面，我们使用预先训练的ResNet50对Danbooru数据集进行动漫对象分类任务，Danbooru数据集是一个丰富的标签动漫插图数据库。由于预训练的网络是ResNet50而不是VGG，因此我们提出了一个类似的中间层比较（详见补充材料）。总的来说，公式如下：</p>
<p>值得注意的是，引入基于ResNet的感知损失作为唯一的感知损失可以解决不想要的颜色伪影，并导致定量改进。但是，可能会出现视觉效果不佳的情况。这归因于Danbooru数据集中的固有偏差，其中大多数图像是人物脸或相对简单的插图。因此，我们寻求一个折衷，使用真实世界的特征作为辅助引物，以指导基于ResNet的知觉损失的训练。此方法可产生视觉上吸引人的图像，并且还解决了不想要的颜色问题。GAN训练的总损失函数如下：<br>$$<br>L = αL_1 + βL_{per} + γL_{adv}\<br>L_{per} = L_{ResNet} + δL_{VGG}<br>$$<br>其中，$L_1、L_{VGG}和L_{adv}$ 是L1像素损失、基于真实感VGG的感知损失和对抗损失。α、β、γ和δ为权重参数。</p>
<h2 id="四、实验-1"><a href="#四、实验-1" class="headerlink" title="四、实验"></a>四、实验</h2><h3 id="4-1-实验细节"><a href="#4-1-实验细节" class="headerlink" title="4.1 实验细节"></a>4.1 实验细节</h3><p>在我们的实验中，我们采用我们提出的API数据集作为图像网络的训练数据集。我们使用的图像网络是GRL 的一个微型版本，带有最近的卷积上采样模块（详见补充）。</p>
<p>为了训练GAN，我们遵循与先前工作相同的两阶段训练方法。在第一阶段中，我们训练具有L1像素损失的网络<strong>300 K</strong>次迭代。在第二阶段，我们引入了我们的平衡孪生感知损失和对抗性损失，进行了额外的<strong>300K</strong>迭代。{α，β，γ，δ}的权值分别为{1，0.5，0.2，1}。</p>
<p>对于ResNet，感知损失的层权重为{0.1，20，25，1，1}，对于VGG，感知损失的层权重为{0.1，1，1，1}。</p>
<p>我们的鉴别器是与AnimeSR 和VQDSR 中相同的三尺度补丁鉴别器[23、35、51]。我们使用亚当优化器[28]，第一阶段的学习率为2×10 - 4，第二阶段的学习率为1×10 - 4。在两个阶段中，每100 K次迭代应用一次学习速率衰减。</p>
<p>训练过程在一台Nvidia RTX 4090上进行，HR patch 大小设置为256 x256，batch_size 为 32。对于退化模型，我们首先对整个HR图像执行退化，而不是像先前的工作中那样直接对裁剪的块执行退化。在退化模型中，噪声和模糊的配置与Real-ESRGAN 相同，第一个面向预测的压缩是用JPEG [47]和WebP [38]实现的。第二种面向预测的压缩包括AVIF [19]、JPEG [47]、WebP [38]以及MPEG 2 [32]、MPEG4 [2]、H. 264 [36]和H. 265 [44]的单帧压缩。放置调整大小模块的概率在所有位置之间平均分配。具体参数设置详见我们的补充资料。</p>
<h3 id="4-2-与最先进方法的比较"><a href="#4-2-与最先进方法的比较" class="headerlink" title="4.2.与最先进方法的比较"></a>4.2.与最先进方法的比较</h3><p>我们将我们的APISR与其他SOTA真实世界图像和视频SR方法进行了定量和定性比较，其中包括Real-ESRGAN 、BSRGAN、RealBasicVSR、AnimeSR 和VQD-SR 。</p>
<p>所有方法的比例因子均为4。为了验证我们方法的有效性，我们的评估基于AVC-RealLQ [56]，其中有46个视频片段，每个片段有100帧。该数据集是唯一已知的用于真实世界动漫SR测试的数据集。对于无参考指标，我们采用了与VQD-SR和AnimeSR中相同的指标，即NIQE [34]和MANIQA [62]。我们还整合了其他基于SOTA学习的图像质量评估指标，如CLIPIQA [49]。所有指标均基于pyiqa[9]库。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241113203113336.png" alt="表1 实验结果"></p>
<p>如表1所示。我们的模型具有最小的网络规模（1.03 M参数），但在所有基于图像和视频的方法中，在所有指标上都具有SOTA性能。</p>
<p>同时，值得一提的是，我们仅以AnimeSR [56]和VQDSR [46]的13.3%和25%的训练样本复杂度实现了该结果。这尤其要归功于在数据集管理中引入了图像复杂度评估，该评估选择了信息丰富的图像，以提高学习动画图像表示的效率。此外，由于我们设计的显式退化模型，我们需要对退化模型进行零训练。</p>
<p>定性比较：如图10所示，与其他方法相比，APISR极大地提高了视觉质量。在恢复高度压缩的图像时，我们的模型比所有其他方法表现出了非凡的熟练度，如第一行所示，其中我们有更少的 ringing 伪影。此外，由于所提出的手绘线增强，我们生成的图像表现出增加的线密度和清晰度，如在第二行中所观察到的。</p>
<p>事实上，我们的模型在有效恢复方面优于其他模型，第三和第四行就是证明。这要归功于我们对图像退化模型的改进，我们在压缩和调整大小功能上提供了强大的恢复能力。同时，由于我们提出的平衡的孪生感知损失，由我们的GAN网络生成的图像不显示如在AnimeSR和VQD-SR中的不想要的颜色伪影。此外，由于我们提出的数据集中收集了多功能场景，我们能够在黑暗场景中实现有效的恢复。更多的视觉效果可以在补充材料中找到。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241113203345011.png" alt="图10 部分内容"></p>
<h3 id="4-3-消融实验"><a href="#4-3-消融实验" class="headerlink" title="4.3 消融实验"></a>4.3 消融实验</h3><p>推理数据集仍为AVCArealLQ [56]。目视比较见补充资料。</p>
<p>数据集的影响：我们用几个备选方案来替代我们的API训练数据集以进行比较分析：AVC-Train [56]、从与我们的API相同的视频源中随机选择的帧、具有IQA选择的 I-Frames 的集合、以及具有ICA选择的IFrames的集合。为了进行公平的比较，我们保持训练数据集大小的相似强度。如果我们将AVC-Train视频训练数据集作为图像数据集进行训练，我们将包括时间失真图像和信息较少的帧，这使得性能在所有指标上都难以与使用API训练的模型竞争。随机选择的图像数据集性能较差，因为它们缺乏对视频中高质量帧的关注。在我们的I-Frame集合中，我们去掉了时间失真的帧，并选择了压缩程度最低的帧，但基于IQA的选择限制了性能。在相同的训练迭代次数和条件下，基于ICA准则选择的数据集优于基于IQA-based选择的数据集。使用720 P重新缩放方法，动画图像比错误放大的版本具有更紧凑的手绘线条和CGI信息，这种回归原始的思维方式提高了所有指标的性能。</p>
<p>退化模型：为了验证退化模型的优越性，我们用Real-ESRGAN [54]中的高阶退化模型和BSRGAN [65]中的随机阶退化模型来代替我们提出的退化模型，这两种模型与我们的方法有一定的相似性。我们的降级模型与面向预测的压缩模型在MANIQA [62]和CLIPIQA [49]度量方面达到了显著的改进。通过我们的混洗调整设计，我们的网络对于各种真实世界的SR场景变得更加健壮，并且性能可以更进一步，尤其是NIQE [34]指标。</p>
<p>proposed Enhancement and Perceptual Loss的优势：我们将我们的模型与未使用所提出的手绘线增强和平衡的双感知损失进行训练的普通版本进行比较。我们引入的手绘线增强功能对CLIPIQA [49]进行了显著改进。当我们在GAN训练中添加ResNet感知损失时，它在NIQE中显示出显著的改善[34]。此外，通过在ResNet感知损失部分的早期层上进行建议的缩放，两种感知损失达到了稳定的平衡，性能进一步提高。这证明了一个与动漫领域兼容的感知缺失是非常有见地和教育意义的。</p>
<h2 id="五、结论"><a href="#五、结论" class="headerlink" title="五、结论"></a>五、结论</h2><p>本文充分利用动漫生产知识的特点，充分利用动漫生产知识丰富和提升动漫SR，提出了一种高质量、高信息量的面向动漫生产的图像（API）SR数据集，并进行了新颖的数据集策展设计。为了恢复和增强手绘线条，我们提出了一个图像退化模型来恢复视频压缩伪影和伪GT增强策略。我们通过引入一个用高级动画任务训练的网络来构建一个平衡的双感知损失，进一步解决了不需要的颜色伪影。大量的实验结果表明，我们优于现有的SOTA方法，在那里我们可以恢复更难的现实世界中的低质量的动漫图像。</p>
<h1 id="CoSeR"><a href="#CoSeR" class="headerlink" title="CoSeR"></a>CoSeR</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/amusi1994/article/details/136497141">CVPR 2024 超分辨率大模型！华为和清华提出CoSeR：基于认知的万物超分大模型-CSDN博客</a></p>
<h2 id="一、介绍-3"><a href="#一、介绍-3" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>现有的图像超分模型问题如下：</p>
<ul>
<li>缺乏泛化能力。为了实现更好的超分效果，通常需要针对特定场景使用特定传感器采集到的数据来进行模型训练，这种学习方式<strong>拟合了某种低清图像和高清图像间的映射，但在其他场景下表现不佳</strong>。此外，逐场景训练的方式计算成本较高，不利于模型的部署和更新。  </li>
<li>缺乏理解能力。现有的超分方法主要依赖于从大量数据中学习图像的退化分布，<strong>忽视了对图像内容的理解，无法利用常识来准确恢复物体的结构和纹理。</strong></li>
</ul>
<p>该论文认为，真正能有效应用于真实场景的画质大模型应该具备类似系统二的多步修复能力，即<strong>基于对图像内容的认知，结合先验知识</strong>来实现图像超分（Cognitive Super-Resolution，CoSeR）</p>
<p>CoSeR 模仿了人类修复低质量图像自上而下的思维方式，首先会<strong>建立对图像内容的全面认知</strong>，包括识别场景和主要物体的特征，随后<strong>将重点转移到对图像细节的检查和还原</strong>。本文的主要贡献如下：</p>
<ul>
<li>提出了一种通用的万物超分画质大模型CoSeR，它能够<strong>从低清图像中提取认知特征，包括场景内容理解和纹理细节信息，从而提高模型的泛化能力和理解能力</strong>。  </li>
<li>提出了一种<strong>基于认知特征</strong>的参考图像生成方法，它能够生成与低清图像内容一致的高质量参考图像，用于指导图像的恢复过程，增强图像的保真度和美感度。</li>
<li>提出了一种“All-in-Attention”模块，它能够将<strong>低清图像、认知特征、参考图像</strong>三个条件注入到模型当中，实现<strong>多源信息的融合和增强</strong>。</li>
<li>在多个测试集和评价指标上，相较于现有方法，CoSeR均取得了更好的效果。同时，CoSeR在真实场景下也展现颇佳。</li>
</ul>
<h2 id="三、结构介绍"><a href="#三、结构介绍" class="headerlink" title="三、结构介绍"></a>三、结构介绍</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/370e38ceae728b2adcc2aebe20d1cf63.png" alt="CoSeR整体结构"></p>
<p>CoSeR 首先<strong>使用认知编码器来对低清图像进行解析</strong>，将提取到的<strong>认知特征传递给Stable Diffusion模型</strong>，用以激活扩散模型中的图像先验，从而恢复更精细的细节。此外，CoSeR <strong>利用认知特征来生成与低清图像内容一致的高质量参考图像</strong>。这些<strong>参考图像作为辅助信息</strong>，有助于提升超分辨率效果。最终，CoSeR使用提出的“All-in-Attention”模块，将<strong>低清图像、认知特征、参考图像</strong>三个条件注入到模型当中，进一步提升结果的保真度。</p>
<p>与直接从低清图像中获取描述的方法相比，CoSeR 的认知特征保留了细粒度的图像特征，在生成具有高度相似内容的参考图像时具有优势。在图5的第一行，使用BLIP2从低清图像生成的描述无法准确识别动物的类别、颜色和纹理。</p>
<h1 id="SwinIR"><a href="#SwinIR" class="headerlink" title="SwinIR"></a>SwinIR</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Wenyuanbo/article/details/121264131"><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40280673/article/details/143366905">图像去噪和超分辨率重建——复现SwinIR网络训练自己数据集及推理测试（详细图文教程）-CSDN博客</a></a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/BigerBang/article/details/134489432">详解SwinIR的论文和代码（SwinIR: Image Restoration Using Swin Transformer）_swinir代码-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/558789076">图像超分辨率：SwinIR学习笔记 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46872424/article/details/128313044">SwinIR: Image Restoration Using Swin Transformer论文笔记_swin transformer 降噪-CSDN博客</a></p>
<h2 id="一、介绍："><a href="#一、介绍：" class="headerlink" title="一、介绍："></a>一、介绍：</h2><p><strong>问题：卷积神经网络有以下缺陷：图像和卷积核之间的交互内容无关，且在局部处理的原则下，卷积对于长距离依赖建模无效。</strong></p>
<p><strong>而Transformer引入后， ViT 的问题在于需要划分patch，具有以下缺点：边界像素不能利用patch之外的邻近像素进行图像恢复；恢复后的图像可能会在每个patch周围引入边界伪影，这个问题能够通过patch overlapping 缓解，但会增加计算量。</strong></p>
<h2 id="三、结构"><a href="#三、结构" class="headerlink" title="三、结构"></a>三、结构</h2><p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/v2-a2c0b5dc6419245df6eb8a1ebe97c0be_1440w.jpg" alt="SwinIR结构"></p>
<p>主要分为三个部分：shallow feature extraction、deep feature extraction、highquality (HQ) image reconstruction modules。对所有的复原任务采用相同的 feature extraction modules，针对不同的任务采用不同的reconstruction modules。</p>
<p>这几个模块详解：</p>
<ul>
<li><p><strong>shallow feature extraction</strong>：首先用一个3×3卷积$H_{SF}$提取浅层特征$F_0$，$F_0=H_{SF}(I_{LQ})$</p>
</li>
<li><p><strong>deep feature extraction</strong>：将提取到的浅层特征$F_0$，使用深层特征提取模块$H_{DF}$进一步提取特征。深层特征提取模块由 K 个Residual Swin Transformer blocks(RSTB)和一个3×3卷积构成。</p>
<p>每个RSTB的输出$F_1, F_2, F_K$，以及输出的深层特征$F_{DK}$如式所示，式中$H_{RSTB_i}$表示第 i 个 RSTB模块，$H_{CONV}$表示最终的卷积层。卷积层能够将卷积的归纳偏置（inductive bias）引入基于Transformer的网络，为后续浅层、深层特征的融合奠定基础。</p>
<p>$$<br>F_i=H_{RSTB_i}(F_{i-1})\F_{DF}= H_{CONV}(F_K)<br>$$</p>
</li>
<li><p><strong>image reconstruction modules</strong>：以图像超分辨率为例，通过融合浅层特征$F_0$和深层特征$F_{DK}$来重建高质量图片$I_{RHQ}$，式中$H_{REC}$为重建模块。 $I_{RHQ}=H_{REC}(F_0+F_{DK})$ </p>
<p>浅层特征 $F_0$ 主要是包含低频信息，而深层特征则专注于恢复丢失的高频信息。SwinIR采用一个<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=212307949&content_type=Article&match_order=1&q=%E9%95%BF%E8%B7%9D%E7%A6%BB%E8%BF%9E%E6%8E%A5&zhida_source=entity">长距离连接</a>，将低频信息直接传输给重建模块，可以帮助<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=212307949&content_type=Article&match_order=1&q=%E6%B7%B1%E5%BA%A6%E7%89%B9%E5%BE%81&zhida_source=entity">深度特征</a>提取模块专注于高频信息，稳定训练。</p>
<p>在图像超分辨率任务中，通过sub-pixel convolution layer将特征<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=212307949&content_type=Article&match_order=1&q=%E4%B8%8A%E9%87%87%E6%A0%B7&zhida_source=entity">上采样</a>，实现重建。在其他任务中，则是采用一个带有残差的卷积操作：$I_{RHQ}=H_{SwinIR}(I_{LQ})+I_{LQ}$</p>
</li>
</ul>
<p>损失函数部分，图像超分辨率任务采用L1损失，通过优化SwinIR生成的高质量图像$I_{RHQ}$及其对应的标签$I_{HQ}$的来优化模型。$L=||I_{RHQ}-I_{HQ}||$，图像去噪任务和压缩任务采用Charbonnier loss，式中 ɛ 通常设置为10-3。</p>
<p>$L=||I_{RHQ}-I_{HQ}||^2+\epsilon^2$</p>
<h3 id="Swin-Transformer"><a href="#Swin-Transformer" class="headerlink" title="Swin Transformer"></a>Swin Transformer</h3><p>如下图所示，Residual Swin Transformer Block (RSTB)由残差块、Swin Transformer Layers (STL)、卷积层构成。卷积操作有利于增强平移不变性，<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=212307949&content_type=Article&match_order=1&q=%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5&zhida_source=entity">残差连接</a>则有利于模型融合不同层级的特征。</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241126153159717.png" alt="image-20241126153159717"></p>
<p>SwinTransformerLayer 和原版 Transformer 中 multi-head self-attention 的不同之处主要有局部注意力（local attention）和滑动窗口机制（shifted window mechanism）。首先，将大小为 $H×W×C$ 的输入特征reshape为 $（HW/M2）×M2×C$，即将其划分为 $HW/M2$个 $M×M$ 的local windows，然后对每个windows计算自注意力。</p>
<p>注意力计算公式如下：$Q = XP_Q, K= XP_K, V = XP_V$，该公式表示Query、Key、Value的计算过程，三个权重在不同的 window 间共享参数。</p>
<p>这个公式表示multi-head self-attention以及add and norm；第三个式子表示 feed forward network 以及add and norm。$X=MSA(LN(X))+X $、$X=MLP(LN(X))+X$</p>
<h1 id="HAT"><a href="#HAT" class="headerlink" title="HAT"></a>HAT</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_49729636/article/details/134954127">HAT（CVPR 2023）：基于混合注意力机制的图像重建网络_hat注意力机制-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zency/article/details/128465767">【图像超分辨率重建】——HAT论文精读笔记_hat模型-CSDN博客</a></p>
<p>CVPR 2023</p>
<h2 id="摘要-3"><a href="#摘要-3" class="headerlink" title="摘要"></a>摘要</h2><p>Transformer 的潜力在现有网络中仍未得到充分利用。为了激活更多的输入像素进行重建，我们提出了一种新颖的混合注意力转换器 (Hybrid Attention Transformer)。它结合了**通道注意力(CAB)和自注意力(SW-MSA)**方案，从而利用它们的互补优势。此外，为了更好地聚合跨窗口信息，我们引入了重叠交叉注意模块(OCAB)来增强相邻窗口特征之间的交互。在训练阶段，我们额外提出了一种相同任务的预训练策略，以带来进一步的改进。</p>
<h2 id="一、介绍-4"><a href="#一、介绍-4" class="headerlink" title="一、介绍"></a>一、介绍</h2><p>基于卷积神经网络（CNN）的方法获得了很好的效果，最近Transformer在NLP领域的成功带动了其在高级视觉任务中的发展，随后其在低级视觉任务方面也起到了很好的效果，尤其是Swin-Transformer。受到Swing-Transformer启发最近兴起的SISR算法是SwinIR。</p>
<p>“为什么 Transformer 比 CNN 更好”？一个直观的解释是，这种网络可以受益于<strong>自我注意机制并利用远程信息</strong>。然而，我们采用归因分析方法 LAM [14] 来检查 SwinIR 中重建所涉及的利用信息范围。有趣的是，我们发现 SwinIR 在超分辨率方面并没有比基于 CNN 的方法（例如 RCAN ）利用更多的输入像素。此外，尽管 SwinIR 获得了更高的定量性能，但在某些情况下，由于使用的信息范围有限，它产生的结果不如 RCAN。这些现象说明Transformer对局部信息的建模能力更强，但其利用信息的范围有待扩展。</p>
<p>为了解决上述局限性并进一步开发Transformer在图像恢复中的潜力，提出了一种混合注意力转换器，即HAT。我们的HAT将渠道注意和 attention 机制结合起来，以充分利用前者对全局信息的利用能力和后者强大的代表能力。此外，我们还引入了重叠交叉注意模块，以实现相邻窗口特征之间更直接的交互。通过这些设计，我们的模型可以激活更多的像素进行重建，从而获得了显著的性能改善。</p>
<p>由于Transformer不像神经网络那样具有感应偏差，因此大规模数据预训练对于释放此类模型的潜力非常重要。本文提出了一种有效的同任务预训练策略。与IPT 使用多个恢复任务进行预训练和EDT 使用特定任务的多个退化水平进行预训练不同，我们直接使用同一任务的大规模数据集进行预训练。我们认为，大规模数据才是预训练的真正重要因素。仿真结果验证了所提方法的有效性。采用上述设计，HAT在SR任务上可以超过最新的方法很大的幅度（0.3dB 〜 1.2dB）。</p>
<p><strong>问题：Transformer对局部信息的建模能力更强，但其利用信息的范围有待扩展，进一步开发Transformer在图像恢复中的潜力。</strong></p>
<p><strong>本文贡献：</strong></p>
<ol>
<li><strong>将通道注意力引入 Transformer 以利用更多输入信息</strong></li>
<li><strong>提出了一个重叠的交叉注意力模块来更好地聚合跨窗口信息</strong></li>
<li><strong>提供了一个相同任务的预训练策略来进一步激活所提出的网络的潜力</strong></li>
</ol>
<h2 id="二、相关工作-3"><a href="#二、相关工作-3" class="headerlink" title="二、相关工作"></a>二、相关工作</h2><p>传统CNN：由于SRCNN首先将深度卷积神经网络（CNN）引入图像SR任务，并获得了比传统SR方法更上级的性能，因此许多深度网络已经提出了SR，以进一步提高重建质量。例如，许多方法应用更精细的卷积模块设计，如残差块（Residual Block）和密集块（Dense Block），以增强模型表示能力。一些研究工作探索了更多不同的框架，如递归神经网络和图神经网络。为了提高感知质量，一些方法引入对抗学习以生成更真实的结果。通过使用注意机制，在重建保真度方面实现了进一步的改善。最近，一系列基于Transformer的网络被提出，并不断刷新SR任务的最新技术水平，显示了 Transformer 强大的表示能力。为了更好地理解SR网络的工作机制，一些工作被提出来分析和解释SR网络。LAM 采用积分梯度方法来探索哪些输入像素对最终性能贡献最大。DDR 揭示了基于深度特征降维和可视化的SR网络中的深层语义表示。FAIG 被提出来为盲SR中的特定退化找到判别滤波器。引入信道显着图来证明Dropout可以帮助防止真实的SR网络的协同适应。SRGA 旨在评估SR方法的泛化能力。在这项工作中，我们利用LAM [23]来分析和理解SR网络的行为。</p>
<p>CV领域的Transformer：Transformer 在自然语言处理领域的成功而引起了计算机视觉界的关注。一系列基于 Transformer 的方法已经被开发用于高级视觉任务，包括图像分类，目标检测，分割等。虽然 viT 已经显示出其在建模远程依赖方面的优势，仍有许多工作证明卷积可以帮助 Transformer 实现更好的视觉表示。由于令人印象深刻的性能，Transformer 也被引入用于低级视觉任务。SwinIR、EDT是将Transformer技术应用到SISR的伟大尝试。现有工作仍然不能充分发挥 Transformer 的潜力，而本文方法可以激活更多的输入信息以实现更好的重建。</p>
<p>图像重建的目的是从劣化的图像中产生高质量的图像。由于深度学习已成功应用于各种图像恢复任务，例如图像超分辨率、图像去噪和压缩伪影减少，因此已提出大量深度网络用于图像恢复，在 Transformer 被应用于低级视觉任务并展示出令人印象深刻的性能之前，基于CNN的网络在该领域占据主导地位。例如，ARCNN 堆叠了几个卷积层，以构建用于JPEG压缩伪影减少的深度网络。DnCNN 采用卷积结合批量归一化的方法构造图像去噪深度网络。RDN 介绍了用于各种图像恢复任务的剩余密集CNN网络。在[73]中，研究了用于图像超分辨率的几十种CNN方法。随着Transformer在计算机视觉中的广泛应用，基于Transformer的图像复原方法也逐渐发展起来。SwinIR [22]基于Swin Transformer [16]，在图像超分辨率、图像去噪和JPEG压缩伪影减少方面表现出色。Uformer [19]设计了一种U型Transformer网络，用于各种图像恢复任务。Restormer [20]基于转置的自注意构造了一个U型Transformer，并在多个图像恢复任务上实现了最先进的性能。与之前的基于CNN的方法相比，基于变压器的网络已经证明了上级的性能。此外，该方法还引入了混合注意机制，进一步提高了图像复原Transformer的性能。</p>
<h2 id="三、具体工作"><a href="#三、具体工作" class="headerlink" title="三、具体工作"></a>三、具体工作</h2><p>Swin Transformer已经在图像超分辨率方面表现出色。然后我们很想知道是什么让它比基于 CNN 的方法更有效。为了揭示其工作机制，我们采用了一种诊断工具——LAM，这是一种为 SR 设计的归因方法。</p>
<p>使用 LAM，我们可以判断哪些输入像素对所选区域的贡献最大。如图 2(a) 所示，红色标记点是有助于重建的信息像素。直观地说，利用的信息越多，可以获得越好的性能。比较 RCAN和 EDSR，这对于基于 CNN 的方法来说是正确的。然而，对于基于 Transformer 的方法——SwinIR，其 LAM 并没有显示出比 RCAN 更大的范围。</p>
<p>这与我们的常识相矛盾，但也可以为我们提供额外的见解。首先，这意味着 SwinIR 具有比 CNN 更强的映射能力，因此可以使用更少的信息来获得更好的性能。其次，如果 SwinIR 可以利用更多的输入像素，它仍有改进空间。 SwinIR 在蓝色框中标记的重建模式不如 RCAN。 通道注意力帮助RCAN看到更多像素，这对Transformer可能也有好处。<br>在 SwinIR 的中间特征图中观察到明显的块效应，如图 2（b）所示。这些伪影是由窗口分区机制引起的，这种现象表明移位窗口机制无法有效地建立跨窗口连接。一些针对高级视觉任务的工作也指出，增强窗口之间的连接可以改进基于窗口的自注意力方法。</p>
<p><strong>基于以上两点，我们研究了基于 Transformer 的模型中的通道注意力，并提出了一个重叠的交叉注意力模块，以更好地为基于窗口的 SR Transformer 聚合跨窗口信息。</strong><br>具体网络结构如下所示：</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/image-20241122103853243.png" alt="HAT结构"></p>
<p>这里面一些比较重要的模块：</p>
<h3 id="HAB-模块"><a href="#HAB-模块" class="headerlink" title="HAB 模块"></a>HAB 模块</h3><p>标准Swin Transformer块的类似结构，并融入了Channel Attention。</p>
<p>增加 window size 大小为16，扩大窗口感受野，因为根据实验，限制窗口大小能节省计算成本，单通过移位窗口逐步增加感受野，却牺牲了自注意力机制的表征能力。</p>
<p>引入通道注意力机制，会激活更多像素，因为其涉及利用全局信息计算，有利于对纹理部分的优化。</p>
<h3 id="overlapping-window-partition"><a href="#overlapping-window-partition" class="headerlink" title="overlapping window partition"></a>overlapping window partition</h3><p>使用像素token计算每个窗口特征内的交叉注意力。</p>
<p>大概：</p>
<ul>
<li><strong>残差混合注意力组（RHAG</strong>）深层特征提取的基本单元</li>
<li><strong>混合注意力块（HAB</strong>）在STL的基础上添加了CAB，修改MSA为(S)W-MSA</li>
<li><strong>重叠交叉注意力块（OCAB</strong>）在STL的基础上将MSA替换为OCA（基于重叠窗口分区计算，查询更大窗口）</li>
</ul>
<h3 id="预训练ImageNet"><a href="#预训练ImageNet" class="headerlink" title="预训练ImageNet"></a>预训练ImageNet</h3><p>IPT、EDT 等表明预训练在低级任务中起着重要作用。这些工作旨在探索多任务预训练对目标任务的影响。相比之下，我们基于相同的任务直接对更大规模的数据集（即 ImageNet ）进行预训练。例如，当我们要为×4 SR训练一个模型时，我们首先在ImageNet上训练一个 ×4 SR模型，然后在特定的数据集上进行微调，比如DF2K。同任务预训练，更简单同时带来更多的性能提升。</p>
<p>值得一提的是，足够的预训练训练迭代次数和适当的小学习率进行微调对于预训练策略的有效性非常重要。我们认为这是因为 Transformer 需要更多的数据和迭代来学习任务的一般知识，但需要较小的学习率进行微调以避免对特定数据集的过度拟合。</p>
<h2 id="四、实验-2"><a href="#四、实验-2" class="headerlink" title="四、实验"></a>四、实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul>
<li>训练集：DIV2K+Flicker2K</li>
<li>预训练：ImageNet</li>
<li>网络详情：RHAG、HAB 均为6个，HAT-L中 RHAG 个数加倍</li>
<li>比较指标：PSNR、SSIM（YCbCr）</li>
</ul>
<h3 id="4-2-不同窗口大小的影响"><a href="#4-2-不同窗口大小的影响" class="headerlink" title="4.2.不同窗口大小的影响"></a>4.2.不同窗口大小的影响</h3><p>16*16的窗口效果最好，本文采用了该设置</p>
<p><img src="/images/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/7be7ea27c461463e4c7591bae7b6a408.png" alt="HAT实验表"></p>
<p>在x2, x3, x4倍数下与 EDSR, RCAN, SAN, IGNN, HAN, NLSN, RCAN-it ,比较，同时在预训练方法下与IPT、 EDT比较，均达到了最优的效果。具体数据见原文。</p>
<p>在同任务情况下，预训练表现更好，不仅在预训练阶段，在微调过程中也表现得更好。与特定任务的预训练相比，多任务预训练似乎削弱了性能。从这个角度来看，我们倾向于认为 “为什么预训练有效” 的原因是数据的多样性，而不是任务之间的相关性。</p>
<p>对将Transformer引入超分 SwinIR 算法进行改进，在 RCAN 的基础上结合通道注意力（RCAN）和自注意力（SwinIR），并提出OCAB。总体而言，<strong>HAT是基于Transformer、注意力机制的超分的优秀作品</strong></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://lightdust02.github.io/2024/11/16/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E8%B6%85%E5%88%86/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86-%E5%A4%8D%E5%8E%9F/" rel="tag">图像超分\复原</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag">计算机视觉</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/2024/09/27/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-%E5%90%88%E9%9B%86/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">论文阅读笔记-DDFM</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.staticfile.org/valine/1.4.16/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "",
    app_key: "",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023-2024
        <i class="ri-heart-fill heart_icon"></i> LightDust
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
        <li>
          <a href="http://www.beian.miit.gov.cn/" target="_black" rel="nofollow">浙ICP备88888888</a>
        </li>
        
    </ul>
    <ul>
      
      <li>
          <img src="/images/beian.png"></img>
          <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=01234567890123" target="_black" rel="nofollow">浙公网安备01234567890123号</a>
      </li>
        
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="LightDust"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>